
# Introduction to Multiple Regression

In the early chapters of our text we observed that everything varies.
In a statistical model, the goal is to identify structure in the variation that the data possess.
This means that we must partition the variation in the data into (1) a systematic component, and (2) a non-systematic, or random, component.
After this unit, you should be able to

* Recognize the structure of a multiple regression model.
* Fit a regression model in R.
* Interpret the coefficients in a regression model.
* Distinguish between the model constructed in design of experiments (ANOVA) and that in regression.


In ANOVA testing, the systematic component is comprised of measured factors of research interest that may or may not relate to the response variable.
The random component is usually a catch-all for everything else: if the model is built well, there should be no systematic information left in the random component: rather, it should only contain random fluctuations due to the natural inherent variability in the measurements.

Suppose instead of factors (categorical inputs) we have measured predictor variables. 
For example, consider the admissions data from Chapter 1 of this text; there may be a systematic tendency to see higher freshman GPAs from students with higher ACT scores. 
Both variables are measured numeric values (compared to pre-determined treatments). 
This leads to the idea of regression modeling.

## Regression Model

The general form of a multiple linear regression (MLR) model is 

$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_k X_k + \varepsilon$$
where $Y$ is the response variable, $X_j$ is the $j^\mathrm{th}$ predictor variable with coefficient $\beta_j$ and $\varepsilon$ is the unexplained random error.

Note that if $\beta_1=\beta_2=\ldots=\beta_k=0$ we reduce to our baseline model

$$Y=\beta_0 + \varepsilon = \mu + \varepsilon$$
we saw earlier in the class. 
The multiple regression model is a generalization of a $k$ factor ANOVA model but instead of categorical inputs, we have numeric, or quantitative, inputs.

**Simple Linear Regression.** A special case of the above model occurs when $k=1$ and the model reduces to
$$Y = \beta_0 + \beta_1 X + \varepsilon$$

This is known as the simple linear regression (SLR) model (covered in your Intro Statistics course).
It is very rare that a practicing Statistician will ever fit a SLR but we will utilize it to explain some key concepts.

It should be noted that multiple regression is in general a $k + 1$ dimensional problem, so it will usually not be feasible to graphically visualize a fitted model like we can with SLR (which was 2-dimensional).
Not to worry though, as we can quantitatively explain what is going on in higher dimensions.
In the following sections will we utilize SLR to help visualize important statistical concepts.

The **goals of linear regression** are:

* Formal assessment of the impact of the predictor variables on the response.
* Prediction of future responses.

These two goals are fundamentally different and may require different techniques to build a model.
We outline the fundamental concepts and statistical methods over the next six chapters.

## Fitting a regression model

Regression models are typically estimated through the method of least squares. 
For the sake of visualizing the concept of least squares, we will consider a SLR example.

### Simple Linear Regression Example

**Example.** Muscle Mass with Age (originally in @KutnerText).

A person's muscle mass is expected to decrease with age.
To explore this relationship in women, a nutritionist randomly selected 15 women from each 10-year age group beginning with age 40 and ending with age 79.
The data reside in the file `musclemass.txt`. 
The variables in the dataset of interest are `mass` and `age`. 

First input the data and take a look at the observations.

```{r ch5-1-noeval, eval=FALSE}
site <- "https://tjfisher19.github.io/introStatModeling/data/musclemass.txt"
muscle <- read_table(site, col_type=cols())
glimpse(muscle)
```

```{r ch5-1data, echo=FALSE}
muscle <- read_table("docs/data/musclemass.txt", col_type=cols())
glimpse(muscle)
```

Figure \@ref(fig:ch5-1) below displays the data.

```{r ch5-1, fig.align='center', fig.cap='Scatterplot showing the relationship between age and muscle mass in a selection of women aged 40 to 79.', out.width='80%', fig.asp=0.55}
ggplot(muscle) + 
  geom_point(aes(x=age,y=mass)) +
  labs(title="Muscle Mass vs Age",
       x="Age (years)", "Muscle Mass (kg)") + 
  theme_minimal()
```

We can clearly see the negative trend one would expect: as age increases, muscle mass tends to decrease.
You should also notice that it decreases in a roughly linear fashion, so it makes sense to fit a simple linear regression model to this data.

The systematic component of a simple linear regression model passes a straight line through the data in an attempt to explain the linear trend (see Figure \@ref(fig:ch5-2)).
We can see that such a line effectively explains the trend, but it does not explain it perfectly since the line does not touch all the observed values.
The random fluctuations around the trend line are what the $\varepsilon$ terms account for in the model.

```{r ch5-2, echo=FALSE, fig.align='center', fig.cap='Scatterplot showing the relationship between age and muscle mass in a selection of women aged 40 to 79 with a fitted overlayed simple linear regression line.',out.width='80%', fig.asp=0.55}
ggplot(muscle) + 
  geom_point(aes(x=age,y=mass)) +
  geom_smooth(aes(x=age,y=mass), method="lm", se=FALSE, formula=y~x)+
  labs(title="Muscle Mass vs Age",
       x="Age (years)", "Muscle Mass (kg)") + 
  theme_minimal()
```

The next goal is to somehow find the best fitting line for this data.
There are an infinite number of possible straight line models of the form $Y = \beta_0 + \beta_1 X + \varepsilon$ that we could fit to a data set, depending on the values of the slope $\beta_1$ and y-intercept $\beta_0$ of the line.
Given a scatterplot, how do we determine which slope and y-intercept produces the "best fitting" line for a given data set?
Well, first we need to define what we mean by "best".

Our criterion for finding the best fitting line is rooted in the residuals that the line would produce.
In the two-dimensional simple linear regression case, it is easy to visualize this. 
When a straight line is "fit" to a data set, the fitted (or "predicted") values for each observation fall on the fitted line (see Figure \@ref(fig:ch5-3)). 
However, the actual observed values randomly scatter around the line.  
The vertical discrepancies between the observed and predicted values are the residuals we spoke of earlier. 
We can visualize this by zooming into the plot.

```{r ch5-3, echo=FALSE, fig.align='center', fig.cap='Zoomed in view of fitted regression line modeling the relationship of age and mass muscle mass, highlighting the residuals of the fit.',out.width='80%', fig.asp=0.55}
muscle <- muscle %>% 
  mutate(Fitted=fitted(lm(mass~age,data=muscle)))

ggplot(muscle) + 
  geom_segment(aes(x=age,xend=age, y=mass, yend=Fitted), color="red") +
  geom_point(aes(x=age,y=mass)) +
  geom_smooth(aes(x=age,y=mass), method="lm", se=FALSE, formula=y~x)+
  labs(title="Zoom in line with 'errors'",
       x="Age (years)", "Muscle Mass (kg)") + 
  coord_cartesian(xlim=c(70,75)) +
  theme_minimal()
```

It makes some logical sense to use a criterion that somehow collectively minimizes these residuals, since the best fitting line should be the one that most closely passes through the observed data.
We need to estimate $\beta_0$ and $\beta_1$ for this "best" line.

Also note in the Figure \@ref(fig:ch5-3) that any reasonable candidate model must pass through the data, producing both positive residuals (actual response values $>$ fitted response values) and negative residuals (actual response values $<$ fitted response values). 
When we collectively assess the residuals, we do not want the positive ones to cancel out or offset the negative ones, so our criterion will be to minimize the sum of squared residuals (thus all are positive).
This brings us to what is known as the method of least squares (LS), which is outlined below in the context of simple linear regression.

**Method of Least Squares**

We propose to "fit" the model $Y = \beta_0 + \beta_1 X + \varepsilon$ to a data set of $n$ pairs: $(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)$. 
The goal is to optimally estimate $\beta_0$ and $\beta_1$ for the given data.  

Denote the estimated values of $\beta_0$ and $\beta_1$ by $b_0$ and $b_1$, respectively. 
Note that it is also common to denote the estimated values as $\hat{\beta}_0$ and $\hat{\beta}_1$.

The fitted values of $Y$ are found via the linear equation $\hat{Y}=b_0 + b_1 X$ (or $\hat{Y}=\hat{\beta}_0 + \hat{\beta}_1 X$). 
In terms of each individual $(x_i, y_i)$ sample observation, the fitted and observed values are found as follows:

$$\begin{array}{c|c}
\textrm{Fitted (predicted) values} & \textrm{Observed (actual) values} \\
\hline
\hat{y}_1 = b_0 + b_1 x_1 & y_1 = b_0 + b_1 x_1 + e_1 \\
\hat{y}_2 = b_0 + b_1 x_2 & y_2 = b_0 + b_1 x_2 + e_2 \\
\vdots & \vdots \\
\hat{y}_n = b_0 + b_1 x_n & y_n = b_0 + b_1 x_n + e_n 
\end{array}$$

 
The difference between each corresponding observed and predicted value is the sample residual for that observation: 
$$\begin{array}{c}
e_1 = y_1 - \hat{y}_1 \\
e_2 = y_2 - \hat{y}_2 \\
\vdots \\
e_n = y_n - \hat{y}_n
\end{array}$$

or in general, $e_i = y_i - \hat{y}_i$. 
The method of least squares determines $b_0$ and $b_1$ so that

$$\textrm{Residual sum of squares (RSS)} = \sum_{i=1}^n e_i^2 = \sum_{i=1}^n\left(y_i - \hat{y}_i\right)^2 = \sum_{i=1}^n\left(y_i - \left(b_0 + b_1 x_i\right)\right)^2$$
is a minimum.
In other words, any other method of estimating the $y$-intercept and slope, $\beta_0$ and $\beta_1$, respectively, will produce a larger RSS value than the method of least squares.  

Minimizing RSS is a calculus exercise, so we will skip the details here.
The resulting line is the "best-fitting" straight line model we could possibly obtain for the data.

* $b_0$ and $b_1$ are called the least squares estimates of $\beta_0$ and $\beta_1$.
* The line given by $\hat{Y} = b_0 + b_1 X$ is called the simple linear regression equation.

We use R to fit such models and estimate $\beta_0$ and $\beta_1$ using the `lm()` function (`lm`=linear model). 
No hand calculations required! 
Linear models are fit using the R function `lm()`, and the basic format for a formula is given by `response ~ predictor`.


The `~` ("tilde") here is read as "is modeled as a linear function of" and is used to separate the response variable from the predictor variable(s). 
For simple linear regression, the form is `lm(y ~ x)`.
In other words, `lm(y ~ x)` fits the regression model $Y = \beta_0 + \beta_1 + \varepsilon$.

The $y$-intercept is always included, unless you specify otherwise. 
`lm()` creates a model object containing essential information about the fit that we can extract with other R functions. 
We illustrate via an example involving the muscle mass dataset.

```{r ch5-4}
muscle.fit <- lm(mass ~ age, data=muscle)
glimpse(muscle.fit)
```

You'll note from the `glimpse()` function there are many attributes inside the `lm` object. 
We will utilize many of these in our exploration of regression models in the coming chapters.

### Multiple Regression Example

To fit a multiple regression, the code is essentially the same. Consider another example.

**Example:** Property appraisals (example from @McClaveSincich2008).

Suppose a property appraiser wants to model the relationship between the sale price (`saleprice`) of a residential property and the following three predictor variables:

* `landvalue` - Appraised land value of the property (in $)
* `impvalue` - Appraised value of improvements to the property (in $)
* `area` - Area of living space on the property (in sq ft)

The data are in the R workspace `appraisal.RData` in our repository.  Lets use R to fit what is known as a "main effects" multiple regression model.  The form of this model is given by:

$$\textrm{Sale Price} = \beta_0 + \beta_1(\textrm{Land Area}) + \beta_2(\textrm{Improvement Value}) + \beta_3(\textrm{Area}) + \varepsilon$$

Note the model is just an extension of the simple linear regression model but with three predictor variables.
Similar to our study of ANOVA modeling we follow a standard pattern for analysis:

1. Describe the data both numerically and graphically.
2. Fit a model.
3. Once satisfied with the fit, check the regression assumptions.
4. Once the assumptions check out, use the model for inference and prediction.

Before embarking on addressing each part, it might be instructive, just once (since this is your first multiple regression encounter), to actually look at the raw data to see its form:

```{r, eval=FALSE}
load(url("https://tjfisher19.github.io/introStatModeling/data/appraisal.RData"))
kable(appraisal)
```

```{r ch5-5, echo=FALSE}
load("docs/data/appraisal.RData")
kable(appraisal)
```

We see there are four variables ($k+1 = 4$) and 20 observations ($n = 20$).
Each row contains a different member of the sample (in this case, a different property).
Notice the one property with the relatively high selling price as compared to the others.

Pairwise scatterplots are given below to visualize the bivariate associations.
Here we use the `ggpairs()` function in the add-on package `GGally` [@R-GGally]. 
Pairwise scatterplots provide a means to visually explore all $k+1$ dimensions of a dataset, but note that as $k$ and $n$ (the sample size) increase, these plots can get very "busy".

```{r ch5-6, message=FALSE, fig.align='center', fig.cap='Pairs plot showing the relationship between the variables `landvalue`, `impvalue`, `area` and `saleprice`.', fig.asp=1, out.width='80%'}
ggpairs(appraisal, columns=c(2:4,1) )
```

Note that using the `columns=c(2:4,1)` option, we have reordered the variables for the plot (generally, it is best to have the response variable last so it appears on the $y$-axis in the scatterplots).
In the bottom left of the matrix of plots in Figure \@ref(fig:ch5-6) we have pairwise scatterplots.
At first glance, it seems as though each of the three predictors positively relates to sales price.
However, we also get to see the plots of predictors against themselves.
This can be highly informative and will be of some importance to us later on.
There appear to be positive associations between all the predictors (not surprisingly given the context).
It is also instructive to note that three specific individual properties with high appraised land values seem to be the catalyst for these apparent associations. 

In the upper right corner of Figure \@ref(fig:ch5-6) are the Pearson correlation coefficients (which measures the amount of linear relationship between the two variables) and along the diagonal are density plots of each variable (providing some information about each variables shape).

We now fit the "main effects" MLR model for predicting $Y$ = `saleprice` from the three predictors.
Initially, we might be interested in seeing how individual characteristic(s) impact sales price.

```{r ch5-7}
appraisal.fit <- lm(saleprice ~ landvalue + impvalue + area, data=appraisal)
summary(appraisal.fit)
```

Here are some observations from this regression model fit:

**Parameter estimates.**  The fitted model, where $Y$ = sale price, is:

$$\hat{Y} = 1384.197 + 0.818(\textrm{Land value}) + 0.819(\textrm{Improvement value}) + 13.605(\textrm{Area})$$

That is, the least squares estimates of the four $\beta$-parameters in the model are $b_0 = 1384.2$, $b_1 = 0.817$, $b_2 = 0.819$, and $b_3 = 13.605$.
We will discuss their interpretation later.

**Residual standard error.** In regression, the error variance $\sigma^2$ is a measure of the variability of all possible population response $Y$-values around their corresponding predicted values as obtained by the true population regression equation.
It is called "error" variance because it deals with the difference between true values vs. model-predicted values, and hence can be thought of as measuring the "error" one would incur by making predictions using the model.

Since a variance is always a sum of squares divided by degrees of freedom (SS/df), we can estimate $\sigma^2$ for a simple linear regression model using the following:
 
$$S^2_{\varepsilon} = \frac{\sum_{i=1}^n(y_i - \hat{y}_i)^2}{n-p} = \frac{\textrm{RSS}}{n-p}.$$

The degrees of freedom for this error variance estimate is $n -p$ where $p$ is the number of parameters estimated in the regression model, here $p = k+1 = 3+1=4$.
So, we had to "spend" $p=k+1$ degrees of freedom from the $n$ available degrees of freedom in order to estimate $\beta_0, \beta_1$, $\ldots$, $\beta_k$.
(Hopefully you can see that you cannot fit a model with $n$ or more parameters to a sample of size $n$; you will have "spent" all your available degrees of freedom, and hence you could not estimate $\sigma^2$.)

We will usually just denote the estimated error variance using $S^2$ instead of $S^2_{\varepsilon}$. 
R provides the residual standard error in the `summary()` output from the linear model fit, which is the square root of the estimated error variance, and thus has the advantage of being in the original units of the response variable $Y$.
Here, $s = 7915$ which is our estimate for $\sigma$.
Applying an Empirical Rule type argument (remember from Intro Statistics!), we could say that approximately 95% of this model's predicted sale prices would fall within $\pm 2(7915) = \pm \$15,830$ of the actual sales prices.
The error degrees of freedom are $n – p$ = $20 – 4 = 16$. 


**Interpretation.** Since this is essentially the standard deviation of the residuals, we could interpret the value of $S$ as essentially the average residual size; *i.e.*, the average size of the prediction errors produced by the regression model.
In the present context, this translates to stating that the regression model produces predicted sale prices that are, on average, \$7,915 dollars off from the actual measured sale price values. 

As you can see, such a measure is valuable in helping us determine how well a model performs (*i.e.*, smaller residual error $\rightarrow$ better fitting model).
So, $S^2$ and the standard errors are important ingredients in the development of inference in regression. 


It should also be noted that at this point we are not even sure if this is a good model, and if not, how we might make it better.
So later on, we will discuss the "art" of model building.



## Interpreting $\beta$-parameter estimates in MLR

Suppose we fit a model to obtain the multiple linear regression equation: 

$$\hat{Y} = b_0 + b_1 X_1 + b_2 X_2 + \ldots + b_k X_k$$

What does $b_1$ mean?
In multiple regression involving simultaneous assessments of many predictors, interpretation can become problematic.
In certain cases, a $b_i$ coefficient might represent some real physical constant, but oftentimes the statistical model is just a convenience for representing a more complex reality, so the real meaning of a particular $b_i$ may not be obvious.

At this point in our trek through statistical model, it is important to remember that there are two methods for obtaining data for analysis: designed experiments and observational studies.
It is important to recall the distinction because each type of data results in a different approach to interpreting the $\beta$-parameter estimates in in a multiple linear regression model. 


### Designed experiments

In a designed experiment, the researcher has control over the settings of the predictor variables $X_1$, $X_2$, $\ldots$, $X_k$.
For example, suppose we wish to study several physical exercise regimens and how they impact calorie burn.
The experimental units (EUs) are the people we use for the study.
We can control some of the predictors such as the amount of time spent exercising or the amount of carbohydrate consumed prior to exercising.
Some other predictors might not be controlled but can be measured, such as baseline metabolic variables.
Other variables, such as the temperature in the room or the type of exercise done, could be held fixed.

Having control over the conditions in an experiment allows us to make stronger conclusions from the analysis.
One important property of a well-designed experiment is called orthogonality.
Orthogonality is useful because it allows us to easily interpret the effect one predictor has on the response without regard to any others.
For example, orthogonality would permit us to examine the effect of increasing $X_1$ = time spent exercising on $Y$ = calorie burn, without any concern for $X_2$ = carbohydrate consumption.
This can only occur in a situation where the predictor settings are judiciously chosen and assigned by the experimenter.
Let us look at an example.

### Example of Orthogonal Design

**Example.** Cleaning experiment (from @NavidiMonk2015).

An experiment was performed to measure the effects of three predictors on the ability of a cleaning solution to remove oil from cloth. The data are in the R workspace `cleaningexp.RData`. Here are some details:

Response:

* `pct.removed` - Percentage of the oil stain removed

Predictors:	

* `soap.conc` - Concentration of soap, in % by weight
* `lauric.acid` - Percentage of lauric acid in the solution
* `citric.acid` - Percentage of citric acid in the solution

Soap concentration was controlled at two levels (15% and 25%), lauric acid at four levels (10%, 20%, 30%, 40%), and citric acid at three levels (10%, 12%, 14%).
Each possible combination of the three predictors was tested on five separate stained cloths, for a total of $5 \times 2 \times 4 \times 3 = 120$ measurements.
We want to illustrate the effect of orthogonality on the $\beta$-parameter estimates.

```{r, eval=FALSE}
load(url("https://tjfisher19.github.io/introStatModeling/data/cleaningexp.RData")
glimpse(cleaningexp)
```

```{r ch5-8, echo=FALSE}
load("docs/data/cleaningexp.RData")
glimpse(cleaningexp)
```

Since we have many numeric levels and we are interested in the numeric association (as compared to the categorical association in ANOVA), we fit a linear model to all three predictors:

```{r ch5-9}
summary(lm(pct.removed ~ soap.conc + lauric.acid + citric.acid, data=cleaningexp))
```

Take note of the model's $\beta$-parameter estimates and their SEs.  

Now for illustration only, let's drop `soap.conc` from the model:

```{r ch5-10}
summary(lm(pct.removed ~ lauric.acid + citric.acid, data=cleaningexp))
```

Notice that the coefficient estimates do not change regardless of what other predictors are in the model (this would hold true if we dropped `lauric.acid` or `citric.acid` from the model as well - try it!).
This is what orthogonality ensures.
This means that we are safe in assessing the size of the impact of, say, soap concentration on the ability to remove the oil stain, without worrying about how other variables might impact the relationship.  

So in a designed experiment with orthogonality properties, we can interpret the value of $b_1$ unconditionally as follows: 

> A one-unit increase in $x_1$ will cause a change of size $b_1$ in the mean response. 

**Side note.** When we deleted the predictors one at a time, we effectively were taking that predictor's explanatory contribution to the response and dumping it into the error component of the model.
Here, since each predictor was significant ($p$-value < 0.0001), this removal caused the residual standard error to increase substantially, which subsequently made the SEs of the coefficients, $t$-statistics and $p$-values change.
We want to make it clear that **in practice it is not recommended to remove significant effects from the model** --- it was only done here to demonstrate that orthogonality ensures that the model's $\beta$-parameter estimates are unchanged regardless of what other predictors are included.
However, the results of tests/CIs for those coefficients may change depending on what is included in the model (if you remove an insignificant predictor, the residual SE will change only slightly and hence have negligible impact on SEs of the coefficients, $t$-statistics and $p$-values).  

### Observational studies

In most regression settings, you simply collect measurements on predictor and response variables as they naturally occur, without intervention from the data collector.
Such data is called observational data.  

Interpreting models built on observational data can be challenging.
There are many opportunities for error and any conclusions will carry with them substantial unquantifiable uncertainty.
Nevertheless, there are many important questions for which only observational data will ever be available.
For example, how else would we study something like differences in prevalence of obesity, diabetes and other cardiovascular risk factors between different ethnic groups?
Or the effect of socio-economic status on self esteem?
It is impossible to design experiments to investigate these since we cannot control variables (or it would be grossly unethical to do so), so we must make the attempt to build good models with observational data in spite of their shortcomings.  

In observational studies, establishing causal connections between response and predictor variables is nearly impossible.
In the limited scope of a single study, the best one can hope for is to establish associations between predictor variables and response variables.
But even this can be difficult due to the uncontrolled nature of observational data.
**Why?** It is because unmeasured and possibly unsuspected "lurking" variables may be the real cause of an observed relationship between response $Y$ and some predictor $X_i$.
Recall the earlier example where we observed a positive correlation between the heights and mathematical abilities of school children?
It turned out that this relationship was really driven by a lurking variable – the age of the child.
In this case, the variables height and age are said to be confounded, because for the purpose of predicting math ability in children, they basically measure the same predictive attribute.

In observational studies, it is important to adjust for the effects of possible confounding variables.
Unfortunately, one can never be sure that the all relevant confounding variables have been identified.
As a result, one must take care in interpreting $\beta$-parameter estimates from regression analyses involving observational data.

Here is probably the best way of interpreting a $\beta$-parameter estimate (say $b_1$) when dealing with observational data:

> $b_1$ measures the effect of a one-unit increase in $x_1$ on the mean response when all the other (specified) predictors are held constant.

Even this, however, is not perfect.
Often in practice, one predictor cannot be changed without changing other predictors.
For example, competing species of ground cover in a botanical field study are often negatively correlated, so increasing the amount of cover of one species will likely mean the lowering of cover of the other.
In health studies, it is unrealistic to presume that an increase of 1\% body fat in an individual would not correlate to changes in other physical characteristics too (*e.g.*, waist circumference).  

Furthermore, this interpretation requires the specification of the other variables – so changing which other variables are included in the model may change the interpretation of $b_1$.
Here's an illustration:

### Example of observational regression interpretation

**Example:** Property appraisals (from @McClaveSincich2008).

We earlier fit a full "main effects" model predicting $Y$ = salesprice from three predictor variables dealing with property appraisals.
This is an observational study, since the predictor values are not set by design by the researcher.
Here's a brief recap of the fitted model:

```{r ch5-11}
summary(appraisal.fit)
```

*Interpretation.* We see $b_3$ = 13.605, this value may be best interpreted in context by either of the following:  

* "For each additional square foot of living space on a property, we estimate an increase of \$13.61 in the mean selling price, holding appraised land and improvement values fixed."
* "Each additional square foot of living space on a property results in an average increase of \$13.61 in the mean selling price, after adjusting for the appraised value of the land and improvements."

**In Summary:** When a predictor's effect on the response variable is assessed in a model that contains other predictor variables, that predictor's effect is said to be adjusted for the other predictors.

Now, suppose we delete `landvalue` (an insignificant predictor, $p$-value $>$ 0.05; we will cover this in more detail in the next chapter). How is the model affected?

```{r ch5-12}
summary(lm(saleprice ~ impvalue + area, data=appraisal))
```

Notice that the values of both $b_2$ and $b_3$ changed after the predictor $X_1$ (`landvalue`) was deleted from the model.
This means that our estimate of the effect that one predictor has on the response depends on what other predictors are in the model (compare this with the example of orthogonality from earlier).



## Why multiple regression?{#whymultiple}

We briefly tangent with a discussion of why multiple linear regression is superior to running "separate" simple linear regressions.

Because complex natural phenomena are typically impacted by many characteristics, it would be naive in most circumstances to think that just one variable serves as an adequate explanation for an outcome.
Instead, we consider the simultaneous impact of potential predictors of interest on the response.
Useful models reflect this fact.

The "one-predictor-at-a-time approach" can be quite bad.
Suppose you are considering three potential predictors $X_1$, $X_2$, $X_3$ on a response $Y$.
You might be tempted to fit three separate simple linear regression models to each predictor:

$$\begin{array}{c}
Y = \beta_0 + \beta_1 X_1 + \varepsilon \\
Y = \beta_0 + \beta_2 X_2 + \varepsilon \\
Y = \beta_0 + \beta_3 X_3 + \varepsilon
\end{array}$$

As we shall see, this approach to regression is fundamentally flawed and is to be avoided at all costs.
The problem is that if you fit "too simple" a model you will not account for the "collective" impact of multiple predictors of interest, you may then fail to detect significant relationships, or even come to completely wrong conclusions.
Below we provide two examples of the dangers of simple linear regression.

To fully understand these examples, you will need to review \@ref(inference-regarding-multiple-regression).
For now we highlight the individual regression $t$-test and residual error to demonstrate the limitations of simple linear regression.

### Example of Spurious Correlation

Here is an example to illustrate potential dangers of using a simple linear regression.

**Example.** Punting Ability in American Football (published in @Walpole2007)

In the game of American Football, a *punter* will kick a ball to the opposing team as far as possible towards the opponent's end zone, attempting to maximize the distance the receiving team must advance the ball in order to score. 
One key feature of a *good punt* is the "hang time," the amount of time the ball 'hangs' in the air before being caught by the punt returner.
An experiment was conducted were each of 13 punters kicked the ball 10 times and the experimenter recorded their average hang time and distances, along with other measures of strength and flexibility. 

* Punter_id - an identifier for each punter
* Hang_time - The average hangtim 
* Distance - 
* RLS - right leg strength (pounds)
* LLS - left leg strength (pounds)
* RHF - right hamstring muscle flexibility (degrees)
* LHF - left hamstring muscle flexibility (degrees)
* Power - Overall leg strength (foot-pounds)

Original Source: @VTpunting1983

The data is available in the file `puntingData.csv`.

```{r, eval=FALSE}
site <- "https://tjfisher19.github.io/introStatModeling/data/puntingData.csv"
punting <- read_csv(site, col_types=cols() )
glimpse(punting)
```

```{r ch5-13, echo=FALSE}
punting <- read_csv("docs/data/puntingData.csv", col_types=cols() )
glimpse(punting)
```

First note that this data was collected through an experiment, but the data is **observational** -- the experimenter did not control any of the variables of interest.

For our example, we will consider RLS, LLS and Power as potential predictor variables for hang time. 
Let us look at the scatterplot matrix of the data.


```{r ch5-14pairs, message=FALSE, fig.align='center', fig.cap='Pairs plot showing the relationship between the variables `RLS`, `LLS`, `Power` and `Hang_time`.', fig.asp=1, out.width='80%'}
ggpairs(punting, columns=c(4,5,8,2) )
```

Based on Figure \@ref(fig:ch5-14pairs) it appears that all three variables, RLS, LLS and Power, are all moderately or strongly correlated with the hang time.
Suppose we fit three simple linear regression models to the data.

```{r ch5-15}
slr_rls <- lm(Hang_time ~ RLS, data=punting)
slr_lls <- lm(Hang_time ~ LLS, data=punting)
slr_power <- lm(Hang_time ~ Power, data=punting)
```

Look at the summary output of each of the three models.

```{r ch5-16}
summary(slr_rls)
summary(slr_lls)
summary(slr_power)
```

Although we have not formally covered inference on regression, we might conclude from these results that each of the predictor variables influences hang time (see the very small $p$-value associated with the predictor variables).
The scatterplots (Figure \@ref(fig:ch5-14pairs)) seems to support this as well.

However, it would be erroneous to conclude that this relationship implies causation (a caution wisely applied to all observational data, as we will discuss later).
The sample of punters span a range of abilities, and each might utilize their left or right leg more in the act of kicking. 
So a logical question to ask is "how do all these variables, collectively, influence hang time?"

We fit a multiple regression predicting hang time from all three predictor variables.

```{r ch5-17}
mlr_punting <- lm(Hang_time ~ RLS + LLS + Power, data=punting)
summary(mlr_punting)
```

Compare this result to the simple regression models.
When all three variables are included, the variable RLS appears to be unimportant ($p$-value = 0.9723).
The earlier result occurred because both hang time and RLS are highly correlated with LLS – so the omission of LLS led us to detect what is known as a **spurious association** between RLS and hang time.
To phrase another way, LLS and RLS appear to be providing much of the same, or redundant, information to the model.
We may also consider the addition of LLS and Power to be *confounding* in regards to the variable RLS.

Further, we can see how the multiple regression model improves on the accuracy of the predicted values of the regression model.
Consider the table of residual standard errors below.

```{r ch5-18, echo=FALSE}
punting_error_df <- data.frame(
  Model = c("SLR: RLS", "SLR: LLS", "SLR: Power", "MLR: RLS+LLS+Power"),
  `Residual Standard Error` = c(summary(slr_rls)$sigma,summary(slr_lls)$sigma,summary(slr_power)$sigma,summary(mlr_punting)$sigma)
)

kbl(punting_error_df) %>%
  kable_styling(full_width=FALSE)
```

Note that the multiple regression model has arguably the 'best' (smallest) residual standard error of the four models considered -- even though it includes the variable RLS which appears to not be important when LLS and Power are included.

The addition of a multiple predictors into the model also alters the nature of the question being asked.
The simple linear regression asks: "Does RLS influence hang time?"
The multiple linear regression, however, asks a more useful question: "Does RLS influence hang time once any differences due to LHS and Power are considered?"
The answer is 'not really'. 
It also asks: "Does LLS influence hang time once any differences due to RLS and Power are accounted for?" 
The answer is 'yes'. 

### Example of Confounding in Regression

In the previous section, \@ref(example-of-spurious-correlation), we saw how a combination of variables can change the interpretation of a variable in the presence of others.
In Section \@ref(blocking) we discussed using *blocks* as a way to control for potential confounding factors, or nuisance variables.
By modeling this extra (in some cases *known*) variability, we can more accurately determine if the variables of interest influence the response variable.
In the below example we demonstrate that regression can be used in a similar way as a block design to control for nuisance variation.

**Example.** Bleaching Pulp (from @WeissText)

The production of paper requires the pupl used in the manufacturing process to be whitened by bleaching in a chemical reaction.
The bleaching agents are usually chlorine dioxide (ClO$_2$) and hydrogen peroxide (H$_2$O$_2$).
The amounts of these two chemical used in the process effects the whiteness of the pulp, and ultimately that of the paper.

Original source: @Zhou1998

The file `bleachingPulp.txt` on the textbook site contains the result of an experiment where 20 combinations of chlorine dioxide and hydrogen peroxide were varied and the *brightness* or *whiteness* of the paper was determined (smaller the number, the brighter the paper).
The file is a tabbed separated value format

```{r, eval=FALSE}
site <- "https://tjfisher19.github.io/introStatModeling/data/bleachingPulp.txt"
pulp <- read_tsv(site, col_type=cols())
glimpse(pulp)
```

```{r ch5-19, echo=FALSE}
pulp <- read_tsv("docs/data/bleachingPulp.txt", col_type=cols())
glimpse(pulp)
```

Suppose a paper scientist wished to know the effect hydrogen peroxide has on the brightening process.
The scientist fits a simple linear regression modeling the whiteness as a function of the amount of hydrogen peroxide.

```{r ch5-20}
slr_h2o2 <- lm(Whiteness ~ H2O2, data=pulp)
summary(slr_h2o2)
```

From the summary output it appears that the amount of hydrogen peroxide does not influence the brightnes of the paper.
Further, we note the residual standard error is `r round(summary(slr_h2o2)$sigma, 5)` which is not all that different than the standard deviation of the `Whiteness` variable, `r round(sd(pulp$Whiteness), 5)`.

However, the above analysis is **incorrect** because we do not account for the potential effect of the chlorine dioxide (a **confounding** variable) -- the experiment used both.
When we fit a multiple regression model we get a different result.

```{r ch5-21}
mlr_pulp <- lm(Whiteness ~ H2O2 + ClO2, data=pulp)
summary(mlr_pulp)
```

Based on the multiple regression model, it appears hydrogen peroxide does predict the brightness of the pulp.
Also note the decrease in the residual standard error.

For reference, the residual standard error for the simple linear regression with just chlorine dioxide is `r round(summary(lm(Whiteness~ClO2,data=pulp))$sigma, 5)`.
So it is apparent that both predictor variables provide a better mode than either simple linear regression approach.

## Concluding the Multiple Regression Model

Hopefully the examples outlined in this chapter will give you an appreciation for why we must strive to find a good model for our data, and **not** *fit once* and "hope for the best".
In observational studies, many different models may be fit to the same data, but that does not mean they are all good!
Findings and results can vary based on which model we choose: precision of our predictions, interpretations of the parameter estimates, etc., so we must use caution and wisdom in our choices.
In the following chapters we will cover methods that will help us in our model building.

As statistician George Box once said (@Box1979):

> "All models are wrong but some are useful."

