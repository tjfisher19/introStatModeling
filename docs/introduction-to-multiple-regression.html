<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 5 Introduction to Multiple Regression | Introduction to Statistical Modeling</title>
  <meta name="description" content="Covers Regression and elements of Design of Experiments in R using the tidyverse." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 5 Introduction to Multiple Regression | Introduction to Statistical Modeling" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Covers Regression and elements of Design of Experiments in R using the tidyverse." />
  <meta name="github-repo" content="tjfisher19/introStatModeling" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 5 Introduction to Multiple Regression | Introduction to Statistical Modeling" />
  
  <meta name="twitter:description" content="Covers Regression and elements of Design of Experiments in R using the tidyverse." />
  

<meta name="author" content="Michael R. Hughes and Thomas J. Fisher" />


<meta name="date" content="2022-01-18" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="advanced-designs.html"/>
<link rel="next" href="inference-regarding-multiple-regression.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Inroduction to Statistical Modeling</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="chapter" data-level="" data-path="important-preliminary-review.html"><a href="important-preliminary-review.html"><i class="fa fa-check"></i>Important Preliminary Review</a>
<ul>
<li class="chapter" data-level="" data-path="important-preliminary-review.html"><a href="important-preliminary-review.html#statistics-background"><i class="fa fa-check"></i>Statistics background</a></li>
<li class="chapter" data-level="" data-path="important-preliminary-review.html"><a href="important-preliminary-review.html#software"><i class="fa fa-check"></i>Software</a></li>
<li class="chapter" data-level="" data-path="important-preliminary-review.html"><a href="important-preliminary-review.html#add-on-packages"><i class="fa fa-check"></i>Add-on packages</a></li>
<li class="chapter" data-level="" data-path="important-preliminary-review.html"><a href="important-preliminary-review.html#help-with-rmarkdown"><i class="fa fa-check"></i>Help with RMarkdown</a></li>
<li class="chapter" data-level="" data-path="important-preliminary-review.html"><a href="important-preliminary-review.html#managing-your-work-in-r"><i class="fa fa-check"></i>Managing your work in R</a></li>
<li class="chapter" data-level="" data-path="important-preliminary-review.html"><a href="important-preliminary-review.html#data-in-this-text"><i class="fa fa-check"></i>Data in this text</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html"><i class="fa fa-check"></i><b>1</b> Introductory Statistics in R</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#goals-of-a-statistical-analysis"><i class="fa fa-check"></i><b>1.1</b> Goals of a statistical analysis</a></li>
<li class="chapter" data-level="1.2" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#before-you-begin-an-analysis"><i class="fa fa-check"></i><b>1.2</b> Before you begin an analysis</a></li>
<li class="chapter" data-level="1.3" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#data-frames"><i class="fa fa-check"></i><b>1.3</b> Data frames</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#built-in-data"><i class="fa fa-check"></i><b>1.3.1</b> Built-in data</a></li>
<li class="chapter" data-level="1.3.2" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#types-of-data"><i class="fa fa-check"></i><b>1.3.2</b> Types of Data</a></li>
<li class="chapter" data-level="1.3.3" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#importing-datasets-into-r"><i class="fa fa-check"></i><b>1.3.3</b> Importing datasets into R</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#referencing-data-from-inside-a-data-frame"><i class="fa fa-check"></i><b>1.4</b> Referencing data from inside a data frame</a></li>
<li class="chapter" data-level="1.5" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#missing-values-and-computer-arithmetic-in-r"><i class="fa fa-check"></i><b>1.5</b> Missing values and computer arithmetic in R</a></li>
<li class="chapter" data-level="1.6" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#exploratory-data-analysis-eda"><i class="fa fa-check"></i><b>1.6</b> Exploratory Data Analysis (EDA)</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#numeric-summaries"><i class="fa fa-check"></i><b>1.6.1</b> Numeric Summaries</a></li>
<li class="chapter" data-level="1.6.2" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#numeric-summaries-in-r"><i class="fa fa-check"></i><b>1.6.2</b> Numeric Summaries in R</a></li>
<li class="chapter" data-level="1.6.3" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#graphical-summaries"><i class="fa fa-check"></i><b>1.6.3</b> Graphical Summaries</a></li>
<li class="chapter" data-level="1.6.4" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#distribution-of-univariate-variables"><i class="fa fa-check"></i><b>1.6.4</b> Distribution of Univariate Variables</a></li>
<li class="chapter" data-level="1.6.5" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#descriptive-statistics-and-visualizations-by-levels-of-a-factor-variable"><i class="fa fa-check"></i><b>1.6.5</b> Descriptive statistics and visualizations by levels of a factor variable</a></li>
<li class="chapter" data-level="1.6.6" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#descriptive-statistics-and-visualizations-for-two-numeric-variables"><i class="fa fa-check"></i><b>1.6.6</b> Descriptive statistics and visualizations for two numeric variables</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#sampling-distributions-describing-how-a-statistic-varies"><i class="fa fa-check"></i><b>1.7</b> Sampling distributions: describing how a statistic varies</a></li>
<li class="chapter" data-level="1.8" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#two-sample-inference"><i class="fa fa-check"></i><b>1.8</b> Two-sample inference</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html"><i class="fa fa-check"></i><b>2</b> Introduction to Statistical Modeling and Designed Experiments</a>
<ul>
<li class="chapter" data-level="2.1" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#statistical-analyses-is-modeling"><i class="fa fa-check"></i><b>2.1</b> Statistical Analyses is Modeling</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#example-of-a-two-sample-t-test-as-a-model"><i class="fa fa-check"></i><b>2.1.1</b> Example of a two-sample <span class="math inline">\(t\)</span>-test as a model</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#observational-studies-versus-designed-experiments"><i class="fa fa-check"></i><b>2.2</b> Observational Studies versus designed experiments</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#observational-studies"><i class="fa fa-check"></i><b>2.2.1</b> Observational Studies</a></li>
<li class="chapter" data-level="2.2.2" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#designed-experiments"><i class="fa fa-check"></i><b>2.2.2</b> Designed experiments</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#designed-experiement-vocabulary"><i class="fa fa-check"></i><b>2.3</b> Designed experiement vocabulary</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#what-is-an-experiment"><i class="fa fa-check"></i><b>2.3.1</b> What is an experiment?</a></li>
<li class="chapter" data-level="2.3.2" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#analysis-of-variance"><i class="fa fa-check"></i><b>2.3.2</b> Analysis of variance</a></li>
<li class="chapter" data-level="2.3.3" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#elements-of-a-designed-experiment"><i class="fa fa-check"></i><b>2.3.3</b> Elements of a designed experiment</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#paired-t-test"><i class="fa fa-check"></i><b>2.4</b> Paired <span class="math inline">\(t\)</span>-test</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#paired-t-test-example"><i class="fa fa-check"></i><b>2.4.1</b> Paired <span class="math inline">\(t\)</span>-test example</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#one-way-anova"><i class="fa fa-check"></i><b>2.5</b> One-Way ANOVA</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#example-of-one-way-anova"><i class="fa fa-check"></i><b>2.5.1</b> Example of One-Way ANOVA</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#assumptionCheck"><i class="fa fa-check"></i><b>2.6</b> Assumption Checking</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#independence"><i class="fa fa-check"></i><b>2.6.1</b> Independence</a></li>
<li class="chapter" data-level="2.6.2" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#constant-variance"><i class="fa fa-check"></i><b>2.6.2</b> Constant Variance</a></li>
<li class="chapter" data-level="2.6.3" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#checking-normality"><i class="fa fa-check"></i><b>2.6.3</b> Checking Normality</a></li>
<li class="chapter" data-level="2.6.4" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#code-to-check-assumptions"><i class="fa fa-check"></i><b>2.6.4</b> Code to check assumptions</a></li>
<li class="chapter" data-level="2.6.5" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#transforming-your-response"><i class="fa fa-check"></i><b>2.6.5</b> Transforming your response</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#follow-up-procedures-multiple-comparisons"><i class="fa fa-check"></i><b>2.7</b> Follow-up procedures – Multiple Comparisons</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#tukeys-hsd-method"><i class="fa fa-check"></i><b>2.7.1</b> Tukey’s HSD method</a></li>
<li class="chapter" data-level="2.7.2" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#dunnett-multiple-comparisons"><i class="fa fa-check"></i><b>2.7.2</b> Dunnett multiple comparisons</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="multiple-factor-designed-experiments.html"><a href="multiple-factor-designed-experiments.html"><i class="fa fa-check"></i><b>3</b> Multiple Factor Designed Experiments</a>
<ul>
<li class="chapter" data-level="3.1" data-path="multiple-factor-designed-experiments.html"><a href="multiple-factor-designed-experiments.html#blocking"><i class="fa fa-check"></i><b>3.1</b> Blocking</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="multiple-factor-designed-experiments.html"><a href="multiple-factor-designed-experiments.html#data-structure-model-form-and-analysis-of-variance-of-a-randomized-block-design"><i class="fa fa-check"></i><b>3.1.1</b> Data structure, model form and analysis of variance of a Randomized Block Design</a></li>
<li class="chapter" data-level="3.1.2" data-path="multiple-factor-designed-experiments.html"><a href="multiple-factor-designed-experiments.html#block-anova-example"><i class="fa fa-check"></i><b>3.1.2</b> Block ANOVA Example</a></li>
<li class="chapter" data-level="3.1.3" data-path="multiple-factor-designed-experiments.html"><a href="multiple-factor-designed-experiments.html#another-block-anova-example"><i class="fa fa-check"></i><b>3.1.3</b> Another Block ANOVA Example</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="multiple-factor-designed-experiments.html"><a href="multiple-factor-designed-experiments.html#two-factor-designs"><i class="fa fa-check"></i><b>3.2</b> Two-factor Designs</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="multiple-factor-designed-experiments.html"><a href="multiple-factor-designed-experiments.html#analysis-of-a-two-factor-design"><i class="fa fa-check"></i><b>3.2.1</b> Analysis of a two-factor design</a></li>
<li class="chapter" data-level="3.2.2" data-path="multiple-factor-designed-experiments.html"><a href="multiple-factor-designed-experiments.html#example-with-no-interaction"><i class="fa fa-check"></i><b>3.2.2</b> Example with No Interaction</a></li>
<li class="chapter" data-level="3.2.3" data-path="multiple-factor-designed-experiments.html"><a href="multiple-factor-designed-experiments.html#example-with-interaction"><i class="fa fa-check"></i><b>3.2.3</b> Example with Interaction</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="advanced-designs.html"><a href="advanced-designs.html"><i class="fa fa-check"></i><b>4</b> Advanced Designs</a>
<ul>
<li class="chapter" data-level="4.1" data-path="advanced-designs.html"><a href="advanced-designs.html#higher-order-factor-models"><i class="fa fa-check"></i><b>4.1</b> Higher order factor models</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="advanced-designs.html"><a href="advanced-designs.html#example-of-three-factor-design"><i class="fa fa-check"></i><b>4.1.1</b> Example of Three-factor design</a></li>
<li class="chapter" data-level="4.1.2" data-path="advanced-designs.html"><a href="advanced-designs.html#three-factor-anova-example"><i class="fa fa-check"></i><b>4.1.2</b> Three-factor ANOVA Example</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="advanced-designs.html"><a href="advanced-designs.html#within-subject-designs"><i class="fa fa-check"></i><b>4.2</b> Within-subject designs</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="advanced-designs.html"><a href="advanced-designs.html#blocks-revisited-an-approach-to-handling-within-subjects-factors"><i class="fa fa-check"></i><b>4.2.1</b> Blocks revisited: an approach to handling within-subjects factors</a></li>
<li class="chapter" data-level="4.2.2" data-path="advanced-designs.html"><a href="advanced-designs.html#a-more-involved-repeated-measures-case-study"><i class="fa fa-check"></i><b>4.2.2</b> A more involved repeated measures case study</a></li>
<li class="chapter" data-level="4.2.3" data-path="advanced-designs.html"><a href="advanced-designs.html#further-study"><i class="fa fa-check"></i><b>4.2.3</b> Further study</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html"><i class="fa fa-check"></i><b>5</b> Introduction to Multiple Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#regression-model"><i class="fa fa-check"></i><b>5.1</b> Regression Model</a></li>
<li class="chapter" data-level="5.2" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#fitting-a-regression-model"><i class="fa fa-check"></i><b>5.2</b> Fitting a regression model</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#simple-linear-regression-example"><i class="fa fa-check"></i><b>5.2.1</b> Simple Linear Regression Example</a></li>
<li class="chapter" data-level="5.2.2" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#multiple-regression-example"><i class="fa fa-check"></i><b>5.2.2</b> Multiple Regression Example</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#interpreting-beta-parameter-estimates-in-mlr"><i class="fa fa-check"></i><b>5.3</b> Interpreting <span class="math inline">\(\beta\)</span>-parameter estimates in MLR</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#designed-experiments-1"><i class="fa fa-check"></i><b>5.3.1</b> Designed experiments</a></li>
<li class="chapter" data-level="5.3.2" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#example-of-orthogonal-design"><i class="fa fa-check"></i><b>5.3.2</b> Example of Orthogonal Design</a></li>
<li class="chapter" data-level="5.3.3" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#observational-studies-1"><i class="fa fa-check"></i><b>5.3.3</b> Observational studies</a></li>
<li class="chapter" data-level="5.3.4" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#example-of-observational-regression-interpretation"><i class="fa fa-check"></i><b>5.3.4</b> Example of observational regression interpretation</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#whymultiple"><i class="fa fa-check"></i><b>5.4</b> Why multiple regression?</a>
<ul>
<li class="chapter" data-level="5.4.1" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#example-of-spurious-correlation"><i class="fa fa-check"></i><b>5.4.1</b> Example of Spurious Correlation</a></li>
<li class="chapter" data-level="5.4.2" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#example-of-confounding-in-regression"><i class="fa fa-check"></i><b>5.4.2</b> Example of Confounding in Regression</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#concluding-the-multiple-regression-model"><i class="fa fa-check"></i><b>5.5</b> Concluding the Multiple Regression Model</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html"><i class="fa fa-check"></i><b>6</b> Inference regarding Multiple Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#assumption-checking"><i class="fa fa-check"></i><b>6.1</b> Assumption checking</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#example-of-assumption-checking-in-slr"><i class="fa fa-check"></i><b>6.1.1</b> Example of Assumption Checking in SLR</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#overall-f-test-for-model-signifance"><i class="fa fa-check"></i><b>6.2</b> Overall <span class="math inline">\(F\)</span>-test for model signifance</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#example-of-model-overall-f-test"><i class="fa fa-check"></i><b>6.2.1</b> Example of model overall <span class="math inline">\(F\)</span>-test</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#individual-parameter-inference"><i class="fa fa-check"></i><b>6.3</b> Individual parameter inference</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#parameter-t-tests"><i class="fa fa-check"></i><b>6.3.1</b> Parameter <span class="math inline">\(t\)</span>-tests</a></li>
<li class="chapter" data-level="6.3.2" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#example-of-individual-parameter-t-test"><i class="fa fa-check"></i><b>6.3.2</b> Example of individual parameter <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="6.3.3" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#model-trimming"><i class="fa fa-check"></i><b>6.3.3</b> Model trimming</a></li>
<li class="chapter" data-level="6.3.4" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#confidence-intervals"><i class="fa fa-check"></i><b>6.3.4</b> Confidence Intervals</a></li>
<li class="chapter" data-level="6.3.5" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#example-of-individual-parameter-confidence-intervals"><i class="fa fa-check"></i><b>6.3.5</b> Example of individual parameter confidence intervals</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#confidence-and-prediction-bands"><i class="fa fa-check"></i><b>6.4</b> Confidence and prediction bands</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#simple-linear-regression-confident-and-prediction-bands"><i class="fa fa-check"></i><b>6.4.1</b> Simple Linear Regression Confident and Prediction bands</a></li>
<li class="chapter" data-level="6.4.2" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#example-of-confidence-and-prediction-intervals"><i class="fa fa-check"></i><b>6.4.2</b> Example of confidence and prediction intervals</a></li>
<li class="chapter" data-level="6.4.3" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#extrapolation-warning"><i class="fa fa-check"></i><b>6.4.3</b> Extrapolation warning!</a></li>
<li class="chapter" data-level="6.4.4" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#confidence-and-prediction-bands-in-multiple-regression"><i class="fa fa-check"></i><b>6.4.4</b> Confidence and Prediction Bands in Multiple Regression</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#goodness-of-fit"><i class="fa fa-check"></i><b>6.5</b> Goodness-of-fit</a>
<ul>
<li class="chapter" data-level="6.5.1" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#coefficient-of-determination"><i class="fa fa-check"></i><b>6.5.1</b> Coefficient of determination</a></li>
<li class="chapter" data-level="6.5.2" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#akaikes-information-criterion"><i class="fa fa-check"></i><b>6.5.2</b> Akaike’s Information Criterion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="more-on-multiple-linear-regression.html"><a href="more-on-multiple-linear-regression.html"><i class="fa fa-check"></i><b>7</b> More on multiple linear regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="more-on-multiple-linear-regression.html"><a href="more-on-multiple-linear-regression.html#model-comparision-reduced-f-tests"><i class="fa fa-check"></i><b>7.1</b> Model comparision – Reduced <span class="math inline">\(F\)</span>-tests</a></li>
<li class="chapter" data-level="7.2" data-path="more-on-multiple-linear-regression.html"><a href="more-on-multiple-linear-regression.html#categorical-predictor-variables"><i class="fa fa-check"></i><b>7.2</b> Categorical Predictor Variables</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="more-on-multiple-linear-regression.html"><a href="more-on-multiple-linear-regression.html#a-qualitative-predictor-with-two-levels"><i class="fa fa-check"></i><b>7.2.1</b> A qualitative predictor with two levels</a></li>
<li class="chapter" data-level="7.2.2" data-path="more-on-multiple-linear-regression.html"><a href="more-on-multiple-linear-regression.html#a-qualitative-predictor-with-more-than-two-levels"><i class="fa fa-check"></i><b>7.2.2</b> A qualitative predictor with more than two levels</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="more-on-multiple-linear-regression.html"><a href="more-on-multiple-linear-regression.html#bridging-regression-and-designed-experiments-ancova"><i class="fa fa-check"></i><b>7.3</b> Bridging Regression and Designed Experiments – ANCOVA</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="more-on-multiple-linear-regression.html"><a href="more-on-multiple-linear-regression.html#an-ancova-example-with-a-two-level-factor"><i class="fa fa-check"></i><b>7.3.1</b> An ANCOVA example with a two-level factor</a></li>
<li class="chapter" data-level="7.3.2" data-path="more-on-multiple-linear-regression.html"><a href="more-on-multiple-linear-regression.html#ancova-with-a-multi-level-factor"><i class="fa fa-check"></i><b>7.3.2</b> ANCOVA with a multi-level factor</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="model-building-considerations.html"><a href="model-building-considerations.html"><i class="fa fa-check"></i><b>8</b> Model Building Considerations</a>
<ul>
<li class="chapter" data-level="8.1" data-path="model-building-considerations.html"><a href="model-building-considerations.html#regression-assumptions-revisited"><i class="fa fa-check"></i><b>8.1</b> Regression assumptions revisited</a></li>
<li class="chapter" data-level="8.2" data-path="model-building-considerations.html"><a href="model-building-considerations.html#violations-of-the-independence-assumption"><i class="fa fa-check"></i><b>8.2</b> Violations of the independence assumption</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="model-building-considerations.html"><a href="model-building-considerations.html#collecting-data-that-are-temporal-or-spatial-in-nature"><i class="fa fa-check"></i><b>8.2.1</b> Collecting data that are temporal or spatial in nature</a></li>
<li class="chapter" data-level="8.2.2" data-path="model-building-considerations.html"><a href="model-building-considerations.html#pseudoreplication"><i class="fa fa-check"></i><b>8.2.2</b> Pseudoreplication</a></li>
<li class="chapter" data-level="8.2.3" data-path="model-building-considerations.html"><a href="model-building-considerations.html#what-if-we-have-non-independent-errors"><i class="fa fa-check"></i><b>8.2.3</b> What if we have non-independent errors?</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="model-building-considerations.html"><a href="model-building-considerations.html#constant-variance-violations"><i class="fa fa-check"></i><b>8.3</b> Constant Variance Violations</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="model-building-considerations.html"><a href="model-building-considerations.html#box-cox-power-tranformations"><i class="fa fa-check"></i><b>8.3.1</b> Box-Cox Power Tranformations</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="model-building-considerations.html"><a href="model-building-considerations.html#normality-violations"><i class="fa fa-check"></i><b>8.4</b> Normality violations</a></li>
<li class="chapter" data-level="8.5" data-path="model-building-considerations.html"><a href="model-building-considerations.html#violations-of-the-linearity-assumption"><i class="fa fa-check"></i><b>8.5</b> Violations of the linearity assumption</a></li>
<li class="chapter" data-level="8.6" data-path="model-building-considerations.html"><a href="model-building-considerations.html#detecting-and-dealing-with-unusual-observations"><i class="fa fa-check"></i><b>8.6</b> Detecting and dealing with unusual observations</a></li>
<li class="chapter" data-level="8.7" data-path="model-building-considerations.html"><a href="model-building-considerations.html#multicollinearity"><i class="fa fa-check"></i><b>8.7</b> Multicollinearity</a></li>
<li class="chapter" data-level="8.8" data-path="model-building-considerations.html"><a href="model-building-considerations.html#standardizingPredictors"><i class="fa fa-check"></i><b>8.8</b> Scale changes</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="model-selection.html"><a href="model-selection.html"><i class="fa fa-check"></i><b>9</b> Model Selection</a>
<ul>
<li class="chapter" data-level="9.1" data-path="model-selection.html"><a href="model-selection.html#stepwise-procedures"><i class="fa fa-check"></i><b>9.1</b> Stepwise Procedures</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="model-selection.html"><a href="model-selection.html#backward-selection"><i class="fa fa-check"></i><b>9.1.1</b> Backward Selection</a></li>
<li class="chapter" data-level="9.1.2" data-path="model-selection.html"><a href="model-selection.html#forward-selection"><i class="fa fa-check"></i><b>9.1.2</b> Forward selection</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="model-selection.html"><a href="model-selection.html#best-subsets"><i class="fa fa-check"></i><b>9.2</b> Best subsets</a></li>
<li class="chapter" data-level="9.3" data-path="model-selection.html"><a href="model-selection.html#shrinkage-methods"><i class="fa fa-check"></i><b>9.3</b> Shrinkage Methods</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="model-validation.html"><a href="model-validation.html"><i class="fa fa-check"></i><b>10</b> Model Validation</a>
<ul>
<li class="chapter" data-level="10.1" data-path="model-validation.html"><a href="model-validation.html#underfitting-vs.-overfitting-models"><i class="fa fa-check"></i><b>10.1</b> Underfitting vs. Overfitting Models</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="model-validation.html"><a href="model-validation.html#the-bias-variance-trade-off"><i class="fa fa-check"></i><b>10.1.1</b> The Bias-Variance Trade-off</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="model-validation.html"><a href="model-validation.html#validation-techniques"><i class="fa fa-check"></i><b>10.2</b> Validation Techniques</a></li>
<li class="chapter" data-level="10.3" data-path="model-validation.html"><a href="model-validation.html#basic-validation-with-a-single-holdout-sample"><i class="fa fa-check"></i><b>10.3</b> Basic Validation with a single holdout sample</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="model-validation.html"><a href="model-validation.html#use-the-training-data-to-fit-and-select-models"><i class="fa fa-check"></i><b>10.3.1</b> Use the training data to fit and select models</a></li>
<li class="chapter" data-level="10.3.2" data-path="model-validation.html"><a href="model-validation.html#model-training"><i class="fa fa-check"></i><b>10.3.2</b> Model training:</a></li>
<li class="chapter" data-level="10.3.3" data-path="model-validation.html"><a href="model-validation.html#model-validation-step"><i class="fa fa-check"></i><b>10.3.3</b> Model validation step:</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="model-validation.html"><a href="model-validation.html#hold-out-sample-validation-using-caret"><i class="fa fa-check"></i><b>10.4</b> Hold-out sample validation using <code>caret</code></a></li>
<li class="chapter" data-level="10.5" data-path="model-validation.html"><a href="model-validation.html#leave-one-out-cross-validation-loocv"><i class="fa fa-check"></i><b>10.5</b> “Leave one out” Cross-Validation (LOOCV)</a></li>
<li class="chapter" data-level="10.6" data-path="model-validation.html"><a href="model-validation.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>10.6</b> <span class="math inline">\(k\)</span>-fold Cross-Validation</a></li>
<li class="chapter" data-level="10.7" data-path="model-validation.html"><a href="model-validation.html#a-final-note"><i class="fa fa-check"></i><b>10.7</b> A final note</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="statistical-odds.html"><a href="statistical-odds.html"><i class="fa fa-check"></i><b>11</b> Statistical Odds</a>
<ul>
<li class="chapter" data-level="11.1" data-path="statistical-odds.html"><a href="statistical-odds.html#probability-versus-odds"><i class="fa fa-check"></i><b>11.1</b> Probability versus Odds</a></li>
<li class="chapter" data-level="11.2" data-path="statistical-odds.html"><a href="statistical-odds.html#odds-ratios"><i class="fa fa-check"></i><b>11.2</b> Odds ratios</a></li>
<li class="chapter" data-level="11.3" data-path="statistical-odds.html"><a href="statistical-odds.html#ideas-of-modeling-odds"><i class="fa fa-check"></i><b>11.3</b> Ideas of modeling odds</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>12</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="12.1" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-model"><i class="fa fa-check"></i><b>12.1</b> Logistic Model</a></li>
<li class="chapter" data-level="12.2" data-path="logistic-regression.html"><a href="logistic-regression.html#fitting-interpreting-and-assessing-a-logistic-model"><i class="fa fa-check"></i><b>12.2</b> Fitting, Interpreting and assessing a logistic model</a></li>
<li class="chapter" data-level="12.3" data-path="logistic-regression.html"><a href="logistic-regression.html#case-study---titanic-dataset"><i class="fa fa-check"></i><b>12.3</b> Case Study - Titanic Dataset</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>13</b> Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="13.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#poisson-regression"><i class="fa fa-check"></i><b>13.1</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#poisson-distribution"><i class="fa fa-check"></i><b>13.1.1</b> Poisson distribution</a></li>
<li class="chapter" data-level="13.1.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#poisson-regression-development"><i class="fa fa-check"></i><b>13.1.2</b> Poisson Regression Development</a></li>
<li class="chapter" data-level="13.1.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#example---tropical-cyclone-counts-in-the-north-atlantic"><i class="fa fa-check"></i><b>13.1.3</b> Example - Tropical Cyclone Counts in the North Atlantic</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#handling-overdispersion"><i class="fa fa-check"></i><b>13.2</b> Handling overdispersion</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#example-attendnace-records"><i class="fa fa-check"></i><b>13.2.1</b> Example – Attendnace Records</a></li>
<li class="chapter" data-level="13.2.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#incorrect-poisson-model"><i class="fa fa-check"></i><b>13.2.2</b> Incorrect Poisson Model</a></li>
<li class="chapter" data-level="13.2.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#a-quasi-poisson-approach"><i class="fa fa-check"></i><b>13.2.3</b> A quasi-Poisson approach</a></li>
<li class="chapter" data-level="13.2.4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#fitting-a-negative-binomial-regression"><i class="fa fa-check"></i><b>13.2.4</b> Fitting a Negative Binomial regression</a></li>
<li class="chapter" data-level="13.2.5" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#picking-between-quasi-poisson-and-negative-binomial"><i class="fa fa-check"></i><b>13.2.5</b> Picking between Quasi-Poisson and Negative Binomial</a></li>
<li class="chapter" data-level="13.2.6" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#infererence-on-predictor-variables"><i class="fa fa-check"></i><b>13.2.6</b> Infererence on predictor variables</a></li>
<li class="chapter" data-level="13.2.7" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#plotting-fitted-model"><i class="fa fa-check"></i><b>13.2.7</b> Plotting fitted model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Statistical Modeling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="introduction-to-multiple-regression" class="section level1" number="5">
<h1><span class="header-section-number">Chapter 5</span> Introduction to Multiple Regression</h1>
<p>In the early chapters of our text we observed that everything varies.
In a statistical model, the goal is to identify structure in the variation that the data possess.
This means that we must partition the variation in the data into (1) a systematic component, and (2) a non-systematic, or random, component.
After this unit, you should be able to</p>
<ul>
<li>Recognize the structure of a multiple regression model.</li>
<li>Fit a regression model in R.</li>
<li>Interpret the coefficients in a regression model.</li>
<li>Distinguish between the model constructed in design of experiments (ANOVA) and that in regression.</li>
</ul>
<p>In ANOVA testing, the systematic component is comprised of measured factors of research interest that may or may not relate to the response variable.
The random component is usually a catch-all for everything else: if the model is built well, there should be no systematic information left in the random component: rather, it should only contain random fluctuations due to the natural inherent variability in the measurements.</p>
<p>Suppose instead of factors (categorical inputs) we have measured predictor variables.
For example, consider the admissions data from Chapter 1 of this text; there may be a systematic tendency to see higher freshman GPAs from students with higher ACT scores.
Both variables are measured numeric values (compared to pre-determined treatments).
This leads to the idea of regression modeling.</p>
<div id="regression-model" class="section level2" number="5.1">
<h2><span class="header-section-number">5.1</span> Regression Model</h2>
<p>The general form of a multiple linear regression (MLR) model is</p>
<p><span class="math display">\[Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_k X_k + \varepsilon\]</span>
where <span class="math inline">\(Y\)</span> is the response variable, <span class="math inline">\(X_j\)</span> is the <span class="math inline">\(j^\mathrm{th}\)</span> predictor variable with coefficient <span class="math inline">\(\beta_j\)</span> and <span class="math inline">\(\varepsilon\)</span> is the unexplained random error.</p>
<p>Note that if <span class="math inline">\(\beta_1=\beta_2=\ldots=\beta_k=0\)</span> we reduce to our baseline model</p>
<p><span class="math display">\[Y=\beta_0 + \varepsilon = \mu + \varepsilon\]</span>
we saw earlier in the class.
The multiple regression model is a generalization of a <span class="math inline">\(k\)</span> factor ANOVA model but instead of categorical inputs, we have numeric, or quantitative, inputs.</p>
<p><strong>Simple Linear Regression.</strong> A special case of the above model occurs when <span class="math inline">\(k=1\)</span> and the model reduces to
<span class="math display">\[Y = \beta_0 + \beta_1 X + \varepsilon\]</span></p>
<p>This is known as the simple linear regression (SLR) model (covered in your Intro Statistics course).
It is very rare that a practicing Statistician will ever fit a SLR but we will utilize it to explain some key concepts.</p>
<p>It should be noted that multiple regression is in general a <span class="math inline">\(k + 1\)</span> dimensional problem, so it will usually not be feasible to graphically visualize a fitted model like we can with SLR (which was 2-dimensional).
Not to worry though, as we can quantitatively explain what is going on in higher dimensions.
In the following sections will we utilize SLR to help visualize important statistical concepts.</p>
<p>The <strong>goals of linear regression</strong> are:</p>
<ul>
<li>Formal assessment of the impact of the predictor variables on the response.</li>
<li>Prediction of future responses.</li>
</ul>
<p>These two goals are fundamentally different and may require different techniques to build a model.
We outline the fundamental concepts and statistical methods over the next six chapters.</p>
</div>
<div id="fitting-a-regression-model" class="section level2" number="5.2">
<h2><span class="header-section-number">5.2</span> Fitting a regression model</h2>
<p>Regression models are typically estimated through the method of least squares.
For the sake of visualizing the concept of least squares, we will consider a SLR example.</p>
<div id="simple-linear-regression-example" class="section level3" number="5.2.1">
<h3><span class="header-section-number">5.2.1</span> Simple Linear Regression Example</h3>
<p><strong>Example.</strong> Muscle Mass with Age (originally in <span class="citation"><a href="#ref-KutnerText" role="doc-biblioref">Kutner et al.</a> (<a href="#ref-KutnerText" role="doc-biblioref">2004</a>)</span>).</p>
<p>A person’s muscle mass is expected to decrease with age.
To explore this relationship in women, a nutritionist randomly selected 15 women from each 10-year age group beginning with age 40 and ending with age 79.
The data reside in the file <code>musclemass.txt</code>.
The variables in the dataset of interest are <code>mass</code> and <code>age</code>.</p>
<p>First input the data and take a look at the observations.</p>
<div class="sourceCode" id="cb192"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb192-1"><a href="introduction-to-multiple-regression.html#cb192-1" aria-hidden="true" tabindex="-1"></a>site <span class="ot">&lt;-</span> <span class="st">&quot;https://tjfisher19.github.io/introStatModeling/data/musclemass.txt&quot;</span></span>
<span id="cb192-2"><a href="introduction-to-multiple-regression.html#cb192-2" aria-hidden="true" tabindex="-1"></a>muscle <span class="ot">&lt;-</span> <span class="fu">read_table</span>(site, <span class="at">col_type=</span><span class="fu">cols</span>())</span>
<span id="cb192-3"><a href="introduction-to-multiple-regression.html#cb192-3" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(muscle)</span></code></pre></div>
<pre><code>## Rows: 60
## Columns: 2
## $ mass &lt;dbl&gt; 106, 106, 97, 113, 96, 119, 92, 112, 92, 102, 107, 107, 102, 115,~
## $ age  &lt;dbl&gt; 43, 41, 47, 46, 45, 41, 47, 41, 48, 48, 42, 47, 43, 44, 42, 55, 5~</code></pre>
<p>Figure <a href="introduction-to-multiple-regression.html#fig:ch5-1">5.1</a> below displays the data.</p>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb194-1"><a href="introduction-to-multiple-regression.html#cb194-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(muscle) <span class="sc">+</span> </span>
<span id="cb194-2"><a href="introduction-to-multiple-regression.html#cb194-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x=</span>age,<span class="at">y=</span>mass)) <span class="sc">+</span></span>
<span id="cb194-3"><a href="introduction-to-multiple-regression.html#cb194-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">title=</span><span class="st">&quot;Muscle Mass vs Age&quot;</span>,</span>
<span id="cb194-4"><a href="introduction-to-multiple-regression.html#cb194-4" aria-hidden="true" tabindex="-1"></a>       <span class="at">x=</span><span class="st">&quot;Age (years)&quot;</span>, <span class="st">&quot;Muscle Mass (kg)&quot;</span>) <span class="sc">+</span> </span>
<span id="cb194-5"><a href="introduction-to-multiple-regression.html#cb194-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch5-1"></span>
<img src="introStatModeling_files/figure-html/ch5-1-1.png" alt="Scatterplot showing the relationship between age and muscle mass in a selection of women aged 40 to 79." width="80%" />
<p class="caption">
Figure 5.1: Scatterplot showing the relationship between age and muscle mass in a selection of women aged 40 to 79.
</p>
</div>
<p>We can clearly see the negative trend one would expect: as age increases, muscle mass tends to decrease.
You should also notice that it decreases in a roughly linear fashion, so it makes sense to fit a simple linear regression model to this data.</p>
<p>The systematic component of a simple linear regression model passes a straight line through the data in an attempt to explain the linear trend (see Figure <a href="introduction-to-multiple-regression.html#fig:ch5-2">5.2</a>).
We can see that such a line effectively explains the trend, but it does not explain it perfectly since the line does not touch all the observed values.
The random fluctuations around the trend line are what the <span class="math inline">\(\varepsilon\)</span> terms account for in the model.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch5-2"></span>
<img src="introStatModeling_files/figure-html/ch5-2-1.png" alt="Scatterplot showing the relationship between age and muscle mass in a selection of women aged 40 to 79 with a fitted overlayed simple linear regression line." width="80%" />
<p class="caption">
Figure 5.2: Scatterplot showing the relationship between age and muscle mass in a selection of women aged 40 to 79 with a fitted overlayed simple linear regression line.
</p>
</div>
<p>The next goal is to somehow find the best fitting line for this data.
There are an infinite number of possible straight line models of the form <span class="math inline">\(Y = \beta_0 + \beta_1 X + \varepsilon\)</span> that we could fit to a data set, depending on the values of the slope <span class="math inline">\(\beta_1\)</span> and y-intercept <span class="math inline">\(\beta_0\)</span> of the line.
Given a scatterplot, how do we determine which slope and y-intercept produces the “best fitting” line for a given data set?
Well, first we need to define what we mean by “best.”</p>
<p>Our criterion for finding the best fitting line is rooted in the residuals that the line would produce.
In the two-dimensional simple linear regression case, it is easy to visualize this.
When a straight line is “fit” to a data set, the fitted (or “predicted”) values for each observation fall on the fitted line (see Figure <a href="introduction-to-multiple-regression.html#fig:ch5-3">5.3</a>).
However, the actual observed values randomly scatter around the line.<br />
The vertical discrepancies between the observed and predicted values are the residuals we spoke of earlier.
We can visualize this by zooming into the plot.</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch5-3"></span>
<img src="introStatModeling_files/figure-html/ch5-3-1.png" alt="Zoomed in view of fitted regression line modeling the relationship of age and mass muscle mass, highlighting the residuals of the fit." width="80%" />
<p class="caption">
Figure 5.3: Zoomed in view of fitted regression line modeling the relationship of age and mass muscle mass, highlighting the residuals of the fit.
</p>
</div>
<p>It makes some logical sense to use a criterion that somehow collectively minimizes these residuals, since the best fitting line should be the one that most closely passes through the observed data.
We need to estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> for this “best” line.</p>
<p>Also note in the Figure <a href="introduction-to-multiple-regression.html#fig:ch5-3">5.3</a> that any reasonable candidate model must pass through the data, producing both positive residuals (actual response values <span class="math inline">\(&gt;\)</span> fitted response values) and negative residuals (actual response values <span class="math inline">\(&lt;\)</span> fitted response values).
When we collectively assess the residuals, we do not want the positive ones to cancel out or offset the negative ones, so our criterion will be to minimize the sum of squared residuals (thus all are positive).
This brings us to what is known as the method of least squares (LS), which is outlined below in the context of simple linear regression.</p>
<p><strong>Method of Least Squares</strong></p>
<p>We propose to “fit” the model <span class="math inline">\(Y = \beta_0 + \beta_1 X + \varepsilon\)</span> to a data set of <span class="math inline">\(n\)</span> pairs: <span class="math inline">\((x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\)</span>.
The goal is to optimally estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> for the given data.</p>
<p>Denote the estimated values of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> by <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span>, respectively.
Note that it is also common to denote the estimated values as <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span>.</p>
<p>The fitted values of <span class="math inline">\(Y\)</span> are found via the linear equation <span class="math inline">\(\hat{Y}=b_0 + b_1 X\)</span> (or <span class="math inline">\(\hat{Y}=\hat{\beta}_0 + \hat{\beta}_1 X\)</span>).
In terms of each individual <span class="math inline">\((x_i, y_i)\)</span> sample observation, the fitted and observed values are found as follows:</p>
<p><span class="math display">\[\begin{array}{c|c}
\textrm{Fitted (predicted) values} &amp; \textrm{Observed (actual) values} \\
\hline
\hat{y}_1 = b_0 + b_1 x_1 &amp; y_1 = b_0 + b_1 x_1 + e_1 \\
\hat{y}_2 = b_0 + b_1 x_2 &amp; y_2 = b_0 + b_1 x_2 + e_2 \\
\vdots &amp; \vdots \\
\hat{y}_n = b_0 + b_1 x_n &amp; y_n = b_0 + b_1 x_n + e_n 
\end{array}\]</span></p>
<p>The difference between each corresponding observed and predicted value is the sample residual for that observation:
<span class="math display">\[\begin{array}{c}
e_1 = y_1 - \hat{y}_1 \\
e_2 = y_2 - \hat{y}_2 \\
\vdots \\
e_n = y_n - \hat{y}_n
\end{array}\]</span></p>
<p>or in general, <span class="math inline">\(e_i = y_i - \hat{y}_i\)</span>.
The method of least squares determines <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> so that</p>
<p><span class="math display">\[\textrm{Residual sum of squares (RSS)} = \sum_{i=1}^n e_i^2 = \sum_{i=1}^n\left(y_i - \hat{y}_i\right)^2 = \sum_{i=1}^n\left(y_i - \left(b_0 + b_1 x_i\right)\right)^2\]</span>
is a minimum.
In other words, any other method of estimating the <span class="math inline">\(y\)</span>-intercept and slope, <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>, respectively, will produce a larger RSS value than the method of least squares.</p>
<p>Minimizing RSS is a calculus exercise, so we will skip the details here.
The resulting line is the “best-fitting” straight line model we could possibly obtain for the data.</p>
<ul>
<li><span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> are called the least squares estimates of <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span>.</li>
<li>The line given by <span class="math inline">\(\hat{Y} = b_0 + b_1 X\)</span> is called the simple linear regression equation.</li>
</ul>
<p>We use R to fit such models and estimate <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> using the <code>lm()</code> function (<code>lm</code>=linear model).
No hand calculations required!
Linear models are fit using the R function <code>lm()</code>, and the basic format for a formula is given by <code>response ~ predictor</code>.</p>
<p>The <code>~</code> (“tilde”) here is read as “is modeled as a linear function of” and is used to separate the response variable from the predictor variable(s).
For simple linear regression, the form is <code>lm(y ~ x)</code>.
In other words, <code>lm(y ~ x)</code> fits the regression model <span class="math inline">\(Y = \beta_0 + \beta_1 + \varepsilon\)</span>.</p>
<p>The <span class="math inline">\(y\)</span>-intercept is always included, unless you specify otherwise.
<code>lm()</code> creates a model object containing essential information about the fit that we can extract with other R functions.
We illustrate via an example involving the muscle mass dataset.</p>
<div class="sourceCode" id="cb195"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb195-1"><a href="introduction-to-multiple-regression.html#cb195-1" aria-hidden="true" tabindex="-1"></a>muscle.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(mass <span class="sc">~</span> age, <span class="at">data=</span>muscle)</span>
<span id="cb195-2"><a href="introduction-to-multiple-regression.html#cb195-2" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(muscle.fit)</span></code></pre></div>
<pre><code>## List of 12
##  $ coefficients : Named num [1:2] 156.35 -1.19
##   ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;(Intercept)&quot; &quot;age&quot;
##  $ residuals    : Named num [1:60] 0.823 -1.557 -3.417 11.393 -6.797 ...
##   ..- attr(*, &quot;names&quot;)= chr [1:60] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...
##  $ effects      : Named num [1:60] -658.15 -107.83 -3.34 11.48 -6.69 ...
##   ..- attr(*, &quot;names&quot;)= chr [1:60] &quot;(Intercept)&quot; &quot;age&quot; &quot;&quot; &quot;&quot; ...
##  $ rank         : int 2
##  $ fitted.values: Named num [1:60] 105 108 100 102 103 ...
##   ..- attr(*, &quot;names&quot;)= chr [1:60] &quot;1&quot; &quot;2&quot; &quot;3&quot; &quot;4&quot; ...
##  $ assign       : int [1:2] 0 1
##  $ qr           :List of 5
##   ..$ qr   : num [1:60, 1:2] -7.746 0.129 0.129 0.129 0.129 ...
##   .. ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..- attr(*, &quot;assign&quot;)= int [1:2] 0 1
##   ..$ qraux: num [1:2] 1.13 1.19
##   ..$ pivot: int [1:2] 1 2
##   ..$ tol  : num 1e-07
##   ..$ rank : int 2
##   ..- attr(*, &quot;class&quot;)= chr &quot;qr&quot;
##  $ df.residual  : int 58
##  $ xlevels      : Named list()
##  $ call         : language lm(formula = mass ~ age, data = muscle)
##  $ terms        :Classes &#39;terms&#39;, &#39;formula&#39;  language mass ~ age
##   .. ..- attr(*, &quot;variables&quot;)= language list(mass, age)
##   .. ..- attr(*, &quot;factors&quot;)= int [1:2, 1] 0 1
##   .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. ..- attr(*, &quot;term.labels&quot;)= chr &quot;age&quot;
##   .. ..- attr(*, &quot;order&quot;)= int 1
##   .. ..- attr(*, &quot;intercept&quot;)= int 1
##   .. ..- attr(*, &quot;response&quot;)= int 1
##   .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; 
##   .. ..- attr(*, &quot;predvars&quot;)= language list(mass, age)
##   .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:2] &quot;numeric&quot; &quot;numeric&quot;
##   .. .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;mass&quot; &quot;age&quot;
##  $ model        :&#39;data.frame&#39;:   60 obs. of  2 variables:
##   ..$ mass: num [1:60] 106 106 97 113 96 119 92 112 92 102 ...
##   ..$ age : num [1:60] 43 41 47 46 45 41 47 41 48 48 ...
##   ..- attr(*, &quot;terms&quot;)=Classes &#39;terms&#39;, &#39;formula&#39;  language mass ~ age
##   .. .. ..- attr(*, &quot;variables&quot;)= language list(mass, age)
##   .. .. ..- attr(*, &quot;factors&quot;)= int [1:2, 1] 0 1
##   .. .. .. ..- attr(*, &quot;dimnames&quot;)=List of 2
##   .. .. ..- attr(*, &quot;term.labels&quot;)= chr &quot;age&quot;
##   .. .. ..- attr(*, &quot;order&quot;)= int 1
##   .. .. ..- attr(*, &quot;intercept&quot;)= int 1
##   .. .. ..- attr(*, &quot;response&quot;)= int 1
##   .. .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_GlobalEnv&gt; 
##   .. .. ..- attr(*, &quot;predvars&quot;)= language list(mass, age)
##   .. .. ..- attr(*, &quot;dataClasses&quot;)= Named chr [1:2] &quot;numeric&quot; &quot;numeric&quot;
##   .. .. .. ..- attr(*, &quot;names&quot;)= chr [1:2] &quot;mass&quot; &quot;age&quot;
##  - attr(*, &quot;class&quot;)= chr &quot;lm&quot;</code></pre>
<p>You’ll note from the <code>glimpse()</code> function there are many attributes inside the <code>lm</code> object.
We will utilize many of these in our exploration of regression models in the coming chapters.</p>
</div>
<div id="multiple-regression-example" class="section level3" number="5.2.2">
<h3><span class="header-section-number">5.2.2</span> Multiple Regression Example</h3>
<p>To fit a multiple regression, the code is essentially the same. Consider another example.</p>
<p><strong>Example:</strong> Property appraisals (example from <span class="citation"><a href="#ref-McClaveSincich2008" role="doc-biblioref">McClave et al.</a> (<a href="#ref-McClaveSincich2008" role="doc-biblioref">2008</a>)</span>).</p>
<p>Suppose a property appraiser wants to model the relationship between the sale price (<code>saleprice</code>) of a residential property and the following three predictor variables:</p>
<ul>
<li><code>landvalue</code> - Appraised land value of the property (in $)</li>
<li><code>impvalue</code> - Appraised value of improvements to the property (in $)</li>
<li><code>area</code> - Area of living space on the property (in sq ft)</li>
</ul>
<p>The data are in the R workspace <code>appraisal.RData</code> in our repository. Lets use R to fit what is known as a “main effects” multiple regression model. The form of this model is given by:</p>
<p><span class="math display">\[\textrm{Sale Price} = \beta_0 + \beta_1(\textrm{Land Area}) + \beta_2(\textrm{Improvement Value}) + \beta_3(\textrm{Area}) + \varepsilon\]</span></p>
<p>Note the model is just an extension of the simple linear regression model but with three predictor variables.
Similar to our study of ANOVA modeling we follow a standard pattern for analysis:</p>
<ol style="list-style-type: decimal">
<li>Describe the data both numerically and graphically.</li>
<li>Fit a model.</li>
<li>Once satisfied with the fit, check the regression assumptions.</li>
<li>Once the assumptions check out, use the model for inference and prediction.</li>
</ol>
<p>Before embarking on addressing each part, it might be instructive, just once (since this is your first multiple regression encounter), to actually look at the raw data to see its form:</p>
<div class="sourceCode" id="cb197"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb197-1"><a href="introduction-to-multiple-regression.html#cb197-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="fu">url</span>(<span class="st">&quot;https://tjfisher19.github.io/introStatModeling/data/appraisal.RData&quot;</span>))</span>
<span id="cb197-2"><a href="introduction-to-multiple-regression.html#cb197-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kable</span>(appraisal)</span></code></pre></div>
<table>
<thead>
<tr>
<th style="text-align:right;">
saleprice
</th>
<th style="text-align:right;">
landvalue
</th>
<th style="text-align:right;">
impvalue
</th>
<th style="text-align:right;">
area
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:right;">
68900
</td>
<td style="text-align:right;">
5960
</td>
<td style="text-align:right;">
44967
</td>
<td style="text-align:right;">
1873
</td>
</tr>
<tr>
<td style="text-align:right;">
48500
</td>
<td style="text-align:right;">
9000
</td>
<td style="text-align:right;">
27860
</td>
<td style="text-align:right;">
928
</td>
</tr>
<tr>
<td style="text-align:right;">
55500
</td>
<td style="text-align:right;">
9500
</td>
<td style="text-align:right;">
31439
</td>
<td style="text-align:right;">
1126
</td>
</tr>
<tr>
<td style="text-align:right;">
62000
</td>
<td style="text-align:right;">
10000
</td>
<td style="text-align:right;">
39592
</td>
<td style="text-align:right;">
1265
</td>
</tr>
<tr>
<td style="text-align:right;">
116500
</td>
<td style="text-align:right;">
18000
</td>
<td style="text-align:right;">
72827
</td>
<td style="text-align:right;">
2214
</td>
</tr>
<tr>
<td style="text-align:right;">
45000
</td>
<td style="text-align:right;">
8500
</td>
<td style="text-align:right;">
27317
</td>
<td style="text-align:right;">
912
</td>
</tr>
<tr>
<td style="text-align:right;">
38000
</td>
<td style="text-align:right;">
8000
</td>
<td style="text-align:right;">
29856
</td>
<td style="text-align:right;">
899
</td>
</tr>
<tr>
<td style="text-align:right;">
83000
</td>
<td style="text-align:right;">
23000
</td>
<td style="text-align:right;">
47752
</td>
<td style="text-align:right;">
1803
</td>
</tr>
<tr>
<td style="text-align:right;">
59000
</td>
<td style="text-align:right;">
8100
</td>
<td style="text-align:right;">
39117
</td>
<td style="text-align:right;">
1204
</td>
</tr>
<tr>
<td style="text-align:right;">
47500
</td>
<td style="text-align:right;">
9000
</td>
<td style="text-align:right;">
29349
</td>
<td style="text-align:right;">
1725
</td>
</tr>
<tr>
<td style="text-align:right;">
40500
</td>
<td style="text-align:right;">
7300
</td>
<td style="text-align:right;">
40166
</td>
<td style="text-align:right;">
1080
</td>
</tr>
<tr>
<td style="text-align:right;">
40000
</td>
<td style="text-align:right;">
8000
</td>
<td style="text-align:right;">
31679
</td>
<td style="text-align:right;">
1529
</td>
</tr>
<tr>
<td style="text-align:right;">
97000
</td>
<td style="text-align:right;">
20000
</td>
<td style="text-align:right;">
58150
</td>
<td style="text-align:right;">
2455
</td>
</tr>
<tr>
<td style="text-align:right;">
45500
</td>
<td style="text-align:right;">
8000
</td>
<td style="text-align:right;">
23454
</td>
<td style="text-align:right;">
1151
</td>
</tr>
<tr>
<td style="text-align:right;">
40900
</td>
<td style="text-align:right;">
8000
</td>
<td style="text-align:right;">
20897
</td>
<td style="text-align:right;">
1173
</td>
</tr>
<tr>
<td style="text-align:right;">
80000
</td>
<td style="text-align:right;">
10500
</td>
<td style="text-align:right;">
56248
</td>
<td style="text-align:right;">
1960
</td>
</tr>
<tr>
<td style="text-align:right;">
56000
</td>
<td style="text-align:right;">
4000
</td>
<td style="text-align:right;">
20859
</td>
<td style="text-align:right;">
1344
</td>
</tr>
<tr>
<td style="text-align:right;">
37000
</td>
<td style="text-align:right;">
4500
</td>
<td style="text-align:right;">
22610
</td>
<td style="text-align:right;">
988
</td>
</tr>
<tr>
<td style="text-align:right;">
50000
</td>
<td style="text-align:right;">
3400
</td>
<td style="text-align:right;">
35948
</td>
<td style="text-align:right;">
1076
</td>
</tr>
<tr>
<td style="text-align:right;">
22400
</td>
<td style="text-align:right;">
1500
</td>
<td style="text-align:right;">
5779
</td>
<td style="text-align:right;">
962
</td>
</tr>
</tbody>
</table>
<p>We see there are four variables (<span class="math inline">\(k+1 = 4\)</span>) and 20 observations (<span class="math inline">\(n = 20\)</span>).
Each row contains a different member of the sample (in this case, a different property).
Notice the one property with the relatively high selling price as compared to the others.</p>
<p>Pairwise scatterplots are given below to visualize the bivariate associations.
Here we use the <code>ggpairs()</code> function in the add-on package <code>GGally</code> <span class="citation">(<a href="#ref-R-GGally" role="doc-biblioref">Schloerke et al., 2018</a>)</span>.
Pairwise scatterplots provide a means to visually explore all <span class="math inline">\(k+1\)</span> dimensions of a dataset, but note that as <span class="math inline">\(k\)</span> and <span class="math inline">\(n\)</span> (the sample size) increase, these plots can get very “busy.”</p>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb198-1"><a href="introduction-to-multiple-regression.html#cb198-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggpairs</span>(appraisal, <span class="at">columns=</span><span class="fu">c</span>(<span class="dv">2</span><span class="sc">:</span><span class="dv">4</span>,<span class="dv">1</span>) )</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch5-6"></span>
<img src="introStatModeling_files/figure-html/ch5-6-1.png" alt="Pairs plot showing the relationship between the variables `landvalue`, `impvalue`, `area` and `saleprice`." width="80%" />
<p class="caption">
Figure 5.4: Pairs plot showing the relationship between the variables <code>landvalue</code>, <code>impvalue</code>, <code>area</code> and <code>saleprice</code>.
</p>
</div>
<p>Note that using the <code>columns=c(2:4,1)</code> option, we have reordered the variables for the plot (generally, it is best to have the response variable last so it appears on the <span class="math inline">\(y\)</span>-axis in the scatterplots).
In the bottom left of the matrix of plots in Figure <a href="introduction-to-multiple-regression.html#fig:ch5-6">5.4</a> we have pairwise scatterplots.
At first glance, it seems as though each of the three predictors positively relates to sales price.
However, we also get to see the plots of predictors against themselves.
This can be highly informative and will be of some importance to us later on.
There appear to be positive associations between all the predictors (not surprisingly given the context).
It is also instructive to note that three specific individual properties with high appraised land values seem to be the catalyst for these apparent associations.</p>
<p>In the upper right corner of Figure <a href="introduction-to-multiple-regression.html#fig:ch5-6">5.4</a> are the Pearson correlation coefficients (which measures the amount of linear relationship between the two variables) and along the diagonal are density plots of each variable (providing some information about each variables shape).</p>
<p>We now fit the “main effects” MLR model for predicting <span class="math inline">\(Y\)</span> = <code>saleprice</code> from the three predictors.
Initially, we might be interested in seeing how individual characteristic(s) impact sales price.</p>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb199-1"><a href="introduction-to-multiple-regression.html#cb199-1" aria-hidden="true" tabindex="-1"></a>appraisal.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(saleprice <span class="sc">~</span> landvalue <span class="sc">+</span> impvalue <span class="sc">+</span> area, <span class="at">data=</span>appraisal)</span>
<span id="cb199-2"><a href="introduction-to-multiple-regression.html#cb199-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(appraisal.fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = saleprice ~ landvalue + impvalue + area, data = appraisal)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -14688  -2026   1025   2717  15967 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept) 1384.197   5744.100    0.24   0.8126   
## landvalue      0.818      0.512    1.60   0.1294   
## impvalue       0.819      0.211    3.89   0.0013 **
## area          13.605      6.569    2.07   0.0549 . 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7920 on 16 degrees of freedom
## Multiple R-squared:  0.898,  Adjusted R-squared:  0.878 
## F-statistic: 46.7 on 3 and 16 DF,  p-value: 3.87e-08</code></pre>
<p>Here are some observations from this regression model fit:</p>
<p><strong>Parameter estimates.</strong> The fitted model, where <span class="math inline">\(Y\)</span> = sale price, is:</p>
<p><span class="math display">\[\hat{Y} = 1384.197 + 0.818(\textrm{Land value}) + 0.819(\textrm{Improvement value}) + 13.605(\textrm{Area})\]</span></p>
<p>That is, the least squares estimates of the four <span class="math inline">\(\beta\)</span>-parameters in the model are <span class="math inline">\(b_0 = 1384.2\)</span>, <span class="math inline">\(b_1 = 0.817\)</span>, <span class="math inline">\(b_2 = 0.819\)</span>, and <span class="math inline">\(b_3 = 13.605\)</span>.
We will discuss their interpretation later.</p>
<p><strong>Residual standard error.</strong> In regression, the error variance <span class="math inline">\(\sigma^2\)</span> is a measure of the variability of all possible population response <span class="math inline">\(Y\)</span>-values around their corresponding predicted values as obtained by the true population regression equation.
It is called “error” variance because it deals with the difference between true values vs. model-predicted values, and hence can be thought of as measuring the “error” one would incur by making predictions using the model.</p>
<p>Since a variance is always a sum of squares divided by degrees of freedom (SS/df), we can estimate <span class="math inline">\(\sigma^2\)</span> for a simple linear regression model using the following:</p>
<p><span class="math display">\[S^2_{\varepsilon} = \frac{\sum_{i=1}^n(y_i - \hat{y}_i)^2}{n-p} = \frac{\textrm{RSS}}{n-p}.\]</span></p>
<p>The degrees of freedom for this error variance estimate is <span class="math inline">\(n -p\)</span> where <span class="math inline">\(p\)</span> is the number of parameters estimated in the regression model, here <span class="math inline">\(p = k+1 = 3+1=4\)</span>.
So, we had to “spend” <span class="math inline">\(p=k+1\)</span> degrees of freedom from the <span class="math inline">\(n\)</span> available degrees of freedom in order to estimate <span class="math inline">\(\beta_0, \beta_1\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(\beta_k\)</span>.
(Hopefully you can see that you cannot fit a model with <span class="math inline">\(n\)</span> or more parameters to a sample of size <span class="math inline">\(n\)</span>; you will have “spent” all your available degrees of freedom, and hence you could not estimate <span class="math inline">\(\sigma^2\)</span>.)</p>
<p>We will usually just denote the estimated error variance using <span class="math inline">\(S^2\)</span> instead of <span class="math inline">\(S^2_{\varepsilon}\)</span>.
R provides the residual standard error in the <code>summary()</code> output from the linear model fit, which is the square root of the estimated error variance, and thus has the advantage of being in the original units of the response variable <span class="math inline">\(Y\)</span>.
Here, <span class="math inline">\(s = 7915\)</span> which is our estimate for <span class="math inline">\(\sigma\)</span>.
Applying an Empirical Rule type argument (remember from Intro Statistics!), we could say that approximately 95% of this model’s predicted sale prices would fall within <span class="math inline">\(\pm 2(7915) = \pm \$15,830\)</span> of the actual sales prices.
The error degrees of freedom are <span class="math inline">\(n – p\)</span> = <span class="math inline">\(20 – 4 = 16\)</span>.</p>
<p><strong>Interpretation.</strong> Since this is essentially the standard deviation of the residuals, we could interpret the value of <span class="math inline">\(S\)</span> as essentially the average residual size; <em>i.e.</em>, the average size of the prediction errors produced by the regression model.
In the present context, this translates to stating that the regression model produces predicted sale prices that are, on average, $7,915 dollars off from the actual measured sale price values.</p>
<p>As you can see, such a measure is valuable in helping us determine how well a model performs (<em>i.e.</em>, smaller residual error <span class="math inline">\(\rightarrow\)</span> better fitting model).
So, <span class="math inline">\(S^2\)</span> and the standard errors are important ingredients in the development of inference in regression.</p>
<p>It should also be noted that at this point we are not even sure if this is a good model, and if not, how we might make it better.
So later on, we will discuss the “art” of model building.</p>
</div>
</div>
<div id="interpreting-beta-parameter-estimates-in-mlr" class="section level2" number="5.3">
<h2><span class="header-section-number">5.3</span> Interpreting <span class="math inline">\(\beta\)</span>-parameter estimates in MLR</h2>
<p>Suppose we fit a model to obtain the multiple linear regression equation:</p>
<p><span class="math display">\[\hat{Y} = b_0 + b_1 X_1 + b_2 X_2 + \ldots + b_k X_k\]</span></p>
<p>What does <span class="math inline">\(b_1\)</span> mean?
In multiple regression involving simultaneous assessments of many predictors, interpretation can become problematic.
In certain cases, a <span class="math inline">\(b_i\)</span> coefficient might represent some real physical constant, but oftentimes the statistical model is just a convenience for representing a more complex reality, so the real meaning of a particular <span class="math inline">\(b_i\)</span> may not be obvious.</p>
<p>At this point in our trek through statistical model, it is important to remember that there are two methods for obtaining data for analysis: designed experiments and observational studies.
It is important to recall the distinction because each type of data results in a different approach to interpreting the <span class="math inline">\(\beta\)</span>-parameter estimates in in a multiple linear regression model.</p>
<div id="designed-experiments-1" class="section level3" number="5.3.1">
<h3><span class="header-section-number">5.3.1</span> Designed experiments</h3>
<p>In a designed experiment, the researcher has control over the settings of the predictor variables <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\(X_k\)</span>.
For example, suppose we wish to study several physical exercise regimens and how they impact calorie burn.
The experimental units (EUs) are the people we use for the study.
We can control some of the predictors such as the amount of time spent exercising or the amount of carbohydrate consumed prior to exercising.
Some other predictors might not be controlled but can be measured, such as baseline metabolic variables.
Other variables, such as the temperature in the room or the type of exercise done, could be held fixed.</p>
<p>Having control over the conditions in an experiment allows us to make stronger conclusions from the analysis.
One important property of a well-designed experiment is called orthogonality.
Orthogonality is useful because it allows us to easily interpret the effect one predictor has on the response without regard to any others.
For example, orthogonality would permit us to examine the effect of increasing <span class="math inline">\(X_1\)</span> = time spent exercising on <span class="math inline">\(Y\)</span> = calorie burn, without any concern for <span class="math inline">\(X_2\)</span> = carbohydrate consumption.
This can only occur in a situation where the predictor settings are judiciously chosen and assigned by the experimenter.
Let us look at an example.</p>
</div>
<div id="example-of-orthogonal-design" class="section level3" number="5.3.2">
<h3><span class="header-section-number">5.3.2</span> Example of Orthogonal Design</h3>
<p><strong>Example.</strong> Cleaning experiment (from <span class="citation"><a href="#ref-NavidiMonk2015" role="doc-biblioref">Navidi et al.</a> (<a href="#ref-NavidiMonk2015" role="doc-biblioref">2015</a>)</span>).</p>
<p>An experiment was performed to measure the effects of three predictors on the ability of a cleaning solution to remove oil from cloth. The data are in the R workspace <code>cleaningexp.RData</code>. Here are some details:</p>
<p>Response:</p>
<ul>
<li><code>pct.removed</code> - Percentage of the oil stain removed</li>
</ul>
<p>Predictors:</p>
<ul>
<li><code>soap.conc</code> - Concentration of soap, in % by weight</li>
<li><code>lauric.acid</code> - Percentage of lauric acid in the solution</li>
<li><code>citric.acid</code> - Percentage of citric acid in the solution</li>
</ul>
<p>Soap concentration was controlled at two levels (15% and 25%), lauric acid at four levels (10%, 20%, 30%, 40%), and citric acid at three levels (10%, 12%, 14%).
Each possible combination of the three predictors was tested on five separate stained cloths, for a total of <span class="math inline">\(5 \times 2 \times 4 \times 3 = 120\)</span> measurements.
We want to illustrate the effect of orthogonality on the <span class="math inline">\(\beta\)</span>-parameter estimates.</p>
<div class="sourceCode" id="cb201"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb201-1"><a href="introduction-to-multiple-regression.html#cb201-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="fu">url</span>(<span class="st">&quot;https://tjfisher19.github.io/introStatModeling/data/cleaningexp.RData&quot;</span>)</span>
<span id="cb201-2"><a href="introduction-to-multiple-regression.html#cb201-2" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(cleaningexp)</span></code></pre></div>
<pre><code>## Rows: 120
## Columns: 5
## $ soap.conc   &lt;int&gt; 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15~
## $ lauric.acid &lt;int&gt; 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10~
## $ citric.acid &lt;int&gt; 10, 10, 10, 10, 10, 12, 12, 12, 12, 12, 14, 14, 14, 14, 14~
## $ rep         &lt;int&gt; 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5, 1, 2, 3, 4, 5~
## $ pct.removed &lt;dbl&gt; 20.5, 14.1, 19.1, 20.8, 18.7, 21.2, 22.7, 20.3, 23.9, 20.1~</code></pre>
<p>Since we have many numeric levels and we are interested in the numeric association (as compared to the categorical association in ANOVA), we fit a linear model to all three predictors:</p>
<div class="sourceCode" id="cb203"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb203-1"><a href="introduction-to-multiple-regression.html#cb203-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(pct.removed <span class="sc">~</span> soap.conc <span class="sc">+</span> lauric.acid <span class="sc">+</span> citric.acid, <span class="at">data=</span>cleaningexp))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = pct.removed ~ soap.conc + lauric.acid + citric.acid, 
##     data = cleaningexp)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -8.084 -1.995  0.045  2.078  8.787 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -49.6392     2.4169   -20.5   &lt;2e-16 ***
## soap.conc     2.1987     0.0554    39.6   &lt;2e-16 ***
## lauric.acid   0.8629     0.0248    34.8   &lt;2e-16 ***
## citric.acid   2.5956     0.1698    15.3   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 3.04 on 116 degrees of freedom
## Multiple R-squared:  0.963,  Adjusted R-squared:  0.962 
## F-statistic: 1.01e+03 on 3 and 116 DF,  p-value: &lt;2e-16</code></pre>
<p>Take note of the model’s <span class="math inline">\(\beta\)</span>-parameter estimates and their SEs.</p>
<p>Now for illustration only, let’s drop <code>soap.conc</code> from the model:</p>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb205-1"><a href="introduction-to-multiple-regression.html#cb205-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(pct.removed <span class="sc">~</span> lauric.acid <span class="sc">+</span> citric.acid, <span class="at">data=</span>cleaningexp))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = pct.removed ~ lauric.acid + citric.acid, data = cleaningexp)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -16.15 -11.04  -1.78  11.45  19.78 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -5.6658     8.1577   -0.69   0.4887    
## lauric.acid   0.8629     0.0942    9.16  2.1e-15 ***
## citric.acid   2.5956     0.6449    4.02   0.0001 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 11.5 on 117 degrees of freedom
## Multiple R-squared:  0.461,  Adjusted R-squared:  0.452 
## F-statistic: 50.1 on 2 and 117 DF,  p-value: &lt;2e-16</code></pre>
<p>Notice that the coefficient estimates do not change regardless of what other predictors are in the model (this would hold true if we dropped <code>lauric.acid</code> or <code>citric.acid</code> from the model as well - try it!).
This is what orthogonality ensures.
This means that we are safe in assessing the size of the impact of, say, soap concentration on the ability to remove the oil stain, without worrying about how other variables might impact the relationship.</p>
<p>So in a designed experiment with orthogonality properties, we can interpret the value of <span class="math inline">\(b_1\)</span> unconditionally as follows:</p>
<blockquote>
<p>A one-unit increase in <span class="math inline">\(x_1\)</span> will cause a change of size <span class="math inline">\(b_1\)</span> in the mean response.</p>
</blockquote>
<p><strong>Side note.</strong> When we deleted the predictors one at a time, we effectively were taking that predictor’s explanatory contribution to the response and dumping it into the error component of the model.
Here, since each predictor was significant (<span class="math inline">\(p\)</span>-value &lt; 0.0001), this removal caused the residual standard error to increase substantially, which subsequently made the SEs of the coefficients, <span class="math inline">\(t\)</span>-statistics and <span class="math inline">\(p\)</span>-values change.
We want to make it clear that <strong>in practice it is not recommended to remove significant effects from the model</strong> — it was only done here to demonstrate that orthogonality ensures that the model’s <span class="math inline">\(\beta\)</span>-parameter estimates are unchanged regardless of what other predictors are included.
However, the results of tests/CIs for those coefficients may change depending on what is included in the model (if you remove an insignificant predictor, the residual SE will change only slightly and hence have negligible impact on SEs of the coefficients, <span class="math inline">\(t\)</span>-statistics and <span class="math inline">\(p\)</span>-values).</p>
</div>
<div id="observational-studies-1" class="section level3" number="5.3.3">
<h3><span class="header-section-number">5.3.3</span> Observational studies</h3>
<p>In most regression settings, you simply collect measurements on predictor and response variables as they naturally occur, without intervention from the data collector.
Such data is called observational data.</p>
<p>Interpreting models built on observational data can be challenging.
There are many opportunities for error and any conclusions will carry with them substantial unquantifiable uncertainty.
Nevertheless, there are many important questions for which only observational data will ever be available.
For example, how else would we study something like differences in prevalence of obesity, diabetes and other cardiovascular risk factors between different ethnic groups?
Or the effect of socio-economic status on self esteem?
It is impossible to design experiments to investigate these since we cannot control variables (or it would be grossly unethical to do so), so we must make the attempt to build good models with observational data in spite of their shortcomings.</p>
<p>In observational studies, establishing causal connections between response and predictor variables is nearly impossible.
In the limited scope of a single study, the best one can hope for is to establish associations between predictor variables and response variables.
But even this can be difficult due to the uncontrolled nature of observational data.
<strong>Why?</strong> It is because unmeasured and possibly unsuspected “lurking” variables may be the real cause of an observed relationship between response <span class="math inline">\(Y\)</span> and some predictor <span class="math inline">\(X_i\)</span>.
Recall the earlier example where we observed a positive correlation between the heights and mathematical abilities of school children?
It turned out that this relationship was really driven by a lurking variable – the age of the child.
In this case, the variables height and age are said to be confounded, because for the purpose of predicting math ability in children, they basically measure the same predictive attribute.</p>
<p>In observational studies, it is important to adjust for the effects of possible confounding variables.
Unfortunately, one can never be sure that the all relevant confounding variables have been identified.
As a result, one must take care in interpreting <span class="math inline">\(\beta\)</span>-parameter estimates from regression analyses involving observational data.</p>
<p>Here is probably the best way of interpreting a <span class="math inline">\(\beta\)</span>-parameter estimate (say <span class="math inline">\(b_1\)</span>) when dealing with observational data:</p>
<blockquote>
<p><span class="math inline">\(b_1\)</span> measures the effect of a one-unit increase in <span class="math inline">\(x_1\)</span> on the mean response when all the other (specified) predictors are held constant.</p>
</blockquote>
<p>Even this, however, is not perfect.
Often in practice, one predictor cannot be changed without changing other predictors.
For example, competing species of ground cover in a botanical field study are often negatively correlated, so increasing the amount of cover of one species will likely mean the lowering of cover of the other.
In health studies, it is unrealistic to presume that an increase of 1% body fat in an individual would not correlate to changes in other physical characteristics too (<em>e.g.</em>, waist circumference).</p>
<p>Furthermore, this interpretation requires the specification of the other variables – so changing which other variables are included in the model may change the interpretation of <span class="math inline">\(b_1\)</span>.
Here’s an illustration:</p>
</div>
<div id="example-of-observational-regression-interpretation" class="section level3" number="5.3.4">
<h3><span class="header-section-number">5.3.4</span> Example of observational regression interpretation</h3>
<p><strong>Example:</strong> Property appraisals (from <span class="citation"><a href="#ref-McClaveSincich2008" role="doc-biblioref">McClave et al.</a> (<a href="#ref-McClaveSincich2008" role="doc-biblioref">2008</a>)</span>).</p>
<p>We earlier fit a full “main effects” model predicting <span class="math inline">\(Y\)</span> = salesprice from three predictor variables dealing with property appraisals.
This is an observational study, since the predictor values are not set by design by the researcher.
Here’s a brief recap of the fitted model:</p>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb207-1"><a href="introduction-to-multiple-regression.html#cb207-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(appraisal.fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = saleprice ~ landvalue + impvalue + area, data = appraisal)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -14688  -2026   1025   2717  15967 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept) 1384.197   5744.100    0.24   0.8126   
## landvalue      0.818      0.512    1.60   0.1294   
## impvalue       0.819      0.211    3.89   0.0013 **
## area          13.605      6.569    2.07   0.0549 . 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7920 on 16 degrees of freedom
## Multiple R-squared:  0.898,  Adjusted R-squared:  0.878 
## F-statistic: 46.7 on 3 and 16 DF,  p-value: 3.87e-08</code></pre>
<p><em>Interpretation.</em> We see <span class="math inline">\(b_3\)</span> = 13.605, this value may be best interpreted in context by either of the following:</p>
<ul>
<li>“For each additional square foot of living space on a property, we estimate an increase of $13.61 in the mean selling price, holding appraised land and improvement values fixed.”</li>
<li>“Each additional square foot of living space on a property results in an average increase of $13.61 in the mean selling price, after adjusting for the appraised value of the land and improvements.”</li>
</ul>
<p><strong>In Summary:</strong> When a predictor’s effect on the response variable is assessed in a model that contains other predictor variables, that predictor’s effect is said to be adjusted for the other predictors.</p>
<p>Now, suppose we delete <code>landvalue</code> (an insignificant predictor, <span class="math inline">\(p\)</span>-value <span class="math inline">\(&gt;\)</span> 0.05; we will cover this in more detail in the next chapter). How is the model affected?</p>
<div class="sourceCode" id="cb209"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb209-1"><a href="introduction-to-multiple-regression.html#cb209-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(<span class="fu">lm</span>(saleprice <span class="sc">~</span> impvalue <span class="sc">+</span> area, <span class="at">data=</span>appraisal))</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = saleprice ~ impvalue + area, data = appraisal)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -15832  -5200   1260   4642  13836 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  -10.191   5931.634    0.00  0.99865    
## impvalue       0.959      0.200    4.79  0.00017 ***
## area          16.492      6.599    2.50  0.02299 *  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 8270 on 17 degrees of freedom
## Multiple R-squared:  0.881,  Adjusted R-squared:  0.867 
## F-statistic:   63 on 2 and 17 DF,  p-value: 1.37e-08</code></pre>
<p>Notice that the values of both <span class="math inline">\(b_2\)</span> and <span class="math inline">\(b_3\)</span> changed after the predictor <span class="math inline">\(X_1\)</span> (<code>landvalue</code>) was deleted from the model.
This means that our estimate of the effect that one predictor has on the response depends on what other predictors are in the model (compare this with the example of orthogonality from earlier).</p>
</div>
</div>
<div id="whymultiple" class="section level2" number="5.4">
<h2><span class="header-section-number">5.4</span> Why multiple regression?</h2>
<p>We briefly tangent with a discussion of why multiple linear regression is superior to running “separate” simple linear regressions.</p>
<p>Because complex natural phenomena are typically impacted by many characteristics, it would be naive in most circumstances to think that just one variable serves as an adequate explanation for an outcome.
Instead, we consider the simultaneous impact of potential predictors of interest on the response.
Useful models reflect this fact.</p>
<p>The “one-predictor-at-a-time approach” can be quite bad.
Suppose you are considering three potential predictors <span class="math inline">\(X_1\)</span>, <span class="math inline">\(X_2\)</span>, <span class="math inline">\(X_3\)</span> on a response <span class="math inline">\(Y\)</span>.
You might be tempted to fit three separate simple linear regression models to each predictor:</p>
<p><span class="math display">\[\begin{array}{c}
Y = \beta_0 + \beta_1 X_1 + \varepsilon \\
Y = \beta_0 + \beta_2 X_2 + \varepsilon \\
Y = \beta_0 + \beta_3 X_3 + \varepsilon
\end{array}\]</span></p>
<p>As we shall see, this approach to regression is fundamentally flawed and is to be avoided at all costs.
The problem is that if you fit “too simple” a model you will not account for the “collective” impact of multiple predictors of interest, you may then fail to detect significant relationships, or even come to completely wrong conclusions.
Below we provide two examples of the dangers of simple linear regression.</p>
<p>To fully understand these examples, you will need to review <a href="inference-regarding-multiple-regression.html#inference-regarding-multiple-regression">6</a>.
For now we highlight the individual regression <span class="math inline">\(t\)</span>-test and residual error to demonstrate the limitations of simple linear regression.</p>
<div id="example-of-spurious-correlation" class="section level3" number="5.4.1">
<h3><span class="header-section-number">5.4.1</span> Example of Spurious Correlation</h3>
<p>Here is an example to illustrate potential dangers of using a simple linear regression.</p>
<p><strong>Example.</strong> Punting Ability in American Football (published in <span class="citation"><a href="#ref-Walpole2007" role="doc-biblioref">Walpole et al.</a> (<a href="#ref-Walpole2007" role="doc-biblioref">2007</a>)</span>)</p>
<p>In the game of American Football, a <em>punter</em> will kick a ball to the opposing team as far as possible towards the opponent’s end zone, attempting to maximize the distance the receiving team must advance the ball in order to score.
One key feature of a <em>good punt</em> is the “hang time,” the amount of time the ball ‘hangs’ in the air before being caught by the punt returner.
An experiment was conducted were each of 13 punters kicked the ball 10 times and the experimenter recorded their average hang time and distances, along with other measures of strength and flexibility.</p>
<ul>
<li>Punter_id - an identifier for each punter</li>
<li>Hang_time - The average hangtim</li>
<li>Distance -</li>
<li>RLS - right leg strength (pounds)</li>
<li>LLS - left leg strength (pounds)</li>
<li>RHF - right hamstring muscle flexibility (degrees)</li>
<li>LHF - left hamstring muscle flexibility (degrees)</li>
<li>Power - Overall leg strength (foot-pounds)</li>
</ul>
<p>Original Source: <span class="citation"><a href="#ref-VTpunting1983" role="doc-biblioref">Department of Health et al.</a> (<a href="#ref-VTpunting1983" role="doc-biblioref">1983</a>)</span></p>
<p>The data is available in the file <code>puntingData.csv</code>.</p>
<div class="sourceCode" id="cb211"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb211-1"><a href="introduction-to-multiple-regression.html#cb211-1" aria-hidden="true" tabindex="-1"></a>site <span class="ot">&lt;-</span> <span class="st">&quot;https://tjfisher19.github.io/introStatModeling/data/puntingData.csv&quot;</span></span>
<span id="cb211-2"><a href="introduction-to-multiple-regression.html#cb211-2" aria-hidden="true" tabindex="-1"></a>punting <span class="ot">&lt;-</span> <span class="fu">read_csv</span>(site, <span class="at">col_types=</span><span class="fu">cols</span>() )</span>
<span id="cb211-3"><a href="introduction-to-multiple-regression.html#cb211-3" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(punting)</span></code></pre></div>
<pre><code>## Rows: 13
## Columns: 8
## $ Punter_id &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13
## $ Hang_time &lt;dbl&gt; 4.75, 4.07, 4.04, 4.18, 4.35, 4.16, 4.43, 3.20, 3.02, 3.64, ~
## $ Distance  &lt;dbl&gt; 162.50, 144.00, 147.50, 163.50, 192.00, 171.75, 162.00, 104.~
## $ RLS       &lt;dbl&gt; 170, 140, 180, 160, 170, 150, 170, 110, 120, 130, 120, 140, ~
## $ LLS       &lt;dbl&gt; 170, 130, 170, 160, 150, 150, 180, 110, 110, 120, 140, 130, ~
## $ RHF       &lt;dbl&gt; 106, 92, 93, 103, 104, 101, 108, 86, 90, 85, 89, 92, 95
## $ LHF       &lt;dbl&gt; 106, 93, 78, 93, 93, 87, 106, 92, 86, 80, 83, 94, 95
## $ Power     &lt;dbl&gt; 240.57, 195.49, 152.99, 197.09, 266.56, 260.56, 219.25, 132.~</code></pre>
<p>First note that this data was collected through an experiment, but the data is <strong>observational</strong> – the experimenter did not control any of the variables of interest.</p>
<p>For our example, we will consider RLS, LLS and Power as potential predictor variables for hang time.
Let us look at the scatterplot matrix of the data.</p>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb213-1"><a href="introduction-to-multiple-regression.html#cb213-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggpairs</span>(punting, <span class="at">columns=</span><span class="fu">c</span>(<span class="dv">4</span>,<span class="dv">5</span>,<span class="dv">8</span>,<span class="dv">2</span>) )</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ch5-14pairs"></span>
<img src="introStatModeling_files/figure-html/ch5-14pairs-1.png" alt="Pairs plot showing the relationship between the variables `RLS`, `LLS`, `Power` and `Hang_time`." width="80%" />
<p class="caption">
Figure 5.5: Pairs plot showing the relationship between the variables <code>RLS</code>, <code>LLS</code>, <code>Power</code> and <code>Hang_time</code>.
</p>
</div>
<p>Based on Figure <a href="introduction-to-multiple-regression.html#fig:ch5-14pairs">5.5</a> it appears that all three variables, RLS, LLS and Power, are all moderately or strongly correlated with the hang time.
Suppose we fit three simple linear regression models to the data.</p>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb214-1"><a href="introduction-to-multiple-regression.html#cb214-1" aria-hidden="true" tabindex="-1"></a>slr_rls <span class="ot">&lt;-</span> <span class="fu">lm</span>(Hang_time <span class="sc">~</span> RLS, <span class="at">data=</span>punting)</span>
<span id="cb214-2"><a href="introduction-to-multiple-regression.html#cb214-2" aria-hidden="true" tabindex="-1"></a>slr_lls <span class="ot">&lt;-</span> <span class="fu">lm</span>(Hang_time <span class="sc">~</span> LLS, <span class="at">data=</span>punting)</span>
<span id="cb214-3"><a href="introduction-to-multiple-regression.html#cb214-3" aria-hidden="true" tabindex="-1"></a>slr_power <span class="ot">&lt;-</span> <span class="fu">lm</span>(Hang_time <span class="sc">~</span> Power, <span class="at">data=</span>punting)</span></code></pre></div>
<p>Look at the summary output of each of the three models.</p>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb215-1"><a href="introduction-to-multiple-regression.html#cb215-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(slr_rls)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Hang_time ~ RLS, data = punting)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -0.458 -0.183  0.035  0.198  0.431 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.28437    0.53569    2.40  0.03538 *  
## RLS          0.01785    0.00359    4.98  0.00042 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.283 on 11 degrees of freedom
## Multiple R-squared:  0.692,  Adjusted R-squared:  0.664 
## F-statistic: 24.8 on 1 and 11 DF,  p-value: 0.000419</code></pre>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb217-1"><a href="introduction-to-multiple-regression.html#cb217-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(slr_lls)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Hang_time ~ LLS, data = punting)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.3616 -0.1701 -0.0662  0.1576  0.4038 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  1.27628    0.47390    2.69  0.02091 *  
## LLS          0.01838    0.00326    5.65  0.00015 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.259 on 11 degrees of freedom
## Multiple R-squared:  0.743,  Adjusted R-squared:  0.72 
## F-statistic: 31.9 on 1 and 11 DF,  p-value: 0.00015</code></pre>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb219-1"><a href="introduction-to-multiple-regression.html#cb219-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(slr_power)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Hang_time ~ Power, data = punting)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.4138 -0.2583  0.0003  0.2523  0.4862 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  2.40448    0.40679    5.91   0.0001 ***
## Power        0.00773    0.00202    3.83   0.0028 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.334 on 11 degrees of freedom
## Multiple R-squared:  0.571,  Adjusted R-squared:  0.532 
## F-statistic: 14.7 on 1 and 11 DF,  p-value: 0.0028</code></pre>
<p>Although we have not formally covered inference on regression, we might conclude from these results that each of the predictor variables influences hang time (see the very small <span class="math inline">\(p\)</span>-value associated with the predictor variables).
The scatterplots (Figure <a href="introduction-to-multiple-regression.html#fig:ch5-14pairs">5.5</a>) seems to support this as well.</p>
<p>However, it would be erroneous to conclude that this relationship implies causation (a caution wisely applied to all observational data, as we will discuss later).
The sample of punters span a range of abilities, and each might utilize their left or right leg more in the act of kicking.
So a logical question to ask is “how do all these variables, collectively, influence hang time?”</p>
<p>We fit a multiple regression predicting hang time from all three predictor variables.</p>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb221-1"><a href="introduction-to-multiple-regression.html#cb221-1" aria-hidden="true" tabindex="-1"></a>mlr_punting <span class="ot">&lt;-</span> <span class="fu">lm</span>(Hang_time <span class="sc">~</span> RLS <span class="sc">+</span> LLS <span class="sc">+</span> Power, <span class="at">data=</span>punting)</span>
<span id="cb221-2"><a href="introduction-to-multiple-regression.html#cb221-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mlr_punting)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Hang_time ~ RLS + LLS + Power, data = punting)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.3463 -0.0832  0.0044  0.0404  0.3412 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept) 1.103924   0.388267    2.84    0.019 *
## RLS         0.000232   0.006195    0.04    0.971  
## LLS         0.013521   0.005744    2.35    0.043 *
## Power       0.004270   0.001540    2.77    0.022 *
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.202 on 9 degrees of freedom
## Multiple R-squared:  0.871,  Adjusted R-squared:  0.828 
## F-statistic: 20.3 on 3 and 9 DF,  p-value: 0.000241</code></pre>
<p>Compare this result to the simple regression models.
When all three variables are included, the variable RLS appears to be unimportant (<span class="math inline">\(p\)</span>-value = 0.9723).
The earlier result occurred because both hang time and RLS are highly correlated with LLS – so the omission of LLS led us to detect what is known as a <strong>spurious association</strong> between RLS and hang time.
To phrase another way, LLS and RLS appear to be providing much of the same, or redundant, information to the model.
We may also consider the addition of LLS and Power to be <em>confounding</em> in regards to the variable RLS.</p>
<p>Further, we can see how the multiple regression model improves on the accuracy of the predicted values of the regression model.
Consider the table of residual standard errors below.</p>
<table class="table" style="width: auto !important; margin-left: auto; margin-right: auto;">
<thead>
<tr>
<th style="text-align:left;">
Model
</th>
<th style="text-align:right;">
Residual.Standard.Error
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
SLR: RLS
</td>
<td style="text-align:right;">
0.283211
</td>
</tr>
<tr>
<td style="text-align:left;">
SLR: LLS
</td>
<td style="text-align:right;">
0.258640
</td>
</tr>
<tr>
<td style="text-align:left;">
SLR: Power
</td>
<td style="text-align:right;">
0.334334
</td>
</tr>
<tr>
<td style="text-align:left;">
MLR: RLS+LLS+Power
</td>
<td style="text-align:right;">
0.202488
</td>
</tr>
</tbody>
</table>
<p>Note that the multiple regression model has arguably the ‘best’ (smallest) residual standard error of the four models considered – even though it includes the variable RLS which appears to not be important when LLS and Power are included.</p>
<p>The addition of a multiple predictors into the model also alters the nature of the question being asked.
The simple linear regression asks: “Does RLS influence hang time?”
The multiple linear regression, however, asks a more useful question: “Does RLS influence hang time once any differences due to LHS and Power are considered?”
The answer is ‘not really.’
It also asks: “Does LLS influence hang time once any differences due to RLS and Power are accounted for?”
The answer is ‘yes.’</p>
</div>
<div id="example-of-confounding-in-regression" class="section level3" number="5.4.2">
<h3><span class="header-section-number">5.4.2</span> Example of Confounding in Regression</h3>
<p>In the previous section, <a href="introduction-to-multiple-regression.html#example-of-spurious-correlation">5.4.1</a>, we saw how a combination of variables can change the interpretation of a variable in the presence of others.
In Section <a href="multiple-factor-designed-experiments.html#blocking">3.1</a> we discussed using <em>blocks</em> as a way to control for potential confounding factors, or nuisance variables.
By modeling this extra (in some cases <em>known</em>) variability, we can more accurately determine if the variables of interest influence the response variable.
In the below example we demonstrate that regression can be used in a similar way as a block design to control for nuisance variation.</p>
<p><strong>Example.</strong> Bleaching Pulp (from <span class="citation"><a href="#ref-WeissText" role="doc-biblioref">Weiss</a> (<a href="#ref-WeissText" role="doc-biblioref">2012</a>)</span>)</p>
<p>The production of paper requires the pupl used in the manufacturing process to be whitened by bleaching in a chemical reaction.
The bleaching agents are usually chlorine dioxide (ClO<span class="math inline">\(_2\)</span>) and hydrogen peroxide (H<span class="math inline">\(_2\)</span>O<span class="math inline">\(_2\)</span>).
The amounts of these two chemical used in the process effects the whiteness of the pulp, and ultimately that of the paper.</p>
<p>Original source: <span class="citation"><a href="#ref-Zhou1998" role="doc-biblioref">Zhou</a> (<a href="#ref-Zhou1998" role="doc-biblioref">1998</a>)</span></p>
<p>The file <code>bleachingPulp.txt</code> on the textbook site contains the result of an experiment where 20 combinations of chlorine dioxide and hydrogen peroxide were varied and the <em>brightness</em> or <em>whiteness</em> of the paper was determined (smaller the number, the brighter the paper).
The file is a tabbed separated value format</p>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb223-1"><a href="introduction-to-multiple-regression.html#cb223-1" aria-hidden="true" tabindex="-1"></a>site <span class="ot">&lt;-</span> <span class="st">&quot;https://tjfisher19.github.io/introStatModeling/data/bleachingPulp.txt&quot;</span></span>
<span id="cb223-2"><a href="introduction-to-multiple-regression.html#cb223-2" aria-hidden="true" tabindex="-1"></a>pulp <span class="ot">&lt;-</span> <span class="fu">read_tsv</span>(site, <span class="at">col_type=</span><span class="fu">cols</span>())</span>
<span id="cb223-3"><a href="introduction-to-multiple-regression.html#cb223-3" aria-hidden="true" tabindex="-1"></a><span class="fu">glimpse</span>(pulp)</span></code></pre></div>
<pre><code>## Rows: 20
## Columns: 3
## $ ClO2      &lt;dbl&gt; 2.06, 2.06, 2.06, 2.06, 2.06, 3.52, 3.52, 3.52, 3.52, 3.52, ~
## $ H2O2      &lt;dbl&gt; 0.0, 0.2, 0.4, 0.6, 0.8, 0.0, 0.2, 0.4, 0.6, 0.8, 0.0, 0.2, ~
## $ Whiteness &lt;dbl&gt; 10.55, 9.59, 8.98, 8.46, 7.73, 6.16, 5.55, 5.06, 4.79, 4.74,~</code></pre>
<p>Suppose a paper scientist wished to know the effect hydrogen peroxide has on the brightening process.
The scientist fits a simple linear regression modeling the whiteness as a function of the amount of hydrogen peroxide.</p>
<div class="sourceCode" id="cb225"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb225-1"><a href="introduction-to-multiple-regression.html#cb225-1" aria-hidden="true" tabindex="-1"></a>slr_h2o2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(Whiteness <span class="sc">~</span> H2O2, <span class="at">data=</span>pulp)</span>
<span id="cb225-2"><a href="introduction-to-multiple-regression.html#cb225-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(slr_h2o2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Whiteness ~ H2O2, data = pulp)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -2.733 -1.756 -0.755  1.179  4.596 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)     5.95       1.01    5.91  1.3e-05 ***
## H2O2           -2.06       2.05   -1.00     0.33    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 2.6 on 18 degrees of freedom
## Multiple R-squared:  0.0527, Adjusted R-squared:  7.12e-05 
## F-statistic:    1 on 1 and 18 DF,  p-value: 0.33</code></pre>
<p>From the summary output it appears that the amount of hydrogen peroxide does not influence the brightnes of the paper.
Further, we note the residual standard error is 2.59922 which is not all that different than the standard deviation of the <code>Whiteness</code> variable, 2.59931.</p>
<p>However, the above analysis is <strong>incorrect</strong> because we do not account for the potential effect of the chlorine dioxide (a <strong>confounding</strong> variable) – the experiment used both.
When we fit a multiple regression model we get a different result.</p>
<div class="sourceCode" id="cb227"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb227-1"><a href="introduction-to-multiple-regression.html#cb227-1" aria-hidden="true" tabindex="-1"></a>mlr_pulp <span class="ot">&lt;-</span> <span class="fu">lm</span>(Whiteness <span class="sc">~</span> H2O2 <span class="sc">+</span> ClO2, <span class="at">data=</span>pulp)</span>
<span id="cb227-2"><a href="introduction-to-multiple-regression.html#cb227-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(mlr_pulp)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Whiteness ~ H2O2 + ClO2, data = pulp)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -1.1019 -0.7244 -0.0817  0.6817  1.5107 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   11.939      0.626   19.08  6.4e-13 ***
## H2O2          -2.056      0.712   -2.89     0.01 *  
## ClO2          -1.407      0.122  -11.53  1.9e-09 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.901 on 17 degrees of freedom
## Multiple R-squared:  0.893,  Adjusted R-squared:  0.88 
## F-statistic: 70.6 on 2 and 17 DF,  p-value: 5.83e-09</code></pre>
<p>Based on the multiple regression model, it appears hydrogen peroxide does predict the brightness of the pulp.
Also note the decrease in the residual standard error.</p>
<p>For reference, the residual standard error for the simple linear regression with just chlorine dioxide is 1.06879.
So it is apparent that both predictor variables provide a better mode than either simple linear regression approach.</p>
</div>
</div>
<div id="concluding-the-multiple-regression-model" class="section level2" number="5.5">
<h2><span class="header-section-number">5.5</span> Concluding the Multiple Regression Model</h2>
<p>Hopefully the examples outlined in this chapter will give you an appreciation for why we must strive to find a good model for our data, and <strong>not</strong> <em>fit once</em> and “hope for the best.”
In observational studies, many different models may be fit to the same data, but that does not mean they are all good!
Findings and results can vary based on which model we choose: precision of our predictions, interpretations of the parameter estimates, etc., so we must use caution and wisdom in our choices.
In the following chapters we will cover methods that will help us in our model building.</p>
<p>As statistician George Box once said (<span class="citation"><a href="#ref-Box1979" role="doc-biblioref">Box</a> (<a href="#ref-Box1979" role="doc-biblioref">1979</a>)</span>):</p>
<blockquote>
<p>“All models are wrong but some are useful.”</p>
</blockquote>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-Box1979" class="csl-entry">
Box, G. E. P., Robustness in the Strategy of Scientific Model Building, in <em>Robustness in Statistics</em>, R. L. LAUNER and G. N. WILKINSON, Eds., Academic Press, pp. 201–36, 1979.
</div>
<div id="ref-VTpunting1983" class="csl-entry">
Department of Health, P. E., and Recreation, The Relationship Between Selected Physical Performance Variables and Football Punting Ability, Virginia Polytechnic Instituteand State University, 1983.
</div>
<div id="ref-KutnerText" class="csl-entry">
Kutner, M. H., Nachtsheim, C., Neter, J. and Li, W., <em>Applied Linear Statistical Models</em>, New York, NY: McGraw-Hill Irwin, 2004.
</div>
<div id="ref-McClaveSincich2008" class="csl-entry">
McClave, J. T., Benson, P. G. and Sincich, T., <em>Statistics for Business and Economics</em>, Pearson Prentice Hall, 2008.
</div>
<div id="ref-NavidiMonk2015" class="csl-entry">
Navidi, W. and Monk, B., <em>Elementary Statistics</em>, McGraw Hill, 2015.
</div>
<div id="ref-R-GGally" class="csl-entry">
Schloerke, B., Crowley, J., Cook, D., Briatte, F., Marbach, M., Thoen, E., Elberg, A. and Larmarange, J., <em>GGally: Extension to ’Ggplot2’</em>, from <a href="https://CRAN.R-project.org/package=GGally">https://CRAN.R-project.org/package=GGally</a>, 2018.
</div>
<div id="ref-Walpole2007" class="csl-entry">
Walpole, R. E., Myers, R. H., Myers, S. L. and Ye, K., <em>Probability &amp; Statistics for Engineers and Scientists</em>, Upper Saddle River: Pearson Education, 2007.
</div>
<div id="ref-WeissText" class="csl-entry">
Weiss, N. A., <em>Introductory Statistics</em>, Pearson Education, 2012.
</div>
<div id="ref-Zhou1998" class="csl-entry">
Zhou, J., Response Model for Bleaching Kraft Pulp, <em>Proceedings of American Statistical Association the Section on Quality and Productivity</em>, pp. 32–35, 1998.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="advanced-designs.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="inference-regarding-multiple-regression.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["introStatModeling.pdf", "introStatModeling.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
