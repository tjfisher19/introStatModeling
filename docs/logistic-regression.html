<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 12 Logistic Regression | Introduction to Statistical Modeling</title>
  <meta name="description" content="Covers Regression and elements of Design of Experiments in R using the tidyverse." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 12 Logistic Regression | Introduction to Statistical Modeling" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Covers Regression and elements of Design of Experiments in R using the tidyverse." />
  <meta name="github-repo" content="tjfisher19/introStatModeling" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 12 Logistic Regression | Introduction to Statistical Modeling" />
  
  <meta name="twitter:description" content="Covers Regression and elements of Design of Experiments in R using the tidyverse." />
  

<meta name="author" content="Michael R. Hughes and Thomas J. Fisher" />


<meta name="date" content="2022-01-05" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="statistical-odds.html"/>
<link rel="next" href="generalized-linear-models.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Inroduction to Statistical Modeling</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="chapter" data-level="" data-path="important-preliminary-review.html"><a href="important-preliminary-review.html"><i class="fa fa-check"></i>Important Preliminary Review</a>
<ul>
<li class="chapter" data-level="" data-path="important-preliminary-review.html"><a href="important-preliminary-review.html#statistics-background"><i class="fa fa-check"></i>Statistics background</a></li>
<li class="chapter" data-level="" data-path="important-preliminary-review.html"><a href="important-preliminary-review.html#software"><i class="fa fa-check"></i>Software</a></li>
<li class="chapter" data-level="" data-path="important-preliminary-review.html"><a href="important-preliminary-review.html#add-on-packages"><i class="fa fa-check"></i>Add-on packages</a></li>
<li class="chapter" data-level="" data-path="important-preliminary-review.html"><a href="important-preliminary-review.html#help-with-rmarkdown"><i class="fa fa-check"></i>Help with RMarkdown</a></li>
<li class="chapter" data-level="" data-path="important-preliminary-review.html"><a href="important-preliminary-review.html#managing-your-work-in-r"><i class="fa fa-check"></i>Managing your work in R</a></li>
<li class="chapter" data-level="" data-path="important-preliminary-review.html"><a href="important-preliminary-review.html#data-in-this-text"><i class="fa fa-check"></i>Data in this text</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html"><i class="fa fa-check"></i><b>1</b> Introductory Statistics in R</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#goals-of-a-statistical-analysis"><i class="fa fa-check"></i><b>1.1</b> Goals of a statistical analysis</a></li>
<li class="chapter" data-level="1.2" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#before-you-begin-an-analysis"><i class="fa fa-check"></i><b>1.2</b> Before you begin an analysis</a></li>
<li class="chapter" data-level="1.3" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#data-frames"><i class="fa fa-check"></i><b>1.3</b> Data frames</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#built-in-data"><i class="fa fa-check"></i><b>1.3.1</b> Built-in data</a></li>
<li class="chapter" data-level="1.3.2" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#types-of-data"><i class="fa fa-check"></i><b>1.3.2</b> Types of Data</a></li>
<li class="chapter" data-level="1.3.3" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#importing-datasets-into-r"><i class="fa fa-check"></i><b>1.3.3</b> Importing datasets into R</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#referencing-data-from-inside-a-data-frame"><i class="fa fa-check"></i><b>1.4</b> Referencing data from inside a data frame</a></li>
<li class="chapter" data-level="1.5" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#missing-values-and-computer-arithmetic-in-r"><i class="fa fa-check"></i><b>1.5</b> Missing values and computer arithmetic in R</a></li>
<li class="chapter" data-level="1.6" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#exploratory-data-analysis-eda"><i class="fa fa-check"></i><b>1.6</b> Exploratory Data Analysis (EDA)</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#numeric-summaries"><i class="fa fa-check"></i><b>1.6.1</b> Numeric Summaries</a></li>
<li class="chapter" data-level="1.6.2" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#numeric-summaries-in-r"><i class="fa fa-check"></i><b>1.6.2</b> Numeric Summaries in R</a></li>
<li class="chapter" data-level="1.6.3" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#graphical-summaries"><i class="fa fa-check"></i><b>1.6.3</b> Graphical Summaries</a></li>
<li class="chapter" data-level="1.6.4" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#distribution-of-univariate-variables"><i class="fa fa-check"></i><b>1.6.4</b> Distribution of Univariate Variables</a></li>
<li class="chapter" data-level="1.6.5" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#descriptive-statistics-and-visualizations-by-levels-of-a-factor-variable"><i class="fa fa-check"></i><b>1.6.5</b> Descriptive statistics and visualizations by levels of a factor variable</a></li>
<li class="chapter" data-level="1.6.6" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#descriptive-statistics-and-visualizations-for-two-numeric-variables"><i class="fa fa-check"></i><b>1.6.6</b> Descriptive statistics and visualizations for two numeric variables</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#sampling-distributions-describing-how-a-statistic-varies"><i class="fa fa-check"></i><b>1.7</b> Sampling distributions: describing how a statistic varies</a></li>
<li class="chapter" data-level="1.8" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#two-sample-inference"><i class="fa fa-check"></i><b>1.8</b> Two-sample inference</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html"><i class="fa fa-check"></i><b>2</b> Introduction to Statistical Modeling and Designed Experiments</a>
<ul>
<li class="chapter" data-level="2.1" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#statistical-analyses-is-modeling"><i class="fa fa-check"></i><b>2.1</b> Statistical Analyses is Modeling</a></li>
<li class="chapter" data-level="2.2" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#observational-studies-versus-designed-experiments"><i class="fa fa-check"></i><b>2.2</b> Observational Studies versus designed experiments</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#observational-studies"><i class="fa fa-check"></i><b>2.2.1</b> Observational Studies</a></li>
<li class="chapter" data-level="2.2.2" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#designed-experiments"><i class="fa fa-check"></i><b>2.2.2</b> Designed experiments</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#designed-experiement-vocabulary"><i class="fa fa-check"></i><b>2.3</b> Designed experiement vocabulary</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#what-is-an-experiment"><i class="fa fa-check"></i><b>2.3.1</b> What is an experiment?</a></li>
<li class="chapter" data-level="2.3.2" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#analysis-of-variance"><i class="fa fa-check"></i><b>2.3.2</b> Analysis of variance</a></li>
<li class="chapter" data-level="2.3.3" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#elements-of-a-designed-experiment"><i class="fa fa-check"></i><b>2.3.3</b> Elements of a designed experiment</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#paired-t-test"><i class="fa fa-check"></i><b>2.4</b> Paired <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="2.5" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#one-way-anova"><i class="fa fa-check"></i><b>2.5</b> One-Way ANOVA</a></li>
<li class="chapter" data-level="2.6" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#assumptionCheck"><i class="fa fa-check"></i><b>2.6</b> Assumption Checking</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#independence"><i class="fa fa-check"></i><b>2.6.1</b> Independence</a></li>
<li class="chapter" data-level="2.6.2" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#constant-variance"><i class="fa fa-check"></i><b>2.6.2</b> Constant Variance</a></li>
<li class="chapter" data-level="2.6.3" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#checking-normality"><i class="fa fa-check"></i><b>2.6.3</b> Checking Normality</a></li>
<li class="chapter" data-level="2.6.4" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#code-to-check-assumption"><i class="fa fa-check"></i><b>2.6.4</b> Code to check assumption</a></li>
<li class="chapter" data-level="2.6.5" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#transforming-your-response"><i class="fa fa-check"></i><b>2.6.5</b> Transforming your response</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#follow-up-procedures-multiple-comparisons"><i class="fa fa-check"></i><b>2.7</b> Follow-up procedures – Multiple Comparisons</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#tukeys-hsd-method"><i class="fa fa-check"></i><b>2.7.1</b> Tukey’s HSD method</a></li>
<li class="chapter" data-level="2.7.2" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#dunnett-multiple-comparisons"><i class="fa fa-check"></i><b>2.7.2</b> Dunnett multiple comparisons</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="multiple-factor-designed-experiments.html"><a href="multiple-factor-designed-experiments.html"><i class="fa fa-check"></i><b>3</b> Multiple Factor Designed Experiments</a>
<ul>
<li class="chapter" data-level="3.1" data-path="multiple-factor-designed-experiments.html"><a href="multiple-factor-designed-experiments.html#blocking"><i class="fa fa-check"></i><b>3.1</b> Blocking</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="multiple-factor-designed-experiments.html"><a href="multiple-factor-designed-experiments.html#data-structure-model-form-and-analysis-of-variance-of-a-randomized-block-design"><i class="fa fa-check"></i><b>3.1.1</b> Data structure, model form and analysis of variance of a Randomized Block Design</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="multiple-factor-designed-experiments.html"><a href="multiple-factor-designed-experiments.html#two-factor-designs"><i class="fa fa-check"></i><b>3.2</b> Two-factor Designs</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="multiple-factor-designed-experiments.html"><a href="multiple-factor-designed-experiments.html#analysis"><i class="fa fa-check"></i><b>3.2.1</b> Analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="advanced-designs.html"><a href="advanced-designs.html"><i class="fa fa-check"></i><b>4</b> Advanced Designs</a>
<ul>
<li class="chapter" data-level="4.1" data-path="advanced-designs.html"><a href="advanced-designs.html#higher-order-factor-models"><i class="fa fa-check"></i><b>4.1</b> Higher order factor models</a></li>
<li class="chapter" data-level="4.2" data-path="advanced-designs.html"><a href="advanced-designs.html#within-subject-designs"><i class="fa fa-check"></i><b>4.2</b> Within-subject designs</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="advanced-designs.html"><a href="advanced-designs.html#blocks-revisited-an-approach-to-handling-within-subjects-factors"><i class="fa fa-check"></i><b>4.2.1</b> Blocks revisited: an approach to handling within-subjects factors</a></li>
<li class="chapter" data-level="4.2.2" data-path="advanced-designs.html"><a href="advanced-designs.html#a-more-involved-repeated-measures-case-study"><i class="fa fa-check"></i><b>4.2.2</b> A more involved repeated measures case study</a></li>
<li class="chapter" data-level="4.2.3" data-path="advanced-designs.html"><a href="advanced-designs.html#further-study"><i class="fa fa-check"></i><b>4.2.3</b> Further study</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html"><i class="fa fa-check"></i><b>5</b> Introduction to Multiple Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#regression-model"><i class="fa fa-check"></i><b>5.1</b> Regression Model</a></li>
<li class="chapter" data-level="5.2" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#fitting-a-regression-model"><i class="fa fa-check"></i><b>5.2</b> Fitting a regression model</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#multiple-regression-example"><i class="fa fa-check"></i><b>5.2.1</b> Multiple Regression Example</a></li>
<li class="chapter" data-level="5.2.2" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#why-should-we-use-more-than-one-predictor"><i class="fa fa-check"></i><b>5.2.2</b> Why should we use more than one predictor?</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#interpreting-beta-parameter-estimates-in-mlr"><i class="fa fa-check"></i><b>5.3</b> Interpreting <span class="math inline">\(\beta\)</span>-parameter estimates in MLR</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#designed-experiments-1"><i class="fa fa-check"></i><b>5.3.1</b> Designed experiments</a></li>
<li class="chapter" data-level="5.3.2" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#observational-studies-1"><i class="fa fa-check"></i><b>5.3.2</b> Observational studies</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html"><i class="fa fa-check"></i><b>6</b> Inference regarding Multiple Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#assumption-checking"><i class="fa fa-check"></i><b>6.1</b> Assumption checking</a></li>
<li class="chapter" data-level="6.2" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#overall-f-test-for-model-signifance"><i class="fa fa-check"></i><b>6.2</b> Overall <span class="math inline">\(F\)</span>-test for model signifance</a></li>
<li class="chapter" data-level="6.3" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#individual-parameter-inference"><i class="fa fa-check"></i><b>6.3</b> Individual parameter inference</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#t-tests"><i class="fa fa-check"></i><b>6.3.1</b> <span class="math inline">\(t\)</span>-tests</a></li>
<li class="chapter" data-level="6.3.2" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#confidence-intervals"><i class="fa fa-check"></i><b>6.3.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="6.3.3" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#confidence-and-prediction-bands"><i class="fa fa-check"></i><b>6.3.3</b> Confidence and prediction bands</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#goodness-of-fit"><i class="fa fa-check"></i><b>6.4</b> Goodness-of-fit</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#coefficient-of-determination"><i class="fa fa-check"></i><b>6.4.1</b> Coefficient of determination</a></li>
<li class="chapter" data-level="6.4.2" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#akaikes-information-criterion"><i class="fa fa-check"></i><b>6.4.2</b> Akaike’s Information Criterion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="more-on-multiple-linear-regression.html"><a href="more-on-multiple-linear-regression.html"><i class="fa fa-check"></i><b>7</b> More on multiple linear regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="more-on-multiple-linear-regression.html"><a href="more-on-multiple-linear-regression.html#model-comparision-reduced-f-tests"><i class="fa fa-check"></i><b>7.1</b> Model comparision – Reduced <span class="math inline">\(F\)</span>-tests</a></li>
<li class="chapter" data-level="7.2" data-path="more-on-multiple-linear-regression.html"><a href="more-on-multiple-linear-regression.html#categorical-predictor-variables"><i class="fa fa-check"></i><b>7.2</b> Categorical Predictor Variables</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="more-on-multiple-linear-regression.html"><a href="more-on-multiple-linear-regression.html#a-qualitative-predictor-with-two-levels"><i class="fa fa-check"></i><b>7.2.1</b> A qualitative predictor with two levels</a></li>
<li class="chapter" data-level="7.2.2" data-path="more-on-multiple-linear-regression.html"><a href="more-on-multiple-linear-regression.html#a-qualitative-predictor-with-more-than-two-levels"><i class="fa fa-check"></i><b>7.2.2</b> A qualitative predictor with more than two levels</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="more-on-multiple-linear-regression.html"><a href="more-on-multiple-linear-regression.html#bridging-regression-and-designed-experiments-ancova"><i class="fa fa-check"></i><b>7.3</b> Bridging Regression and Designed Experiments – ANCOVA</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="more-on-multiple-linear-regression.html"><a href="more-on-multiple-linear-regression.html#an-ancova-example-with-a-two-level-factor"><i class="fa fa-check"></i><b>7.3.1</b> An ANCOVA example with a two-level factor</a></li>
<li class="chapter" data-level="7.3.2" data-path="more-on-multiple-linear-regression.html"><a href="more-on-multiple-linear-regression.html#ancova-with-a-multi-level-factor"><i class="fa fa-check"></i><b>7.3.2</b> ANCOVA with a multi-level factor</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="model-building-considerations.html"><a href="model-building-considerations.html"><i class="fa fa-check"></i><b>8</b> Model Building Considerations</a>
<ul>
<li class="chapter" data-level="8.1" data-path="model-building-considerations.html"><a href="model-building-considerations.html#regression-assumptions-revisited"><i class="fa fa-check"></i><b>8.1</b> Regression assumptions revisited</a></li>
<li class="chapter" data-level="8.2" data-path="model-building-considerations.html"><a href="model-building-considerations.html#violations-of-the-independence-assumption"><i class="fa fa-check"></i><b>8.2</b> Violations of the independence assumption</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="model-building-considerations.html"><a href="model-building-considerations.html#collecting-data-that-are-temporal-or-spatial-in-nature"><i class="fa fa-check"></i><b>8.2.1</b> Collecting data that are temporal or spatial in nature</a></li>
<li class="chapter" data-level="8.2.2" data-path="model-building-considerations.html"><a href="model-building-considerations.html#pseudoreplication"><i class="fa fa-check"></i><b>8.2.2</b> Pseudoreplication</a></li>
<li class="chapter" data-level="8.2.3" data-path="model-building-considerations.html"><a href="model-building-considerations.html#what-if-we-have-non-independent-errors"><i class="fa fa-check"></i><b>8.2.3</b> What if we have non-independent errors?</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="model-building-considerations.html"><a href="model-building-considerations.html#constant-variance-violations"><i class="fa fa-check"></i><b>8.3</b> Constant Variance Violations</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="model-building-considerations.html"><a href="model-building-considerations.html#box-cox-power-tranformations"><i class="fa fa-check"></i><b>8.3.1</b> Box-Cox Power Tranformations</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="model-building-considerations.html"><a href="model-building-considerations.html#normality-violations"><i class="fa fa-check"></i><b>8.4</b> Normality violations</a></li>
<li class="chapter" data-level="8.5" data-path="model-building-considerations.html"><a href="model-building-considerations.html#violations-of-the-linearity-assumption"><i class="fa fa-check"></i><b>8.5</b> Violations of the linearity assumption</a></li>
<li class="chapter" data-level="8.6" data-path="model-building-considerations.html"><a href="model-building-considerations.html#detecting-and-dealing-with-unusual-observations"><i class="fa fa-check"></i><b>8.6</b> Detecting and dealing with unusual observations</a></li>
<li class="chapter" data-level="8.7" data-path="model-building-considerations.html"><a href="model-building-considerations.html#multicollinearity"><i class="fa fa-check"></i><b>8.7</b> Multicollinearity</a></li>
<li class="chapter" data-level="8.8" data-path="model-building-considerations.html"><a href="model-building-considerations.html#standardizingPredictors"><i class="fa fa-check"></i><b>8.8</b> Scale changes</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="model-selection.html"><a href="model-selection.html"><i class="fa fa-check"></i><b>9</b> Model Selection</a>
<ul>
<li class="chapter" data-level="9.1" data-path="model-selection.html"><a href="model-selection.html#stepwise-procedures"><i class="fa fa-check"></i><b>9.1</b> Stepwise Procedures</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="model-selection.html"><a href="model-selection.html#backward-selection"><i class="fa fa-check"></i><b>9.1.1</b> Backward Selection</a></li>
<li class="chapter" data-level="9.1.2" data-path="model-selection.html"><a href="model-selection.html#forward-selection"><i class="fa fa-check"></i><b>9.1.2</b> Forward selection</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="model-selection.html"><a href="model-selection.html#best-subsets"><i class="fa fa-check"></i><b>9.2</b> Best subsets</a></li>
<li class="chapter" data-level="9.3" data-path="model-selection.html"><a href="model-selection.html#shrinkage-methods"><i class="fa fa-check"></i><b>9.3</b> Shrinkage Methods</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="model-validation.html"><a href="model-validation.html"><i class="fa fa-check"></i><b>10</b> Model Validation</a>
<ul>
<li class="chapter" data-level="10.1" data-path="model-validation.html"><a href="model-validation.html#underfitting-vs.-overfitting-models"><i class="fa fa-check"></i><b>10.1</b> Underfitting vs. Overfitting Models</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="model-validation.html"><a href="model-validation.html#the-bias-variance-trade-off"><i class="fa fa-check"></i><b>10.1.1</b> The Bias-Variance Trade-off</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="model-validation.html"><a href="model-validation.html#validation-techniques"><i class="fa fa-check"></i><b>10.2</b> Validation Techniques</a></li>
<li class="chapter" data-level="10.3" data-path="model-validation.html"><a href="model-validation.html#basic-validation-with-a-single-holdout-sample"><i class="fa fa-check"></i><b>10.3</b> Basic Validation with a single holdout sample</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="model-validation.html"><a href="model-validation.html#use-the-training-data-to-fit-and-select-models"><i class="fa fa-check"></i><b>10.3.1</b> Use the training data to fit and select models</a></li>
<li class="chapter" data-level="10.3.2" data-path="model-validation.html"><a href="model-validation.html#model-training"><i class="fa fa-check"></i><b>10.3.2</b> Model training:</a></li>
<li class="chapter" data-level="10.3.3" data-path="model-validation.html"><a href="model-validation.html#model-validation-step"><i class="fa fa-check"></i><b>10.3.3</b> Model validation step:</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="model-validation.html"><a href="model-validation.html#hold-out-sample-validation-using-caret"><i class="fa fa-check"></i><b>10.4</b> Hold-out sample validation using <code>caret</code></a></li>
<li class="chapter" data-level="10.5" data-path="model-validation.html"><a href="model-validation.html#leave-one-out-cross-validation-loocv"><i class="fa fa-check"></i><b>10.5</b> “Leave one out” Cross-Validation (LOOCV)</a></li>
<li class="chapter" data-level="10.6" data-path="model-validation.html"><a href="model-validation.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>10.6</b> <span class="math inline">\(k\)</span>-fold Cross-Validation</a></li>
<li class="chapter" data-level="10.7" data-path="model-validation.html"><a href="model-validation.html#a-final-note"><i class="fa fa-check"></i><b>10.7</b> A final note</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="statistical-odds.html"><a href="statistical-odds.html"><i class="fa fa-check"></i><b>11</b> Statistical Odds</a>
<ul>
<li class="chapter" data-level="11.1" data-path="statistical-odds.html"><a href="statistical-odds.html#probability-versus-odds"><i class="fa fa-check"></i><b>11.1</b> Probability versus Odds</a></li>
<li class="chapter" data-level="11.2" data-path="statistical-odds.html"><a href="statistical-odds.html#odds-ratios"><i class="fa fa-check"></i><b>11.2</b> Odds ratios</a></li>
<li class="chapter" data-level="11.3" data-path="statistical-odds.html"><a href="statistical-odds.html#ideas-of-modeling-odds"><i class="fa fa-check"></i><b>11.3</b> Ideas of modeling odds</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>12</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="12.1" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-model"><i class="fa fa-check"></i><b>12.1</b> Logistic Model</a></li>
<li class="chapter" data-level="12.2" data-path="logistic-regression.html"><a href="logistic-regression.html#fitting-interpreting-and-assessing-a-logistic-model"><i class="fa fa-check"></i><b>12.2</b> Fitting, Interpreting and assessing a logistic model</a></li>
<li class="chapter" data-level="12.3" data-path="logistic-regression.html"><a href="logistic-regression.html#case-study---titanic-dataset"><i class="fa fa-check"></i><b>12.3</b> Case Study - Titanic Dataset</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>13</b> Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="13.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#poisson-regression"><i class="fa fa-check"></i><b>13.1</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#poisson-distribution"><i class="fa fa-check"></i><b>13.1.1</b> Poisson distribution</a></li>
<li class="chapter" data-level="13.1.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#poisson-regression-development"><i class="fa fa-check"></i><b>13.1.2</b> Poisson Regression Development</a></li>
<li class="chapter" data-level="13.1.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#example---tropical-cyclone-counts-in-the-north-atlantic"><i class="fa fa-check"></i><b>13.1.3</b> Example - Tropical Cyclone Counts in the North Atlantic</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#handling-overdispersion"><i class="fa fa-check"></i><b>13.2</b> Handling overdispersion</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#example-attendnace-records"><i class="fa fa-check"></i><b>13.2.1</b> Example – Attendnace Records</a></li>
<li class="chapter" data-level="13.2.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#incorrect-poisson-model"><i class="fa fa-check"></i><b>13.2.2</b> Incorrect Poisson Model</a></li>
<li class="chapter" data-level="13.2.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#a-quasi-poisson-approach"><i class="fa fa-check"></i><b>13.2.3</b> A quasi-Poisson approach</a></li>
<li class="chapter" data-level="13.2.4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#fitting-a-negative-binomial-regression"><i class="fa fa-check"></i><b>13.2.4</b> Fitting a Negative Binomial regression</a></li>
<li class="chapter" data-level="13.2.5" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#picking-between-quasi-poisson-and-negative-binomial"><i class="fa fa-check"></i><b>13.2.5</b> Picking between Quasi-Poisson and Negative Binomial</a></li>
<li class="chapter" data-level="13.2.6" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#infererence-on-predictor-variables"><i class="fa fa-check"></i><b>13.2.6</b> Infererence on predictor variables</a></li>
<li class="chapter" data-level="13.2.7" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#plotting-fitted-model"><i class="fa fa-check"></i><b>13.2.7</b> Plotting fitted model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Statistical Modeling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="logistic-regression" class="section level1" number="12">
<h1><span class="header-section-number">Chapter 12</span> Logistic Regression</h1>
<p>Earlier, we introduced diagnostics to check the assumptions in a standard regression analysis. Recall these assumptions were that the errors <span class="math inline">\(\varepsilon_i\)</span> are independent <span class="math inline">\(N(0, \sigma^2)\)</span>; i.e.,</p>
<ol style="list-style-type: decimal">
<li>the <span class="math inline">\(\varepsilon_i\)</span> are independent.</li>
<li>the <span class="math inline">\(\varepsilon_i\)</span> have constant variance,</li>
<li>the <span class="math inline">\(\varepsilon_i\)</span> are normally distributed,</li>
</ol>
<p>In many cases, remedial measures such as transformations, weighted least squares, etc., can help validate the assumptions. However, some kinds of responses by their very nature will violate these assumptions. An easy-to-see example is the dichotomous responses from the previous chapter (odds of surviving the sinking of the Titanic). In cases like this, it is much preferable to apply an appropriate modeling technique that acknowledges the true nature of the response, rather than putting the data through contortions in an attempt to “fit a square peg in a round hole.”</p>
<p><strong>Example: Predicting an election outcome.</strong> Suppose we are interested in factors that influence whether or not a political candidate wins an election. Predictor variables might include the amount of money spent on the campaign, the amount of time spent campaigning negatively, whether or
not the candidate is an incumbent, etc.</p>
<p>What makes this problem different from problems we’ve seen before? It’s that the outcome (i.e., the response variable <span class="math inline">\(Y\)</span>) is not quantitative; rather, it is binary: the outcome is “win” or “lose.” We could code these numerically as 0 and 1 (lose = 0, win = 1). Because the response variable is
binary, we need to use a model that handles binary outcomes correctly.</p>
<p>In situations as the above we use a <strong>Generalized Linear Model</strong>, or GLM. A GLM is a “generalization” of the ideas in the more
typical “normal” regression modeling, but with the following added flexibility:</p>
<ol style="list-style-type: decimal">
<li>Errors, and thus the response variable, need not be normally distributed. For binary data, a more appropriate choice for
the errors is the binomial distribution. For count data, the Poisson distribution may be a more appropriate choice. Most introductory statistics textbooks cover these distributions in detail.</li>
<li>Error variance need not be constant. In fact, part of the reason the binomial and Poisson distributions are more appropriate in certain cases is because in these distributions, the mean and variance are linked: so if the mean changes, so does the variance. It’s a built-in property.</li>
<li>Linearity is not assumed. In GLM, we use what is known as a link function <span class="math inline">\(g(Y)\)</span> to connect the response <span class="math inline">\(Y\)</span> to the linear combination of the predictors <span class="math inline">\(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_k X_k\)</span> on the right-hand side of our model.</li>
</ol>
<p>If the response <span class="math inline">\(Y\)</span> is binary (0 or 1), we can view the property of interested for the response response to be the probability of an event occurring. For example, if win = 1 and lose = 0, then we can think of the underlying problem as modeling the probability of winning the election, <span class="math inline">\(P(win)\)</span>. We then use regression to model the likelihood of winning as a function of the predictors (amount of money spent on the campaign, etc.). We might even like to predict the probability of winning based on the specific amount of money spent on the campaign, the amount of time spent campaigning negatively, whether or not the candidate is an incumbent, etc. From this perspective, clearly the response <span class="math inline">\(Y = P(win)\)</span> must be between 0 and 1, because probabilities must be between 0 and 1. However, standard regression methods do not ensure this will happen (e.g., what if your model predicted a 113% chance of winning the election?!). The use of an
appropriate link function will ensure an appropriate prediction.</p>
<div id="logistic-model" class="section level2" number="12.1">
<h2><span class="header-section-number">12.1</span> Logistic Model</h2>
<p>From a practical standpoint, logistic regression and least squares regression are similar. Both methods produce prediction equations. In both cases, the regression <span class="math inline">\(\beta\)</span>-coefficients measure the predictive capability of the predictor variables.</p>
<p>The response variable that characterizes logistic regression is what makes it special. With linear least squares regression, the response variable <span class="math inline">\(Y\)</span> is a quantitative variable. With logistic regression, the response variable is an indicator of some characteristic; that is, a 0 or 1 variable. Logistic regression is used to determine whether other measurements are related to the presence or absence of some characteristic.</p>
<p>Here are a few examples of problems suited to logistic regression:</p>
<ul>
<li>Loan defaults. With the mortgage loan crisis afflicting the markets during the 2007-2009 recession, it is especially important for banks to know whether a prospective borrower will be at a high risk of defaulting on a loan. Historical data for past borrowers might includes annual income, amount of the loan, amount in savings or assets, years of work, and credit rating as predictors. The outcome of interest is whether or not a borrower defaulted on their loan.</li>
<li>Heart attack risk. We wish to study the influence of age, gender and exercise on whether or not someone has a heart attack. Again, we have a binary response variable, whether or not a heart attack occurs.</li>
<li>Graduate school admission. How do predictors such as GRE (Graduate Record Exam) scores, GPA, and prestige of the undergraduate program effect admission into graduate school? The response variable, whether or not a student is admitted, is a binary variable.</li>
</ul>
<p>While the response variable in a logistic regression is a 0/1 variable, the logistic regression equation, which is a linear equation, does not predict the 0/1 variable itself. Instead, logistic regression predicts the probability that an indicator variable is equal to 1. To be more precise, a logistic regression equation does not directly predict the probability that the indicator is equal to 1, <span class="math inline">\(P(Y = 1)\)</span>. Rather, it predicts the log odds that an observation will have an indicator equal to 1.</p>
<p><strong>Odds.</strong> The odds of an event is defined as the ratio of the probability that an event occurs to the probability that it fails to occur. Let <span class="math inline">\(p\)</span> denote the probability of an event of interest occurring, i.e., let <span class="math inline">\(p = P(Y = 1)\)</span>. Then the odds of the event is given by <span class="math inline">\(p/(1-p)\)</span></p>
<p>The general form of a (multiple) logistic regression model always expressed in terms of the log odds:</p>
<p><span class="math display">\[\log\left(\frac{p}{1-p}\right) = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots \beta_k X_k\]</span>
The link function <span class="math inline">\(\log\left(p/(1-p)\right)\)</span> is call the <em>logit</em> transformation. The logit ensures that our model will produce estimates of <span class="math inline">\(p\)</span> between 0 and 1. The quantity <span class="math inline">\(p/(1 – p)\)</span> is the odds of the event of interest occurring. The only downside of this transformation is that the linear predictor <span class="math inline">\(\beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots \beta_k X_k\)</span> is in terms of “log odds,” which is not easy to interpret. However, once we fit a model, we can back-transform in order to express our predictions as probabilities.</p>
<p>Here are some facts to keep in mind:</p>
<ul>
<li>Probabilities of events (e.g. <span class="math inline">\(p\)</span>) must lie between 0 and 1, with 1⁄2 as a neutral value for which both outcomes are equally likely. The constraints at 0 and 1 make it impossible to construct a linear equation for predicting probabilities.</li>
<li>Odds, on the other hand, lie between 0 and <span class="math inline">\(+\infty\)</span>, with 1 as a neutral value for which both outcomes are equally likely.</li>
<li>Odds are asymmetric: when the roles of the two outcomes are switched, each value in the range 0 to 1 is transformed by taking its inverse (1/value) to a value in the range 1 to <span class="math inline">\(+\infty\)</span>. For example, if the odds of having a low birthweight baby are 1/4, then the odds of not having a low birth weight baby is 4/1.</li>
<li>Log odds are symmetric. They lie in the range <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(+\infty\)</span>. The value for which both outcomes are equally likely is 0. When the roles of the two outcomes are switched, the log odds are multiplied by –1, since <span class="math inline">\(\log(a/b) = –\log(b/a)\)</span>. For example, if the log odds of having a low birth weight baby are –1.39, the odds of not having a low birth weight baby are 1.39.</li>
</ul>
<p>Those new to log odds can take comfort in knowing that as the probability of something increases, the odds and log odds increase too. Talking about the behavior of the log odds an event is qualitatively the same thing as talking about the behavior of the probability of the event. Because log odds take on any value between <span class="math inline">\(-\infty\)</span> to <span class="math inline">\(+\infty\)</span>, the <span class="math inline">\(\beta\)</span>-coefficients from a logistic regression equation can be interpreted in the usual way, namely, they represent the change in log odds of the response per unit change in the predictor.</p>
<p><strong>Example.</strong> Suppose we fit a logistic regression model to a sample of postmenopausal women, where the response variable <span class="math inline">\(Y\)</span> equals 1 if a subject is osteoporotic and 0 otherwise. The predictor variable of interest is age.</p>
<p>Let <span class="math inline">\(p\)</span> = the probability that a postmenopausal woman is osteoporotic. Suppose our fitted model is</p>
<p><span class="math display">\[\log\left(\frac{\hat{p}}{1-\hat{p}}\right) = -4353 + 0.038(age)\]</span></p>
<p>Since the coefficient for age is positive, the log odds (and, therefore, the probability) of osteoporosis increases with age. Taking anti-logarithms of both sides gives</p>
<p><span class="math display">\[\frac{\hat{p}}{1-\hat{p}} = e^{-4353 + 0.038(age)}\]</span></p>
<p>With a little algebraic manipulation, one can express the fitted model in terms of the predicted probability of osteoporosis <span class="math inline">\(\hat{\pi}\)</span></p>
<p><span class="math display">\[\hat{p} = \displaystyle\frac{\displaystyle e^{-4353 + 0.038(age)}}{\displaystyle 1+e^{-4353 + 0.038(age)}}\]</span></p>
<p>We’ll see what such an equation looks like plotted with a later example.</p>
</div>
<div id="fitting-interpreting-and-assessing-a-logistic-model" class="section level2" number="12.2">
<h2><span class="header-section-number">12.2</span> Fitting, Interpreting and assessing a logistic model</h2>
<p>If <span class="math inline">\(b_1\)</span> is the “slope” estimate for age in the logistic regression example above, then <span class="math inline">\(e^b_1\)</span> is the odds ratio corresponding to a one unit increase in age. For example, if age equals some amount <span class="math inline">\(w\)</span>, then the predicted odds of having osterporosis at age <span class="math inline">\(w\)</span> is given by</p>
<p><span class="math display">\[e^{-4.53 + 0.038w}\]</span></p>
<p>while one year later (i.e. at age <span class="math inline">\(w + 1\)</span>) the predicted odds are given by</p>
<p><span class="math display">\[e^{-4.53 + 0.038(w+1)}\]</span>
Dividing one equation by the other yields something interesting:</p>
<p><span class="math display">\[\frac{odds~of~osteroporosis~at~age=w+1}{odds~of~osteroporosis~at~age=w} = \frac{e^{-4.53 + 0.038(w+1)}}{e^{-4.53 + 0.038(w)}} = e^{0.038} = 1.0387\]</span></p>
<p>Thus, the odds that an older female has osteoporosis are 1.0387 times the odds of an female one year younger. This may alternatively be stated to mean that the odds of osteoporosis increases 3.87% over that of a younger individual with each year of age. Note that this percentage change in the odds is <span class="math inline">\(e^b_1 − 1\)</span>. For a 10 year age difference, the increase is <span class="math inline">\(e^{b_1\times 10} = e^{(0.038\times 10)} = 1.0387^10 = 1.46\)</span>, or a 46% increase.</p>
<p><strong>Example: Tadpole exposure to UV light.</strong> An experiment was performed that involved the exposure of 80 tadpoles to different level of ultraviolet light. There were 10 levels of UV light exposure (expressed as percentage of full exposure): 0% (dark), 1%, 9%, 16%, 23%, 31%, 40%, 57%, 78%, and 100%. The survival status of each tadpole is recorded after 21 days (variable Survival: 0=died, 1=survived). The researcher is most interested in looking at how well UV exposure predicts mortality. (The data are courtesy of Claire Meikle, and appear in the R workspace tadpoleUV.RData.)</p>
<div class="sourceCode" id="cb397"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb397-1"><a href="logistic-regression.html#cb397-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;tadpoleUV.RData&quot;</span>)</span>
<span id="cb397-2"><a href="logistic-regression.html#cb397-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(tadUV)</span></code></pre></div>
<pre><code>##   Day.Exposed UV.Level Days.survived Survived
## 1           2     0.01             7        0
## 2           2     1.00             1        0
## 3           2     0.23             2        0
## 4           1     0.00             9        0
## 5           1     0.16            19        0
## 6           1     0.16             5        0</code></pre>
<p>First, we must recognize that the response variable, <code>Survived</code>, is a binary variable. The predictor of survival in the model is <code>UV.Level</code>. A plot of the data is below, and might look a little strange compared to data plots we’ve seen to date.</p>
<div class="sourceCode" id="cb399"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb399-1"><a href="logistic-regression.html#cb399-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(tadUV) <span class="sc">+</span> </span>
<span id="cb399-2"><a href="logistic-regression.html#cb399-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x=</span>UV.Level, <span class="at">y=</span>Survived)) <span class="sc">+</span> </span>
<span id="cb399-3"><a href="logistic-regression.html#cb399-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code></pre></div>
<p><img src="introStatModeling_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
<p>One thing we can see is that there is a higher prevalence of mortality under higher UV conditions. So there may be an association between survival and exposure.</p>
<p>We model <span class="math inline">\(p = P(survival)\)</span> as a function of UV level as follows:</p>
<div class="sourceCode" id="cb400"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb400-1"><a href="logistic-regression.html#cb400-1" aria-hidden="true" tabindex="-1"></a>tadpole.fit <span class="ot">&lt;-</span> <span class="fu">glm</span>(Survived <span class="sc">~</span> UV.Level, <span class="at">data=</span>tadUV, <span class="at">family=</span>binomial)</span>
<span id="cb400-2"><a href="logistic-regression.html#cb400-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(tadpole.fit)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Survived ~ UV.Level, family = binomial, data = tadUV)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -0.694  -0.632  -0.451  -0.294   2.161  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)   
## (Intercept)   -1.300      0.464   -2.80   0.0051 **
## UV.Level      -2.330      1.471   -1.58   0.1131   
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 60.283  on 79  degrees of freedom
## Residual deviance: 57.022  on 78  degrees of freedom
## AIC: 61.02
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>The fitted logistic regression model is <span class="math inline">\(\log\left(\frac{\hat{p}}{1-\hat{p}}\right) = -1.300 - 0.0233(UV~level)\)</span>.</p>
<p>Here is some model info and interpretation:</p>
<ul>
<li>In the summary table, the column <code>Pr(&gt;|z|)</code> contains the p-values for individual tests of model parameters. The row labeled <code>UV.Level</code> is a test of <span class="math inline">\(H_0 : \beta_1 = 0\)</span>, i.e. whether UV level is a significant predictor of the probability of survival. Since the <em>p</em>-value is large (0.113) we conclude that there is no significant relationship between the probability of survival and UV level. This is known as the <em>Wald</em> test for parameter <span class="math inline">\(\beta_j\)</span>.</li>
<li>Even though there is not a significant relationship, we can still quantify this relationship by exponentiating <span class="math inline">\(b_1 = –0.0233\)</span></li>
</ul>
<div class="sourceCode" id="cb402"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb402-1"><a href="logistic-regression.html#cb402-1" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">coef</span>(tadpole.fit)[<span class="dv">2</span>])</span></code></pre></div>
<pre><code>##  UV.Level 
## 0.0972602</code></pre>
<p>Recall that the percentage change in the odds for each one unit increase in the predictor can be found via <span class="math inline">\(e^b_1 − 1\)</span>. Thus, percentage change in the odds of survival for each 1% increase in UV is <span class="math inline">\(0.9769658 – 1 = –0.023\)</span>; i.e., a 2.3% decrease in the survival odds per 1% increase in UV exposure.</p>
<ul>
<li>We can find Wald based confidence intervals for the percentage change in the odds by using the R function <code>confint()</code></li>
</ul>
<div class="sourceCode" id="cb404"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb404-1"><a href="logistic-regression.html#cb404-1" aria-hidden="true" tabindex="-1"></a><span class="fu">exp</span>(<span class="fu">confint</span>(tadpole.fit)) <span class="sc">-</span> <span class="dv">1</span></span></code></pre></div>
<pre><code>## Waiting for profiling to be done...</code></pre>
<pre><code>##                 2.5 %    97.5 %
## (Intercept) -0.897605 -0.351891
## UV.Level    -0.996857  0.192072</code></pre>
<p>Note that the CI for the percentage change in survival odds for a 1% increase in UV level contains 0, i.e. there is no significant effect on survival odds.</p>
<p>There is no whole-model <span class="math inline">\(F\)</span>-test in logistic regression, but we can construct a whole-model test called a likelihood ratio chi-square test using the <code>anova()</code> model comparison function:</p>
<div class="sourceCode" id="cb407"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb407-1"><a href="logistic-regression.html#cb407-1" aria-hidden="true" tabindex="-1"></a>null.model <span class="ot">&lt;-</span> <span class="fu">glm</span>(Survived <span class="sc">~</span> <span class="dv">1</span>, <span class="at">data=</span>tadUV, <span class="at">family=</span>binomial)</span>
<span id="cb407-2"><a href="logistic-regression.html#cb407-2" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(null.model, tadpole.fit, <span class="at">test=</span><span class="st">&quot;Chisq&quot;</span>)</span></code></pre></div>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: Survived ~ 1
## Model 2: Survived ~ UV.Level
##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)  
## 1        79      60.28                       
## 2        78      57.02  1    3.262   0.0709 .
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The <span class="math inline">\(p\)</span>-value is large (0.07092), indicating that the model fits are not significantly different; i.e., this result says that the model using UV level as a predictor is not significantly better than a model containing no predictors at all. This corroborates the earlier test result for <span class="math inline">\(\beta_1\)</span>.</p>
<p><strong>Probability prediction.</strong> Since odds ratios can be hard to interpret, we can use predicted probabilities to interpret results. For example, here is how to have R obtain the predicted probability of tadpole survival at 40% UV exposure and 75% UV exposure:</p>
<div class="sourceCode" id="cb409"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb409-1"><a href="logistic-regression.html#cb409-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(tadpole.fit, <span class="at">newdata=</span><span class="fu">data.frame</span>(<span class="at">UV.Level=</span><span class="fl">0.40</span>), <span class="at">type=</span><span class="st">&quot;response&quot;</span>)</span></code></pre></div>
<pre><code>##         1 
## 0.0968759</code></pre>
<div class="sourceCode" id="cb411"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb411-1"><a href="logistic-regression.html#cb411-1" aria-hidden="true" tabindex="-1"></a><span class="fu">predict</span>(tadpole.fit, <span class="at">newdata=</span><span class="fu">data.frame</span>(<span class="at">UV.Level=</span><span class="fl">0.75</span>), <span class="at">type=</span><span class="st">&quot;response&quot;</span>)</span></code></pre></div>
<pre><code>##         1 
## 0.0453014</code></pre>
<p>Making a 2D plot. We can coerce R into making a plot of the fitted logistic regression model. Here is code to do it in the present example.</p>
<div class="sourceCode" id="cb413"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb413-1"><a href="logistic-regression.html#cb413-1" aria-hidden="true" tabindex="-1"></a>fake.tadpoles <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">UV.Level =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.05</span>) )</span>
<span id="cb413-2"><a href="logistic-regression.html#cb413-2" aria-hidden="true" tabindex="-1"></a>fake.tadpoles <span class="ot">&lt;-</span> fake.tadpoles <span class="sc">%&gt;%</span></span>
<span id="cb413-3"><a href="logistic-regression.html#cb413-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">Fitted =</span> <span class="fu">predict</span>(tadpole.fit, <span class="at">newdata=</span>fake.tadpoles, <span class="at">type=</span><span class="st">&quot;response&quot;</span>))</span>
<span id="cb413-4"><a href="logistic-regression.html#cb413-4" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(tadUV) <span class="sc">+</span> </span>
<span id="cb413-5"><a href="logistic-regression.html#cb413-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x=</span>UV.Level, <span class="at">y=</span>Survived) ) <span class="sc">+</span></span>
<span id="cb413-6"><a href="logistic-regression.html#cb413-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_line</span>(<span class="at">data=</span>fake.tadpoles, <span class="fu">aes</span>(<span class="at">x=</span>UV.Level, <span class="at">y=</span>Fitted) ) <span class="sc">+</span> </span>
<span id="cb413-7"><a href="logistic-regression.html#cb413-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code></pre></div>
<p><img src="introStatModeling_files/figure-html/unnamed-chunk-29-1.png" width="672" /></p>
<p>The notions of logistic regression extend into multiple predictors, ANCOVA settings, variable selection, etc. We will see some of these applications in the below case study.</p>
</div>
<div id="case-study---titanic-dataset" class="section level2" number="12.3">
<h2><span class="header-section-number">12.3</span> Case Study - Titanic Dataset</h2>
<p>We are now going to use logistic regression for the Titanic dataset. Recall the basic format of the dataset</p>
<div class="sourceCode" id="cb414"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb414-1"><a href="logistic-regression.html#cb414-1" aria-hidden="true" tabindex="-1"></a><span class="fu">tail</span>(titanic)</span></code></pre></div>
<pre><code>##         X Class   Age Gender Survived
## 2196 2196  Crew Adult Female      Yes
## 2197 2197  Crew Adult Female      Yes
## 2198 2198  Crew Adult Female      Yes
## 2199 2199  Crew Adult Female       No
## 2200 2200  Crew Adult Female       No
## 2201 2201  Crew Adult Female       No</code></pre>
<p>By default, if we performed a logisitic regression with the <code>Survived</code> variable as the response, R would consider “No” to be a success (1) and “Yes” to be a failure (0) due to alphabetical ordering. We typically think of problems like this as a success being surviving! So we will create a new variable correspond to how we think of the problem.</p>
<div class="sourceCode" id="cb416"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb416-1"><a href="logistic-regression.html#cb416-1" aria-hidden="true" tabindex="-1"></a>titanic <span class="ot">&lt;-</span> titanic <span class="sc">%&gt;%</span></span>
<span id="cb416-2"><a href="logistic-regression.html#cb416-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">Survive =</span> <span class="fu">ifelse</span>(Survived<span class="sc">==</span><span class="st">&quot;Yes&quot;</span>, <span class="dv">1</span>, <span class="dv">0</span>) )</span></code></pre></div>
<p>Now we will perform a logistic regression on the new variable <code>Survive</code> with <code>Class</code>, <code>Age</code> and <code>Gender</code> all being predictor variables. We note that all three predictor variables are <code>factors</code>.</p>
<div class="sourceCode" id="cb417"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb417-1"><a href="logistic-regression.html#cb417-1" aria-hidden="true" tabindex="-1"></a>titanic.fit1 <span class="ot">&lt;-</span> <span class="fu">glm</span>(Survive <span class="sc">~</span> Class <span class="sc">+</span> Gender <span class="sc">+</span> Age, <span class="at">data=</span>titanic, <span class="at">family=</span><span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb417-2"><a href="logistic-regression.html#cb417-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(titanic.fit1)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Survive ~ Class + Gender + Age, family = &quot;binomial&quot;, 
##     data = titanic)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -2.081  -0.715  -0.666   0.686   2.128  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)    2.044      0.168   12.17  &lt; 2e-16 ***
## Class2        -1.018      0.196   -5.19  2.1e-07 ***
## Class3        -1.778      0.172  -10.36  &lt; 2e-16 ***
## ClassCrew     -0.858      0.157   -5.45  5.0e-08 ***
## GenderMale    -2.420      0.140  -17.24  &lt; 2e-16 ***
## AgeChild       1.062      0.244    4.35  1.4e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2769.5  on 2200  degrees of freedom
## Residual deviance: 2210.1  on 2195  degrees of freedom
## AIC: 2222
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<p>Not surprising given the contigency tables in the previous chapter, every predictor variable is significant. We need to consider the coefficients carefully.</p>
<ul>
<li><code>(Intercept)</code> - This corresponds to a first class adult female. We see the <span class="math inline">\(b_0 = 2.044\)</span> which corresponds to a probability of surviving as <span class="math inline">\(e^{2.044}/(e^{2.044}+1) = 0.88534\)</span>, so roughly a 90% probability of surviving (recall from the previous chapter that 140 out of 144 first class adult female passengers survived, the difference has to do with the model structure compared to just calculuated an observed relative frequency).</li>
<li><code>GenderMale</code> - This is the influence of being a male compared to a first class adult women. So for a first class adult male, the probability of survival is <span class="math inline">\(e^{2.044-2.420}/(e^{2.044-2.420}+1) = 0.407092\)</span> and recall that 57 of the 175 first class adult males survivied. Again, the difference between the observed relative frequency and the result of the logistic regression is the model structure (compared to just reporting a number).</li>
<li>Other variable can be interpreted in a similar fashion.</li>
</ul>
<p>In the previous section we saw that some of these predictor variable may interact. That is, there seemed to be connection between some of the predictor variables. However, we also saw that some of the interactions resulted in a large number of 0 occurences. As such, here we consider a model with Gender and Class interacting (being part of the Crew or Third class and male was not good for your chances of surviving). So now we will consider a model with some interaction.</p>
<div class="sourceCode" id="cb419"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb419-1"><a href="logistic-regression.html#cb419-1" aria-hidden="true" tabindex="-1"></a>titanic.fit2 <span class="ot">&lt;-</span> <span class="fu">glm</span>(Survive <span class="sc">~</span> Class <span class="sc">*</span> Gender <span class="sc">+</span> Age, <span class="at">data=</span>titanic, <span class="at">family=</span><span class="st">&quot;binomial&quot;</span>)</span>
<span id="cb419-2"><a href="logistic-regression.html#cb419-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(titanic.fit2)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Survive ~ Class * Gender + Age, family = &quot;binomial&quot;, 
##     data = titanic)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -2.678  -0.710  -0.580   0.529   2.023  
## 
## Coefficients:
##                      Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)             3.558      0.507    7.02  2.3e-12 ***
## Class2                 -1.681      0.588   -2.86   0.0042 ** 
## Class3                 -3.885      0.529   -7.35  2.0e-13 ***
## ClassCrew              -1.661      0.800   -2.08   0.0380 *  
## GenderMale             -4.233      0.531   -7.97  1.6e-15 ***
## AgeChild                1.054      0.230    4.57  4.8e-06 ***
## Class2:GenderMale       0.448      0.646    0.69   0.4877    
## Class3:GenderMale       2.863      0.563    5.08  3.7e-07 ***
## ClassCrew:GenderMale    1.086      0.820    1.33   0.1852    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2769.5  on 2200  degrees of freedom
## Residual deviance: 2143.4  on 2192  degrees of freedom
## AIC: 2161
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<p>Before proceding to interpret this model, let’s first ask if it imrpoves over the previous model. You’ll note that <code>titanic.fit1</code> is a simpler version of <code>titanic.fit2</code>. We use the <code>anova()</code> function to test if adding the interaction terms significantly improves the model.</p>
<div class="sourceCode" id="cb421"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb421-1"><a href="logistic-regression.html#cb421-1" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(titanic.fit1, titanic.fit2, <span class="at">test=</span><span class="st">&quot;Chisq&quot;</span>)</span></code></pre></div>
<pre><code>## Analysis of Deviance Table
## 
## Model 1: Survive ~ Class + Gender + Age
## Model 2: Survive ~ Class * Gender + Age
##   Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi)    
## 1      2195       2210                         
## 2      2192       2143  3    66.67 2.21e-14 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We see overwhelming support that the interactive model improves over the main effects model (<span class="math inline">\(p\)</span>-value near zero). Interpreting this model is complicated:</p>
<ul>
<li><p><code>(Intercept)</code> - This corresponds to a first class adult female, and the probability of surviging is <span class="math inline">\(e^{3.558}/(e^{3.558}+1) = 0.972294\)</span> which is much closer to the observed relative frequency of <span class="math inline">\(140/144 = 0.9722222\)</span>.</p></li>
<li><p><code>AgeChild</code> - This is the impact of being a child compared to a first class adult female. So <span class="math inline">\(e^{3.558+1.054}/(e^{3.558+1.054}+1)=0.991066\)</span> is the probability of a first class female child surviving (there was only 1 on the titanic, and they survived).</p></li>
<li><p><code>Class3:GenderMale</code> - Interpreting the interaction terms is wonky and here is why. First consider the full model</p>
<p><span class="math display">\[\textrm{logit}(p) = \beta_0 + \beta_1(Class2) + \beta_3(Class3) + \beta_4(Crew) + \beta_5(Male) + \beta_6(Child) + \beta_7(Class2Male) + \beta_8(Class3Male) + \beta_9(CrewMale)\]</span></p>
<p>For the <code>Class3:Male</code> term to be in effect both <code>Class2</code> and <code>GenderMale</code> will be activated (remember everything is a 1 or 0 dummary variable). So interpretation of this interaction term actually involves many terms. The probability of a second class adult male surviving is
<span class="math display">\[\frac{e^{3.558-3.885-4.233+2.863}}{1+e^{3.558-3.885-4.233+2.863}} = 0.154857\]</span> which is relativaly close to the <span class="math inline">\(75/(75+387) = 0.162338\)</span> that did survive.</p></li>
</ul>
<p>Overall we see this more complex model appears to more closely match the observed relative frequencies from the previous chapter. This is not too surprising given the measure of <code>Residual deviance</code>. Before explaining, let’s look at both fitted models one more time</p>
<div class="sourceCode" id="cb423"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb423-1"><a href="logistic-regression.html#cb423-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(titanic.fit1)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Survive ~ Class + Gender + Age, family = &quot;binomial&quot;, 
##     data = titanic)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -2.081  -0.715  -0.666   0.686   2.128  
## 
## Coefficients:
##             Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)    2.044      0.168   12.17  &lt; 2e-16 ***
## Class2        -1.018      0.196   -5.19  2.1e-07 ***
## Class3        -1.778      0.172  -10.36  &lt; 2e-16 ***
## ClassCrew     -0.858      0.157   -5.45  5.0e-08 ***
## GenderMale    -2.420      0.140  -17.24  &lt; 2e-16 ***
## AgeChild       1.062      0.244    4.35  1.4e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2769.5  on 2200  degrees of freedom
## Residual deviance: 2210.1  on 2195  degrees of freedom
## AIC: 2222
## 
## Number of Fisher Scoring iterations: 4</code></pre>
<div class="sourceCode" id="cb425"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb425-1"><a href="logistic-regression.html#cb425-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(titanic.fit2)</span></code></pre></div>
<pre><code>## 
## Call:
## glm(formula = Survive ~ Class * Gender + Age, family = &quot;binomial&quot;, 
##     data = titanic)
## 
## Deviance Residuals: 
##    Min      1Q  Median      3Q     Max  
## -2.678  -0.710  -0.580   0.529   2.023  
## 
## Coefficients:
##                      Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept)             3.558      0.507    7.02  2.3e-12 ***
## Class2                 -1.681      0.588   -2.86   0.0042 ** 
## Class3                 -3.885      0.529   -7.35  2.0e-13 ***
## ClassCrew              -1.661      0.800   -2.08   0.0380 *  
## GenderMale             -4.233      0.531   -7.97  1.6e-15 ***
## AgeChild                1.054      0.230    4.57  4.8e-06 ***
## Class2:GenderMale       0.448      0.646    0.69   0.4877    
## Class3:GenderMale       2.863      0.563    5.08  3.7e-07 ***
## ClassCrew:GenderMale    1.086      0.820    1.33   0.1852    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 2769.5  on 2200  degrees of freedom
## Residual deviance: 2143.4  on 2192  degrees of freedom
## AIC: 2161
## 
## Number of Fisher Scoring iterations: 6</code></pre>
<p>Both model report a <code>Null deviance: 2769.5</code> which can be loosely interpreted as the “error” in the most basic model, <code>Survive ~ 1</code> (an intercept only model). The <code>Residual deviance</code> corresponds to the error of the fitted models. You’ll note that <code>titanic.fit1</code> has a deviance of 2210.1 while <code>titanic.fit2</code> has a smaller deviance of 2143.4, this we can argue is a better fit. The <code>anova()</code> function output tells us it is a significantly better fit.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="statistical-odds.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="generalized-linear-models.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["introStatModeling.pdf", "introStatModeling.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
