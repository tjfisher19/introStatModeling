<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 More on multiple linear regression | Introduction to Statistical Modeling</title>
  <meta name="description" content="Covers Regression and elements of Design of Experiments in R using the tidyverse." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 More on multiple linear regression | Introduction to Statistical Modeling" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Covers Regression and elements of Design of Experiments in R using the tidyverse." />
  <meta name="github-repo" content="tjfisher19/introStatModeling" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 More on multiple linear regression | Introduction to Statistical Modeling" />
  
  <meta name="twitter:description" content="Covers Regression and elements of Design of Experiments in R using the tidyverse." />
  

<meta name="author" content="Michael Hughes and Thomas Fisher" />


<meta name="date" content="2021-12-25" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="inference-regarding-multiple-regression.html"/>
<link rel="next" href="model-building-considerations.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Inroduction to Statistical Modeling</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="about-the-authors.html"><a href="about-the-authors.html"><i class="fa fa-check"></i>About the Authors</a></li>
<li class="chapter" data-level="" data-path="important-preliminary-review.html"><a href="important-preliminary-review.html"><i class="fa fa-check"></i>Important Preliminary Review</a>
<ul>
<li class="chapter" data-level="" data-path="important-preliminary-review.html"><a href="important-preliminary-review.html#statistics-background"><i class="fa fa-check"></i>Statistics background</a></li>
<li class="chapter" data-level="" data-path="important-preliminary-review.html"><a href="important-preliminary-review.html#software"><i class="fa fa-check"></i>Software</a></li>
<li class="chapter" data-level="" data-path="important-preliminary-review.html"><a href="important-preliminary-review.html#add-on-packages"><i class="fa fa-check"></i>Add-on packages</a></li>
<li class="chapter" data-level="" data-path="important-preliminary-review.html"><a href="important-preliminary-review.html#help-with-rmarkdown"><i class="fa fa-check"></i>Help with RMarkdown</a></li>
<li class="chapter" data-level="" data-path="important-preliminary-review.html"><a href="important-preliminary-review.html#managing-your-work-in-r"><i class="fa fa-check"></i>Managing your work in R</a></li>
<li class="chapter" data-level="" data-path="important-preliminary-review.html"><a href="important-preliminary-review.html#data-in-this-text"><i class="fa fa-check"></i>Data in this text</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html"><i class="fa fa-check"></i><b>1</b> Introductory Statistics in R</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#goals-of-a-statistical-analysis"><i class="fa fa-check"></i><b>1.1</b> Goals of a statistical analysis</a></li>
<li class="chapter" data-level="1.2" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#before-you-begin-an-analysis"><i class="fa fa-check"></i><b>1.2</b> Before you begin an analysis</a></li>
<li class="chapter" data-level="1.3" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#data-frames"><i class="fa fa-check"></i><b>1.3</b> Data frames</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#built-in-data"><i class="fa fa-check"></i><b>1.3.1</b> Built-in data</a></li>
<li class="chapter" data-level="1.3.2" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#types-of-data"><i class="fa fa-check"></i><b>1.3.2</b> Types of Data</a></li>
<li class="chapter" data-level="1.3.3" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#importing-datasets-into-r"><i class="fa fa-check"></i><b>1.3.3</b> Importing datasets into R</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#referencing-data-from-inside-a-data-frame"><i class="fa fa-check"></i><b>1.4</b> Referencing data from inside a data frame</a></li>
<li class="chapter" data-level="1.5" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#missing-values-and-computer-arithmetic-in-r"><i class="fa fa-check"></i><b>1.5</b> Missing values and computer arithmetic in R</a></li>
<li class="chapter" data-level="1.6" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#exploratory-data-analysis-eda"><i class="fa fa-check"></i><b>1.6</b> Exploratory Data Analysis (EDA)</a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#numeric-summaries"><i class="fa fa-check"></i><b>1.6.1</b> Numeric Summaries</a></li>
<li class="chapter" data-level="1.6.2" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#numeric-summaries-in-r"><i class="fa fa-check"></i><b>1.6.2</b> Numeric Summaries in R</a></li>
<li class="chapter" data-level="1.6.3" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#graphical-summaries"><i class="fa fa-check"></i><b>1.6.3</b> Graphical Summaries</a></li>
<li class="chapter" data-level="1.6.4" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#distribution-of-univariate-variables"><i class="fa fa-check"></i><b>1.6.4</b> Distribution of Univariate Variables</a></li>
<li class="chapter" data-level="1.6.5" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#descriptive-statistics-and-visualizations-by-levels-of-a-factor-variable"><i class="fa fa-check"></i><b>1.6.5</b> Descriptive statistics and visualizations by levels of a factor variable</a></li>
<li class="chapter" data-level="1.6.6" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#descriptive-statistics-and-visualizations-for-two-numeric-variables"><i class="fa fa-check"></i><b>1.6.6</b> Descriptive statistics and visualizations for two numeric variables</a></li>
</ul></li>
<li class="chapter" data-level="1.7" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#sampling-distributions-describing-how-a-statistic-varies"><i class="fa fa-check"></i><b>1.7</b> Sampling distributions: describing how a statistic varies</a></li>
<li class="chapter" data-level="1.8" data-path="introductory-statistics-in-r.html"><a href="introductory-statistics-in-r.html#two-sample-inference"><i class="fa fa-check"></i><b>1.8</b> Two-sample inference</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html"><i class="fa fa-check"></i><b>2</b> Introduction to Statistical Modeling and Designed Experiments</a>
<ul>
<li class="chapter" data-level="2.1" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#statistical-analyses-is-modeling"><i class="fa fa-check"></i><b>2.1</b> Statistical Analyses is Modeling</a></li>
<li class="chapter" data-level="2.2" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#observational-studies-versus-designed-experiments"><i class="fa fa-check"></i><b>2.2</b> Observational Studies versus designed experiments</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#observational-studies"><i class="fa fa-check"></i><b>2.2.1</b> Observational Studies</a></li>
<li class="chapter" data-level="2.2.2" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#designed-experiments"><i class="fa fa-check"></i><b>2.2.2</b> Designed experiments</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#designed-experiement-vocabulary"><i class="fa fa-check"></i><b>2.3</b> Designed experiement vocabulary</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#what-is-an-experiment"><i class="fa fa-check"></i><b>2.3.1</b> What is an experiment?</a></li>
<li class="chapter" data-level="2.3.2" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#analysis-of-variance"><i class="fa fa-check"></i><b>2.3.2</b> Analysis of variance</a></li>
<li class="chapter" data-level="2.3.3" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#elements-of-a-designed-experiment"><i class="fa fa-check"></i><b>2.3.3</b> Elements of a designed experiment</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#paired-t-test"><i class="fa fa-check"></i><b>2.4</b> Paired <span class="math inline">\(t\)</span>-test</a></li>
<li class="chapter" data-level="2.5" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#one-way-anova"><i class="fa fa-check"></i><b>2.5</b> One-Way ANOVA</a></li>
<li class="chapter" data-level="2.6" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#assumption-checking"><i class="fa fa-check"></i><b>2.6</b> Assumption Checking</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#independence"><i class="fa fa-check"></i><b>2.6.1</b> Independence</a></li>
<li class="chapter" data-level="2.6.2" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#constant-variance"><i class="fa fa-check"></i><b>2.6.2</b> Constant Variance</a></li>
<li class="chapter" data-level="2.6.3" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#checking-normality"><i class="fa fa-check"></i><b>2.6.3</b> Checking Normality</a></li>
<li class="chapter" data-level="2.6.4" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#code-to-check-assumption"><i class="fa fa-check"></i><b>2.6.4</b> Code to check assumption</a></li>
<li class="chapter" data-level="2.6.5" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#transforming-your-response"><i class="fa fa-check"></i><b>2.6.5</b> Transforming your response</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#follow-up-procedures-multiple-comparisons"><i class="fa fa-check"></i><b>2.7</b> Follow-up procedures – Multiple Comparisons</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#tukeys-hsd-method"><i class="fa fa-check"></i><b>2.7.1</b> Tukey’s HSD method</a></li>
<li class="chapter" data-level="2.7.2" data-path="introduction-to-statistical-modeling-and-designed-experiments.html"><a href="introduction-to-statistical-modeling-and-designed-experiments.html#dunnett-multiple-comparisons"><i class="fa fa-check"></i><b>2.7.2</b> Dunnett multiple comparisons</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="multiple-factor-designed-experiments.html"><a href="multiple-factor-designed-experiments.html"><i class="fa fa-check"></i><b>3</b> Multiple Factor Designed Experiments</a>
<ul>
<li class="chapter" data-level="3.1" data-path="multiple-factor-designed-experiments.html"><a href="multiple-factor-designed-experiments.html#blocking"><i class="fa fa-check"></i><b>3.1</b> Blocking</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="multiple-factor-designed-experiments.html"><a href="multiple-factor-designed-experiments.html#data-structure-model-form-and-analysis-of-variance-of-a-randomized-block-design"><i class="fa fa-check"></i><b>3.1.1</b> Data structure, model form and analysis of variance of a Randomized Block Design</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="multiple-factor-designed-experiments.html"><a href="multiple-factor-designed-experiments.html#two-factor-designs"><i class="fa fa-check"></i><b>3.2</b> Two-factor Designs</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="multiple-factor-designed-experiments.html"><a href="multiple-factor-designed-experiments.html#analysis"><i class="fa fa-check"></i><b>3.2.1</b> Analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="advanced-designs.html"><a href="advanced-designs.html"><i class="fa fa-check"></i><b>4</b> Advanced Designs</a>
<ul>
<li class="chapter" data-level="4.1" data-path="advanced-designs.html"><a href="advanced-designs.html#higher-order-factor-models"><i class="fa fa-check"></i><b>4.1</b> Higher order factor models</a></li>
<li class="chapter" data-level="4.2" data-path="advanced-designs.html"><a href="advanced-designs.html#within-subject-designs"><i class="fa fa-check"></i><b>4.2</b> Within-subject designs</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="advanced-designs.html"><a href="advanced-designs.html#blocks-revisited-an-approach-to-handling-within-subjects-factors"><i class="fa fa-check"></i><b>4.2.1</b> Blocks revisited: an approach to handling within-subjects factors</a></li>
<li class="chapter" data-level="4.2.2" data-path="advanced-designs.html"><a href="advanced-designs.html#a-more-involved-repeated-measures-case-study"><i class="fa fa-check"></i><b>4.2.2</b> A more involved repeated measures case study</a></li>
<li class="chapter" data-level="4.2.3" data-path="advanced-designs.html"><a href="advanced-designs.html#further-study"><i class="fa fa-check"></i><b>4.2.3</b> Further study</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html"><i class="fa fa-check"></i><b>5</b> Introduction to Multiple Regression</a>
<ul>
<li class="chapter" data-level="5.1" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#regression-model"><i class="fa fa-check"></i><b>5.1</b> Regression Model</a></li>
<li class="chapter" data-level="5.2" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#fitting-a-regression-model"><i class="fa fa-check"></i><b>5.2</b> Fitting a regression model</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#why-should-we-use-more-than-one-predictor"><i class="fa fa-check"></i><b>5.2.1</b> Why should we use more than one predictor?</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#interpreting-beta-parameter-estimates-in-mlr"><i class="fa fa-check"></i><b>5.3</b> Interpreting <span class="math inline">\(\beta\)</span>-parameter estimates in MLR</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#designed-experiments-1"><i class="fa fa-check"></i><b>5.3.1</b> Designed experiments</a></li>
<li class="chapter" data-level="5.3.2" data-path="introduction-to-multiple-regression.html"><a href="introduction-to-multiple-regression.html#observational-studies-1"><i class="fa fa-check"></i><b>5.3.2</b> Observational studies</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html"><i class="fa fa-check"></i><b>6</b> Inference regarding Multiple Regression</a>
<ul>
<li class="chapter" data-level="6.1" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#assumption-checking-1"><i class="fa fa-check"></i><b>6.1</b> Assumption checking</a></li>
<li class="chapter" data-level="6.2" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#overall-f-test-for-model-signifance"><i class="fa fa-check"></i><b>6.2</b> Overall <span class="math inline">\(F\)</span>-test for model signifance</a></li>
<li class="chapter" data-level="6.3" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#individual-parameter-inference"><i class="fa fa-check"></i><b>6.3</b> Individual parameter inference</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#t-tests"><i class="fa fa-check"></i><b>6.3.1</b> <span class="math inline">\(t\)</span>-tests</a></li>
<li class="chapter" data-level="6.3.2" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#confidence-intervals"><i class="fa fa-check"></i><b>6.3.2</b> Confidence Intervals</a></li>
<li class="chapter" data-level="6.3.3" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#confidence-and-prediction-bands"><i class="fa fa-check"></i><b>6.3.3</b> Confidence and prediction bands</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#goodness-of-fit"><i class="fa fa-check"></i><b>6.4</b> Goodness-of-fit</a>
<ul>
<li class="chapter" data-level="6.4.1" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#coefficient-of-determination"><i class="fa fa-check"></i><b>6.4.1</b> Coefficient of determination</a></li>
<li class="chapter" data-level="6.4.2" data-path="inference-regarding-multiple-regression.html"><a href="inference-regarding-multiple-regression.html#akaikes-information-criterion"><i class="fa fa-check"></i><b>6.4.2</b> Akaike’s Information Criterion</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="more-on-multiple-linear-regression.html"><a href="more-on-multiple-linear-regression.html"><i class="fa fa-check"></i><b>7</b> More on multiple linear regression</a>
<ul>
<li class="chapter" data-level="7.1" data-path="more-on-multiple-linear-regression.html"><a href="more-on-multiple-linear-regression.html#model-comparision-reduced-f-tests"><i class="fa fa-check"></i><b>7.1</b> Model comparision – Reduced <span class="math inline">\(F\)</span>-tests</a></li>
<li class="chapter" data-level="7.2" data-path="more-on-multiple-linear-regression.html"><a href="more-on-multiple-linear-regression.html#categorical-predictor-variables"><i class="fa fa-check"></i><b>7.2</b> Categorical Predictor Variables</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="more-on-multiple-linear-regression.html"><a href="more-on-multiple-linear-regression.html#a-qualitative-predictor-with-two-levels"><i class="fa fa-check"></i><b>7.2.1</b> A qualitative predictor with two levels</a></li>
<li class="chapter" data-level="7.2.2" data-path="more-on-multiple-linear-regression.html"><a href="more-on-multiple-linear-regression.html#a-qualitative-predictor-with-more-than-two-levels"><i class="fa fa-check"></i><b>7.2.2</b> A qualitative predictor with more than two levels</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="more-on-multiple-linear-regression.html"><a href="more-on-multiple-linear-regression.html#bridging-regression-and-designed-experiments-ancova"><i class="fa fa-check"></i><b>7.3</b> Bridging Regression and Designed Experiments – ANCOVA</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="more-on-multiple-linear-regression.html"><a href="more-on-multiple-linear-regression.html#an-ancova-example-with-a-two-level-factor"><i class="fa fa-check"></i><b>7.3.1</b> An ANCOVA example with a two-level factor</a></li>
<li class="chapter" data-level="7.3.2" data-path="more-on-multiple-linear-regression.html"><a href="more-on-multiple-linear-regression.html#ancova-with-a-multi-level-factor"><i class="fa fa-check"></i><b>7.3.2</b> ANCOVA with a multi-level factor</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="model-building-considerations.html"><a href="model-building-considerations.html"><i class="fa fa-check"></i><b>8</b> Model Building Considerations</a>
<ul>
<li class="chapter" data-level="8.1" data-path="model-building-considerations.html"><a href="model-building-considerations.html#regression-assumptions-revisited"><i class="fa fa-check"></i><b>8.1</b> Regression assumptions revisited</a></li>
<li class="chapter" data-level="8.2" data-path="model-building-considerations.html"><a href="model-building-considerations.html#violations-of-the-independence-assumption"><i class="fa fa-check"></i><b>8.2</b> Violations of the independence assumption</a>
<ul>
<li class="chapter" data-level="8.2.1" data-path="model-building-considerations.html"><a href="model-building-considerations.html#collecting-data-that-are-temporal-or-spatial-in-nature"><i class="fa fa-check"></i><b>8.2.1</b> Collecting data that are temporal or spatial in nature</a></li>
<li class="chapter" data-level="8.2.2" data-path="model-building-considerations.html"><a href="model-building-considerations.html#pseudoreplication"><i class="fa fa-check"></i><b>8.2.2</b> Pseudoreplication</a></li>
<li class="chapter" data-level="8.2.3" data-path="model-building-considerations.html"><a href="model-building-considerations.html#what-if-we-have-non-independent-errors"><i class="fa fa-check"></i><b>8.2.3</b> What if we have non-independent errors?</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="model-building-considerations.html"><a href="model-building-considerations.html#constant-variance-violations"><i class="fa fa-check"></i><b>8.3</b> Constant Variance Violations</a>
<ul>
<li class="chapter" data-level="8.3.1" data-path="model-building-considerations.html"><a href="model-building-considerations.html#box-cox-power-tranformations"><i class="fa fa-check"></i><b>8.3.1</b> Box-Cox Power Tranformations</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="model-building-considerations.html"><a href="model-building-considerations.html#normality-violations"><i class="fa fa-check"></i><b>8.4</b> Normality violations</a></li>
<li class="chapter" data-level="8.5" data-path="model-building-considerations.html"><a href="model-building-considerations.html#violations-of-the-linearity-assumption"><i class="fa fa-check"></i><b>8.5</b> Violations of the linearity assumption</a></li>
<li class="chapter" data-level="8.6" data-path="model-building-considerations.html"><a href="model-building-considerations.html#detecting-and-dealing-with-unusual-observations"><i class="fa fa-check"></i><b>8.6</b> Detecting and dealing with unusual observations</a></li>
<li class="chapter" data-level="8.7" data-path="model-building-considerations.html"><a href="model-building-considerations.html#multicollinearity"><i class="fa fa-check"></i><b>8.7</b> Multicollinearity</a></li>
<li class="chapter" data-level="8.8" data-path="model-building-considerations.html"><a href="model-building-considerations.html#standardizingPredictors"><i class="fa fa-check"></i><b>8.8</b> Scale changes</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="model-selection.html"><a href="model-selection.html"><i class="fa fa-check"></i><b>9</b> Model Selection</a>
<ul>
<li class="chapter" data-level="9.1" data-path="model-selection.html"><a href="model-selection.html#stepwise-procedures"><i class="fa fa-check"></i><b>9.1</b> Stepwise Procedures</a>
<ul>
<li class="chapter" data-level="9.1.1" data-path="model-selection.html"><a href="model-selection.html#backward-selection"><i class="fa fa-check"></i><b>9.1.1</b> Backward Selection</a></li>
<li class="chapter" data-level="9.1.2" data-path="model-selection.html"><a href="model-selection.html#forward-selection"><i class="fa fa-check"></i><b>9.1.2</b> Forward selection</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="model-selection.html"><a href="model-selection.html#best-subsets"><i class="fa fa-check"></i><b>9.2</b> Best subsets</a></li>
<li class="chapter" data-level="9.3" data-path="model-selection.html"><a href="model-selection.html#shrinkage-methods"><i class="fa fa-check"></i><b>9.3</b> Shrinkage Methods</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="model-validation.html"><a href="model-validation.html"><i class="fa fa-check"></i><b>10</b> Model Validation</a>
<ul>
<li class="chapter" data-level="10.1" data-path="model-validation.html"><a href="model-validation.html#underfitting-vs.-overfitting-models"><i class="fa fa-check"></i><b>10.1</b> Underfitting vs. Overfitting Models</a>
<ul>
<li class="chapter" data-level="10.1.1" data-path="model-validation.html"><a href="model-validation.html#the-bias-variance-trade-off"><i class="fa fa-check"></i><b>10.1.1</b> The Bias-Variance Trade-off</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="model-validation.html"><a href="model-validation.html#validation-techniques"><i class="fa fa-check"></i><b>10.2</b> Validation Techniques</a></li>
<li class="chapter" data-level="10.3" data-path="model-validation.html"><a href="model-validation.html#basic-validation-with-a-single-holdout-sample"><i class="fa fa-check"></i><b>10.3</b> Basic Validation with a single holdout sample</a>
<ul>
<li class="chapter" data-level="10.3.1" data-path="model-validation.html"><a href="model-validation.html#use-the-training-data-to-fit-and-select-models"><i class="fa fa-check"></i><b>10.3.1</b> Use the training data to fit and select models</a></li>
<li class="chapter" data-level="10.3.2" data-path="model-validation.html"><a href="model-validation.html#model-training"><i class="fa fa-check"></i><b>10.3.2</b> Model training:</a></li>
<li class="chapter" data-level="10.3.3" data-path="model-validation.html"><a href="model-validation.html#model-validation-step"><i class="fa fa-check"></i><b>10.3.3</b> Model validation step:</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="model-validation.html"><a href="model-validation.html#hold-out-sample-validation-using-caret"><i class="fa fa-check"></i><b>10.4</b> Hold-out sample validation using <code>caret</code></a></li>
<li class="chapter" data-level="10.5" data-path="model-validation.html"><a href="model-validation.html#leave-one-out-cross-validation-loocv"><i class="fa fa-check"></i><b>10.5</b> “Leave one out” Cross-Validation (LOOCV)</a></li>
<li class="chapter" data-level="10.6" data-path="model-validation.html"><a href="model-validation.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>10.6</b> <span class="math inline">\(k\)</span>-fold Cross-Validation</a></li>
<li class="chapter" data-level="10.7" data-path="model-validation.html"><a href="model-validation.html#a-final-note"><i class="fa fa-check"></i><b>10.7</b> A final note</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="statistical-odds.html"><a href="statistical-odds.html"><i class="fa fa-check"></i><b>11</b> Statistical Odds</a>
<ul>
<li class="chapter" data-level="11.1" data-path="statistical-odds.html"><a href="statistical-odds.html#probability-versus-odds"><i class="fa fa-check"></i><b>11.1</b> Probability versus Odds</a></li>
<li class="chapter" data-level="11.2" data-path="statistical-odds.html"><a href="statistical-odds.html#odds-ratios"><i class="fa fa-check"></i><b>11.2</b> Odds ratios</a></li>
<li class="chapter" data-level="11.3" data-path="statistical-odds.html"><a href="statistical-odds.html#ideas-of-modeling-odds"><i class="fa fa-check"></i><b>11.3</b> Ideas of modeling odds</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>12</b> Logistic Regression</a>
<ul>
<li class="chapter" data-level="12.1" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-model"><i class="fa fa-check"></i><b>12.1</b> Logistic Model</a></li>
<li class="chapter" data-level="12.2" data-path="logistic-regression.html"><a href="logistic-regression.html#fitting-interpreting-and-assessing-a-logistic-model"><i class="fa fa-check"></i><b>12.2</b> Fitting, Interpreting and assessing a logistic model</a></li>
<li class="chapter" data-level="12.3" data-path="logistic-regression.html"><a href="logistic-regression.html#case-study---titanic-dataset"><i class="fa fa-check"></i><b>12.3</b> Case Study - Titanic Dataset</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html"><i class="fa fa-check"></i><b>13</b> Generalized Linear Models</a>
<ul>
<li class="chapter" data-level="13.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#poisson-regression"><i class="fa fa-check"></i><b>13.1</b> Poisson Regression</a>
<ul>
<li class="chapter" data-level="13.1.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#poisson-distribution"><i class="fa fa-check"></i><b>13.1.1</b> Poisson distribution</a></li>
<li class="chapter" data-level="13.1.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#poisson-regression-development"><i class="fa fa-check"></i><b>13.1.2</b> Poisson Regression Development</a></li>
<li class="chapter" data-level="13.1.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#example---tropical-cyclone-counts-in-the-north-atlantic"><i class="fa fa-check"></i><b>13.1.3</b> Example - Tropical Cyclone Counts in the North Atlantic</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#handling-overdispersion"><i class="fa fa-check"></i><b>13.2</b> Handling overdispersion</a>
<ul>
<li class="chapter" data-level="13.2.1" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#example-attendnace-records"><i class="fa fa-check"></i><b>13.2.1</b> Example – Attendnace Records</a></li>
<li class="chapter" data-level="13.2.2" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#incorrect-poisson-model"><i class="fa fa-check"></i><b>13.2.2</b> Incorrect Poisson Model</a></li>
<li class="chapter" data-level="13.2.3" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#a-quasi-poisson-approach"><i class="fa fa-check"></i><b>13.2.3</b> A quasi-Poisson approach</a></li>
<li class="chapter" data-level="13.2.4" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#fitting-a-negative-binomial-regression"><i class="fa fa-check"></i><b>13.2.4</b> Fitting a Negative Binomial regression</a></li>
<li class="chapter" data-level="13.2.5" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#picking-between-quasi-poisson-and-negative-binomial"><i class="fa fa-check"></i><b>13.2.5</b> Picking between Quasi-Poisson and Negative Binomial</a></li>
<li class="chapter" data-level="13.2.6" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#infererence-on-predictor-variables"><i class="fa fa-check"></i><b>13.2.6</b> Infererence on predictor variables</a></li>
<li class="chapter" data-level="13.2.7" data-path="generalized-linear-models.html"><a href="generalized-linear-models.html#plotting-fitted-model"><i class="fa fa-check"></i><b>13.2.7</b> Plotting fitted model</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Introduction to Statistical Modeling</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="more-on-multiple-linear-regression" class="section level1" number="7">
<h1><span class="header-section-number">Chapter 7</span> More on multiple linear regression</h1>
<div id="model-comparision-reduced-f-tests" class="section level2" number="7.1">
<h2><span class="header-section-number">7.1</span> Model comparision – Reduced <span class="math inline">\(F\)</span>-tests</h2>
<p>Occasionally, we might be interested in more specialized tests of a multiple regression model’s <span class="math inline">\(\beta\)</span> parameters. For example, consider the property appraisals data (again). The regression model we are using is</p>
<p><span class="math display">\[\textrm{Sale price} = \beta_0 + \beta_1(\textrm{Land value}) + \beta_2(\textrm{Improvements value}) + \beta_3(\textrm{Area}) + \varepsilon\]</span>
Think about questions like these:</p>
<ul>
<li>Do the two appraisals collectively have a significant effect on mean sale price?</li>
<li>Is the effect of land value appraisal different from the effect of improvements value appraisal on the mean sale price?</li>
</ul>
<p>To address questions like these, we first need to translate them into equivalent expressions in terms of the model’s <span class="math inline">\(\beta\)</span> parameters. Doing so will give rise to hypotheses we can test using the principle of ANOVA that was introduced earlier.</p>
<p><strong>Logic.</strong> To formulate an approach to conducting the test, let’s work on the first question above: “Do the two appraisals collectively have a significant effect on mean sale price?” To get started, you need to think about what the null and alternative hypotheses would be to test such a statement. They would look like this:</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: The two appraisals collectively have no effect on mean sale price</li>
<li><span class="math inline">\(H_a\)</span>: At least one of the two appraisals has an effect on mean sale price</li>
</ul>
<p>Now, translate what this means in terms of the model’s <span class="math inline">\(\beta\)</span> parameters:</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: <span class="math inline">\(\beta_1 = \beta_2 = 0\)</span></li>
<li><span class="math inline">\(H_a\)</span>: at least one of <span class="math inline">\(\beta_1\)</span> or <span class="math inline">\(\beta_2\)</span> is not 0</li>
</ul>
<p>How we proceeded with tests like those in the previous chapters where we essentially compared two models: one involving a “full” model and the other involving a “reduced” null model, and checking if there is a significant difference between them. The transition from the full model to the reduced model is to impose the null hypothesis on the full model. Thinking this way, the hypotheses above may be rewritten as:</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: the model is <span class="math inline">\(\textrm{Sale price} = \beta_0 + \beta_3(\textrm{Area}) + \varepsilon\)</span></li>
<li><span class="math inline">\(H_a\)</span>: the model is <span class="math inline">\(\textrm{Sale price} = \beta_0 + \beta_1(\textrm{Land value}) + \beta_2(\textrm{Improvement value}) + \beta_3(\textrm{Area}) + \varepsilon\)</span></li>
</ul>
<p>The R function <code>anova()</code> will do the computation of the ANOVA <span class="math inline">\(F\)</span>-statistic and its associated <span class="math inline">\(p\)</span>-value for this test, once you feed it the fitted full and reduced R model objects. The form of the R function is <code>anova(reducedmodel, fullmodel)</code></p>
<p>So, the test of <span class="math inline">\(H_0\)</span>: The two appraisals collectively have no effect on mean sale price (i.e. <span class="math inline">\(\beta_1 = \beta_2 = 0\)</span>) is conducted as follows (recall <code>appraisal.fit</code> is the full model):</p>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb228-1"><a href="more-on-multiple-linear-regression.html#cb228-1" aria-hidden="true" tabindex="-1"></a>reduced.appraisal.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(saleprice <span class="sc">~</span> area, <span class="at">data=</span>appraisal)</span>
<span id="cb228-2"><a href="more-on-multiple-linear-regression.html#cb228-2" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(reduced.appraisal.fit, appraisal.fit)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: saleprice ~ area
## Model 2: saleprice ~ landvalue + impvalue + area
##   Res.Df       RSS Df Sum of Sq    F   Pr(&gt;F)    
## 1     18 2.732e+09                               
## 2     16 1.002e+09  2 1.729e+09 13.8 0.000329 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The <span class="math inline">\(F\)</span>-statistic value is 13.802, with 2 and 16 degrees of freedom. The <span class="math inline">\(p\)</span>-value is 0.0003, so we reject <span class="math inline">\(H_0\)</span>. There is sufficient evidence to conclude that the appraisals collectively have a significant effect on mean sale price.</p>
<p>Before doing one more example, here is a summary of the steps involved:</p>
<ol style="list-style-type: decimal">
<li>Fit a full model, and save it as an R model object.</li>
<li>Impose your null hypothesis <span class="math inline">\(H_0\)</span> onto the full model to create the reduced model.</li>
<li>Fit the reduced model, and save it as an R model object.</li>
<li>Run the ANOVA F-test by issuing <code>anova(reducedmodel, fullmodel)</code>.</li>
</ol>
<p>We now perform another example to demonstrate further functionality. <em>Is the effect of land value appraisal different from the effect of improvements value appraisal on the mean sale price</em>?</p>
<p>The null and alternative hypotheses for testing this can be expressed as follows:</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: Land value appraisal and improvements value appraisal have the same effect</li>
<li><span class="math inline">\(H_a\)</span>: Land value appraisal and improvements value appraisal have different effects</li>
</ul>
<p>In terms of the model’s <span class="math inline">\(\beta\)</span> parameters, this null hypothesis is just stating that <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> are equal (but not necessarily 0). So, the hypotheses may be rewritten as</p>
<p><span class="math display">\[H_0: \beta_1 = \beta_2 ~~~~\textrm{versus}~~~~ H_a: \beta_1 \neq \beta_2\]</span></p>
<p>Let’s denote the common value of <span class="math inline">\(\beta_1\)</span> and <span class="math inline">\(\beta_2\)</span> under the null hypothesis using the symbol <span class="math inline">\(\gamma\)</span>. Then it is crucial to see that we can write the hypotheses in terms of full and reduced “null” models as follows:</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: the model is <span class="math inline">\(\textrm{Sale price} = \beta_0 + \gamma(\textrm{Land}) + \gamma(\textrm{Improv}) + \beta_3(\textrm{Area}) + \varepsilon\)</span></li>
<li><span class="math inline">\(H_a\)</span>: the model is <span class="math inline">\(\textrm{Sale price} = \beta_0 + \beta_1(\textrm{Land}) + \beta_2(\textrm{Improv}) + \beta_3(\textrm{Area}) + \varepsilon\)</span></li>
</ul>
<p>or equivalently,</p>
<ul>
<li><span class="math inline">\(H_0\)</span>: the model is <span class="math inline">\(\textrm{Sale price} = \beta_0 + \gamma(\textrm{Land} + \textrm{Improv}) + \beta_3(\textrm{Area}) + \varepsilon\)</span></li>
<li><span class="math inline">\(H_a\)</span>: the model is <span class="math inline">\(\textrm{Sale price} = \beta_0 + \beta_1(\textrm{Land}) + \beta_2(\textrm{Improv}) + \beta_3(\textrm{Area}) + \varepsilon\)</span></li>
</ul>
<p>Note in this last notation the null hypothesis essentially is performing a multiple regression on two predictor variables: Area and a new variable Land + Improv. We can perform this test in one of two ways.</p>
<p><strong>Method 1</strong> - Creating a new variable</p>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb230-1"><a href="more-on-multiple-linear-regression.html#cb230-1" aria-hidden="true" tabindex="-1"></a>appraisal <span class="ot">&lt;-</span> appraisal <span class="sc">%&gt;%</span></span>
<span id="cb230-2"><a href="more-on-multiple-linear-regression.html#cb230-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">LandImprov =</span> landvalue <span class="sc">+</span> impvalue)</span>
<span id="cb230-3"><a href="more-on-multiple-linear-regression.html#cb230-3" aria-hidden="true" tabindex="-1"></a>red.model1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(saleprice <span class="sc">~</span> LandImprov <span class="sc">+</span> area, <span class="at">data=</span>appraisal)</span>
<span id="cb230-4"><a href="more-on-multiple-linear-regression.html#cb230-4" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(red.model1, appraisal.fit)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: saleprice ~ LandImprov + area
## Model 2: saleprice ~ landvalue + impvalue + area
##   Res.Df       RSS Df Sum of Sq  F Pr(&gt;F)
## 1     17 1.002e+09                       
## 2     16 1.002e+09  1     426.8  0  0.998</code></pre>
<p><strong>Method 2</strong> - Calculation “on the fly”</p>
<p>The <code>I()</code> function in R can be used to create a single new predictor for the null model by calculating the sum of the landvalue and impvalue variables “on the fly.” It is important to note this variable is not saved for future use. Here is the analysis:</p>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb232-1"><a href="more-on-multiple-linear-regression.html#cb232-1" aria-hidden="true" tabindex="-1"></a>red.model2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(saleprice <span class="sc">~</span> <span class="fu">I</span>(landvalue <span class="sc">+</span> impvalue) <span class="sc">+</span> area, <span class="at">data=</span>appraisal)</span>
<span id="cb232-2"><a href="more-on-multiple-linear-regression.html#cb232-2" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(red.model2, appraisal.fit)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: saleprice ~ I(landvalue + impvalue) + area
## Model 2: saleprice ~ landvalue + impvalue + area
##   Res.Df       RSS Df Sum of Sq  F Pr(&gt;F)
## 1     17 1.002e+09                       
## 2     16 1.002e+09  1     426.8  0  0.998</code></pre>
<p>In both cases we see the <span class="math inline">\(F\)</span>-statistic value is essentially 0, with 1 and 16 degrees of freedom. The <span class="math inline">\(p\)</span>-value is 0.998, so we fail to reject <span class="math inline">\(H_0\)</span>. There is insufficient evidence to conclude that land value appraisal and improvements value appraisal have different effects on the mean sale price of a property.</p>
<p>Lastly, note the two fitted reduced models are the same</p>
<div class="sourceCode" id="cb234"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb234-1"><a href="more-on-multiple-linear-regression.html#cb234-1" aria-hidden="true" tabindex="-1"></a>red.model1</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = saleprice ~ LandImprov + area, data = appraisal)
## 
## Coefficients:
## (Intercept)   LandImprov         area  
##    1386.277        0.819       13.605</code></pre>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb236-1"><a href="more-on-multiple-linear-regression.html#cb236-1" aria-hidden="true" tabindex="-1"></a>red.model2</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = saleprice ~ I(landvalue + impvalue) + area, data = appraisal)
## 
## Coefficients:
##             (Intercept)  I(landvalue + impvalue)                     area  
##                1386.277                    0.819                   13.605</code></pre>
<p>Also note the fitted full model</p>
<div class="sourceCode" id="cb238"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb238-1"><a href="more-on-multiple-linear-regression.html#cb238-1" aria-hidden="true" tabindex="-1"></a>appraisal.fit</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = saleprice ~ landvalue + impvalue + area, data = appraisal)
## 
## Coefficients:
## (Intercept)    landvalue     impvalue         area  
##    1384.197        0.818        0.819       13.605</code></pre>
<p>The coefficients on <code>landvalue</code> and <code>impvalue</code> are nearly identical (differ in the third decimal place). It should not be too surprising we failed to reject the null hypothesis that they were statistically the same.</p>
</div>
<div id="categorical-predictor-variables" class="section level2" number="7.2">
<h2><span class="header-section-number">7.2</span> Categorical Predictor Variables</h2>
<p>Up to now, all of our regression examples have included predictors that have been numerically valued variables; i.e., they have been quantitative variables. Predictors that are qualitative in nature (e.g., gender, ethnic group, eye color) are usually called categorical predictors or factors. How can predictors such as these be incorporated into a regression analysis?</p>
<p>To use qualitative predictors in a regression model, we need to recode their values. The way to do this is to create one or more dummy variables. Dummy variables are variables that only take on the value 0 or 1, indicating the absence (0) or presence (1) of a particular qualitative characteristic.</p>
<div id="a-qualitative-predictor-with-two-levels" class="section level3" number="7.2.1">
<h3><span class="header-section-number">7.2.1</span> A qualitative predictor with two levels</h3>
<p>Suppose we are interested in the effect of a fixed dosage of two different medications on reducing blood glucose level in men. One of these meds is an experimental drug (let’s call it drug A), and the other is a placebo (drug P). For a two-level variable such as this, one dummy variable is all that is required to distinguish between the two drugs. We could define our dummy variable as follows:</p>
<p><span class="math display">\[X = \left\{\begin{array}{ll} 0 &amp; \textrm{if drug } P \\ 1 &amp; \textrm{if drug } A\end{array}\right.\]</span></p>
<p><span class="math inline">\(X\)</span> is now a “numerically valued” (0 or 1) variable that simply serves to indicate that drug A was the administered drug. In such a coding, the placebo will serve as a reference (or “baseline”) group to which the drug A group will be compared. We will see this when we construct models using <span class="math inline">\(X\)</span>.</p>
<p>How does the regression model look? First, we fit a linear model in the usual way using <span class="math inline">\(X\)</span> as the predictor. The model form is given by</p>
<p><span class="math display">\[Y = \beta_0 + \beta_1 X + \varepsilon\]</span></p>
<p>Writing out the model explicitly for both drug groups will prove enlightening. Since <span class="math inline">\(X\)</span> is basically an on/off switch indicating if <span class="math inline">\(A\)</span> is the administered drug, the model parameters and their interpretations break down as follows:</p>
<p><span class="math display">\[\begin{array}{ccc}
\hline
\textbf{Drug} &amp; \mathbf{X} &amp; \textbf{Model for mean response} \\
\hline
Placebo &amp; 0 &amp; \mu_{placebo} =  \beta_0 + \beta_1(0) = \beta_0 \\
A &amp; 1 &amp; \mu_A = \beta_0 + \beta_1(1) = \beta_0 + \beta_1 \\
\hline
\end{array}\]</span></p>
<p>Key items to notice:</p>
<ul>
<li>The structural part of the model under the placebo is just <span class="math inline">\(\beta_0\)</span>. So, <span class="math inline">\(\beta_0\)</span> represents <span class="math inline">\(\mu_{placebo}\)</span>, the mean response for <span class="math inline">\(Y\)</span> under the placebo.</li>
<li>The structural part of the model under drug <span class="math inline">\(A\)</span> is <span class="math inline">\(\beta_0 + \beta_1\)</span>. So, <span class="math inline">\(\beta_0 + \beta_1\)</span> represents <span class="math inline">\(\mu_A\)</span>, the mean response for <span class="math inline">\(Y\)</span> under drug <span class="math inline">\(A\)</span>.</li>
<li>Now, put these two facts together. What does <span class="math inline">\(\beta_1\)</span> alone estimate? It should quickly become clear that <span class="math inline">\(\beta_1\)</span> represents the change in the mean response for <span class="math inline">\(Y\)</span> from <span class="math inline">\(placebo\)</span> to drug <span class="math inline">\(A\)</span>. In other words, <span class="math inline">\(\beta_1\)</span> is the key parameter of interest in the model, since it is an estimate of <span class="math inline">\(\mu_A - \mu_{placebo}\)</span>. So, the usual regression <em>t</em>-test of <span class="math inline">\(H_0: \beta_1 = 0\)</span> is really a test of <span class="math inline">\(H_0: \mu_A - \mu_{placebo} = 0\)</span>, or equivalently, <span class="math inline">\(H_0: \mu_A = \mu_{placebo}\)</span> (essentially the two-sample <em>t</em>-test). Let’s consider an example.</li>
</ul>
<p><strong>Example.</strong> Previous research indicates that children borne by diabetic mothers may suffer from obesity, high blood pressure and glucose intolerance. Independent random samples of adolescent offspring of diabetic and non-diabetic mothers were evaluated for potential differences in vital measurements, including blood pressure. The data are in the file <code>diabeticoffspring.txt</code> in our data repository. Use this data to (a) test to see if there is a difference in mean systolic BP between diabetic and non-diabetic offspring, and if so (b) estimate the true mean difference using a 95% CI. This is essentially an intro statistics problem.</p>
<p>We begin by obtaining the data.</p>
<div class="sourceCode" id="cb240"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb240-1"><a href="more-on-multiple-linear-regression.html#cb240-1" aria-hidden="true" tabindex="-1"></a>www <span class="ot">&lt;-</span> <span class="st">&quot;http://www.users.miamioh.edu/hughesmr/sta363/diabeticoffspring.txt&quot;</span></span>
<span id="cb240-2"><a href="more-on-multiple-linear-regression.html#cb240-2" aria-hidden="true" tabindex="-1"></a>diabetes <span class="ot">&lt;-</span> <span class="fu">read.table</span>(www, <span class="at">header=</span><span class="cn">TRUE</span>)</span>
<span id="cb240-3"><a href="more-on-multiple-linear-regression.html#cb240-3" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(diabetes)</span></code></pre></div>
<pre><code>##   sys.bp   mother
## 1    127 diabetic
## 2    137 diabetic
## 3    112 diabetic
## 4    108 diabetic
## 5    123 diabetic
## 6    120 diabetic</code></pre>
<p>Now we perform a two-sample <em>t</em>-test.</p>
<div class="sourceCode" id="cb242"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb242-1"><a href="more-on-multiple-linear-regression.html#cb242-1" aria-hidden="true" tabindex="-1"></a><span class="fu">t.test</span>(sys.bp <span class="sc">~</span> mother, <span class="at">var.equal=</span><span class="cn">TRUE</span>, <span class="at">data=</span>diabetes)</span></code></pre></div>
<pre><code>## 
##  Two Sample t-test
## 
## data:  sys.bp by mother
## t = 4.559, df = 177, p-value = 9.58e-06
## alternative hypothesis: true difference in means between group diabetic and group nondiabetic is not equal to 0
## 95 percent confidence interval:
##   4.54379 11.48121
## sample estimates:
##    mean in group diabetic mean in group nondiabetic 
##                   118.000                   109.987</code></pre>
<p>Now, let’s re-do this as a regression problem using a quantitative predictor. If the assumptions underlying the independent samples <em>t</em>-test are met, then the assumptions underlying a regression approach to the problem are also met (we will check these momentarily).</p>
<p>Note that the variable <code>mother</code> is a character variable in the data frame, R automatically treates it as a qualitative variable and creates a dummy variable for you.</p>
<div class="sourceCode" id="cb244"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb244-1"><a href="more-on-multiple-linear-regression.html#cb244-1" aria-hidden="true" tabindex="-1"></a><span class="fu">class</span>(diabetes<span class="sc">$</span>mother)</span></code></pre></div>
<pre><code>## [1] &quot;character&quot;</code></pre>
<p>So we can perform regression as before, and R will handle the <code>factor</code> variable correctly.</p>
<div class="sourceCode" id="cb246"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb246-1"><a href="more-on-multiple-linear-regression.html#cb246-1" aria-hidden="true" tabindex="-1"></a>diabetes.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(sys.bp <span class="sc">~</span> mother, <span class="at">data=</span>diabetes)</span>
<span id="cb246-2"><a href="more-on-multiple-linear-regression.html#cb246-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(diabetes.fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = sys.bp ~ mother, data = diabetes)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -33.00  -6.99  -0.99   8.00  32.01 
## 
## Coefficients:
##                   Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)         118.00       1.18  100.42  &lt; 2e-16 ***
## mothernondiabetic    -8.01       1.76   -4.56  9.6e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 11.7 on 177 degrees of freedom
## Multiple R-squared:  0.105,  Adjusted R-squared:   0.1 
## F-statistic: 20.8 on 1 and 177 DF,  p-value: 9.58e-06</code></pre>
<p>Note that the independent samples t-test and the regression <em>t</em>-test for <span class="math inline">\(\beta_1\)</span> produce identical <span class="math inline">\(p\)</span>-values. That’s because they both test the exact same hypothesis. Also note that the estimate for <span class="math inline">\(\beta_{1}\)</span> in the regression model is <span class="math inline">\(b_1 = –8.013\)</span>, which is what you get when you calculate the difference the two group’s mean estimates (109.9875 – 118.0000).</p>
<p>You should note that R has labeled the predictor variable <code>mothernondiabetic</code>. This is your indication as to the coding scheme used by R for the qualitative predictor. In this case, R created a dummy variable as</p>
<p><span class="math display">\[X = \left\{\begin{array}{ll} 0 &amp; \textrm{if mother is } diabetic \\ 1 &amp; \textrm{if mother is } not~diabetic\end{array}\right.\]</span></p>
<p>and so we can interpret the meaning of the <span class="math inline">\(\beta\)</span> parameters in the model <span class="math inline">\(Y = \beta_0 + \beta_1 X + \varepsilon\)</span>:</p>
<p><span class="math display">\[\begin{array}{ccc}
\hline
\textbf{Mother} &amp; \mathbf{X} &amp; \textbf{Model for mean response} \\
\hline
Diabetic &amp; 0 &amp; \mu_{diabetic} =  \beta_0 + \beta_1(0) = \beta_0 \\
Not~diabetic &amp; 1 &amp; \mu_{nondiabetic} = \beta_0 + \beta_1(1) = \beta_0 + \beta_1 \\
\hline
\end{array}\]</span></p>
<ul>
<li><span class="math inline">\(\beta_0\)</span> is the true mean systolic blood pressure of diabetic mothers.</li>
<li><span class="math inline">\(\beta_0 + \beta_1\)</span> is the true mean systolic blood pressure of nondiabetic mothers.</li>
<li>Therefore, <span class="math inline">\(\beta_1 = (\beta_0 + \beta_1) – \beta_0\)</span> is the true mean change in systolic blood pressure from diabetic to nondiabetic mothers.</li>
</ul>
<p>Confidence intervals for the <span class="math inline">\(\beta\)</span> parameters in this regression model should be interpreted accordingly, and are found the same as before using <code>confint()</code>:</p>
<div class="sourceCode" id="cb248"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb248-1"><a href="more-on-multiple-linear-regression.html#cb248-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(diabetes.fit)</span></code></pre></div>
<pre><code>##                      2.5 %    97.5 %
## (Intercept)       115.6811 120.31892
## mothernondiabetic -11.4812  -4.54379</code></pre>
<p>So, our interpretations are as follows:</p>
<ul>
<li><span class="math inline">\(b_0 = 118.0\)</span> is our estimate of <span class="math inline">\(\beta_0\)</span>, the true mean systolic blood pressure of diabetic mothers. The 95% CI for <span class="math inline">\(\beta_0\)</span> is (115.68, 120.32). Thus, we can be 95% confident that the true mean systolic blood pressure of diabetic mothers is between 115.68 to 120.32.</li>
<li><span class="math inline">\(b_1 = -8.013\)</span> is our estimate of <span class="math inline">\(\beta_1\)</span>, the true mean change in systolic blood pressure from diabetic to nondiabetic mothers. The 95% CI for <span class="math inline">\(\beta_1\)</span> is (–11.48, –4.54). Thus, we can be 95% confident that the true mean systolic blood pressure in nondiabetic mothers is between 4.54 to 11.48 lower than in diabetic mothers.</li>
</ul>
<p>If you look up at the original independent samples <em>t</em>-test output, you’ll see this same confidence interval expressed in terms of <span class="math inline">\(\mu_{diabetic} – \mu_{nondiabetic}\)</span>.</p>
<p><strong>Important.</strong> When we find confidence intervals for the <span class="math inline">\(\beta\)</span> parameters in a regression model with a qualitative predictor, it is imperative to know the interpretation of each of the <span class="math inline">\(\beta\)</span> parameters in the context of the problem. In other words, you need to know both of</p>
<ul>
<li>The form of the model used (in the last example, this was <span class="math inline">\(Y = \beta_0 + \beta_1 X + \varepsilon\)</span>)</li>
<li>The manner of coding used for the qualitative variable (here, <span class="math inline">\(X=1\)</span> is a non-diabetic mother).</li>
</ul>
</div>
<div id="a-qualitative-predictor-with-more-than-two-levels" class="section level3" number="7.2.2">
<h3><span class="header-section-number">7.2.2</span> A qualitative predictor with more than two levels</h3>
<p>When using a qualitative predictor with more than two levels, we must use a coding scheme (i.e., a method of constructing dummy variables) that adequately distinguishes between all the levels of the qualitative predictor. Basically, let this be your guide:</p>
<blockquote>
<p>In general, if a factor has <span class="math inline">\(k\)</span> levels, we will require <span class="math inline">\(k – 1\)</span> dummy variables to sufficiently code the factor for regression.</p>
</blockquote>
<p>For example, consider a study of the efficacy of four drugs (<span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, <span class="math inline">\(C\)</span> and a placebo <span class="math inline">\(P\)</span>) on glucose reduction. Coding up these 4 drugs in the regression will require 4 – 1 = 3 dummy variables. We have flexibility in how we could define these three dummies, but here is probably the most meaningful way given the context of the experiment:</p>
<p><span class="math display">\[I_A = \left\{\begin{array}{ll} 1 &amp; \textrm{if drug } A \\ 0 &amp; \textrm{otherwise}\end{array}\right., ~~~ I_B = \left\{\begin{array}{ll} 1 &amp; \textrm{if drug } B \\ 0 &amp; \textrm{otherwise}\end{array}\right.,~~~ I_C = \left\{\begin{array}{ll} 1 &amp; \textrm{if drug } C \\ 0 &amp; \textrm{otherwise}\end{array}\right.\]</span>
<span class="math inline">\(I_A\)</span> serves as an indicator that drug <span class="math inline">\(A\)</span> was administered, <span class="math inline">\(I_B\)</span> for drug <span class="math inline">\(B\)</span>, and <span class="math inline">\(I_C\)</span> for drug <span class="math inline">\(C\)</span>. The model may be written as</p>
<p><span class="math display">\[Y = \beta_0 + \beta_1(I_A) + \beta_2(I_B) + \beta_3(I_C) + \varepsilon\]</span></p>
<p>So how do we indicate when the placebo <span class="math inline">\(P\)</span> is the administered drug? That’s easy: it’s when all three dummy variables are 0 (i.e. “switched off”). In this coding, the placebo <span class="math inline">\(P\)</span> serves as the reference level to which drugs <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span> will be compared. You can see this by constructing the model using <span class="math inline">\(I_A\)</span>, <span class="math inline">\(I_B\)</span> and <span class="math inline">\(I_C\)</span>:</p>
<p><span class="math display">\[\begin{array}{ccccc}
\hline
\textbf{Drug} &amp; \mathbf{I_A} &amp; \mathbf{I_B} &amp; \mathbf{I_C} &amp; \textbf{Model for mean response} \\
\hline
A &amp; 1 &amp; 0 &amp; 0 &amp; \mu_{A} =  \beta_0 + \beta_1(1) + \beta_2(0) + \beta_3(0) = \beta_0 +\beta_1\\
B &amp; 0 &amp; 1 &amp; 0 &amp; \mu_{B} = \beta_0 + \beta_1(0) + \beta_2(1) + \beta_3(0) = \beta_0 + \beta_2\\
C &amp; 0 &amp; 0 &amp; 1 &amp; \mu_{C} = \beta_0 + \beta_1(0) + \beta_2(0) + \beta_3(1) = \beta_0 + \beta_3\\
Placebo &amp; 0 &amp; 0 &amp; 0 &amp; \mu_{P} = \beta_0 + \beta_1(0) + \beta_2(0) + \beta_3(0) = \beta_0\\
\hline
\end{array}\]</span></p>
<p>From this coding stems the physical interpretation of the model’s <span class="math inline">\(\beta\)</span> coefficients:</p>
<ul>
<li><span class="math inline">\(\beta_0\)</span> is the true mean glucose reduction under the placebo.</li>
<li><span class="math inline">\(\beta_1\)</span> is the true mean change in glucose reduction from placebo to drug <span class="math inline">\(A\)</span>.</li>
<li><span class="math inline">\(\beta_2\)</span> is the true mean change in glucose reduction from placebo to drug <span class="math inline">\(B\)</span>.</li>
<li><span class="math inline">\(\beta_3\)</span> is the true mean change in glucose reduction from placebo to drug <span class="math inline">\(C\)</span>.</li>
</ul>
<p><strong>Important note.</strong> The above dummy variable coding scheme is the most meaningful way to code drug for this analysis because the placebo is the natural choice for a “reference” drug. However, there are many ways to create a valid coding, and we must know which one R uses in order to correctly interpret the model.</p>
<p><strong>Whole model <span class="math inline">\(F\)</span>-test.</strong> We’ve seen that the whole model ANOVA <span class="math inline">\(F\)</span>-test for a multiple linear regression model given by <span class="math inline">\(Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \ldots + \beta_k X_k + \varepsilon\)</span> is a test of the hypotheses</p>
<p><span class="math display">\[H_0: \beta_1 = \beta_2 = \ldots = \beta_k = 0 ~~~\textrm{versus}~~~ H_a: \textrm{At least one } \beta_i \neq 0\]</span></p>
<p>So when we have a regression model containing one qualitative predictor with more than two levels, what does the whole-model F-test really test? It all stems back to the interpretation of the model’s <span class="math inline">\(\beta\)</span> coefficients based on the coding scheme used.</p>
<p>Consider the previous four drug example. The whole model <span class="math inline">\(F\)</span>-test would be a test of the hypothesis <span class="math inline">\(H_0: \beta_1 = \beta_2 = \beta_3 = 0\)</span>. If you look at the aforementioned table, it is not too difficult to see that</p>
<ul>
<li><span class="math inline">\(\beta_1 = (\beta_0 + \beta_1) – \beta_0 = \mu_A – \mu_P\)</span></li>
<li><span class="math inline">\(\beta_2 = (\beta_0 + \beta_2) – \beta_0 = \mu_B – \mu_P\)</span></li>
<li><span class="math inline">\(\beta_3 = (\beta_0 + \beta_3) – \beta_0 = \mu_C – \mu_P\)</span></li>
</ul>
<p>So, the following must be interchangeable:</p>
<p><span class="math display">\[H_0: \beta_1 = \beta_2 = \beta_3 = 0 ~~\longleftrightarrow~~ H_0: \mu_A – \mu_P = \mu_B – \mu_P = \mu_C – \mu_P = 0\]</span></p>
<p>If we just add <span class="math inline">\(\mu_P\)</span> to each piece of the hypothesis as expressed on the right, we see that the null hypothesis for the whole model <span class="math inline">\(F\)</span>-test here is actually a test of</p>
<p><span class="math inline">\(H_0: \mu_A = \mu_B = \mu_C = \mu_P\)</span></p>
<p>In other words, we now have a way to test for the simultaneous equivalence of multiple population means! This is a powerful tool for facilitating population comparisons that we will draw on frequently.</p>
<p><strong>Note</strong> We have essentially derived a One-Way ANOVA test using multiple regression! Mathematically it can be shown that One-Way ANOVA (in fact, most of experimental design) can be expressed a multiple regression problem.</p>
<p><strong>Example:</strong> Posttraumatic stress disorder in rape victims. This example is based on a study in the Journal of Counseling and Clinical Psychology. The subjects were 45 rape victims who were randomly assigned to one of four groups. The four groups were:</p>
<ol style="list-style-type: decimal">
<li>Stress Inoculation Therapy (SIT) in which subjects were taught a variety of coping skills</li>
<li>Prolonged Exposure (PE) in which subjects went over the rape in their mind repeatedly for seven sessions</li>
<li>Supportive Counseling (SC) which was a standard therapy control group</li>
<li>Waiting List (WL) – a baseline control group.</li>
</ol>
<p>In the actual study, pre- and post-treatment measures were taken on a number of variables. For our purposes we will only look at post-treatment data on PTSD severity, which was the total number of symptoms endorsed by the subject. The goal is to compare the effects of the treatments with respect to post-treatment PTSD severity. The data appear in the R workspace <code>ptsd.RData</code> in our repository.</p>
<div class="sourceCode" id="cb250"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb250-1"><a href="more-on-multiple-linear-regression.html#cb250-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;ptsd.RData&quot;</span>)</span>
<span id="cb250-2"><a href="more-on-multiple-linear-regression.html#cb250-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(ptsd)</span></code></pre></div>
<pre><code>##   ID Group Score
## 1  1     1     3
## 2  2     1    13
## 3  3     1    13
## 4  4     1     8
## 5  5     1    11
## 6  6     1     9</code></pre>
<p>As always, first look at the data. Since the predictor is qualitative (although coded as numeric), side-by-side boxplots are in order. It is also a good idea to coerce the numeric codes for the variable Group to be recognized by R as qualitative values rather than quantitative numbers. This can be done via the <code>as.factor()</code> command:</p>
<div class="sourceCode" id="cb252"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb252-1"><a href="more-on-multiple-linear-regression.html#cb252-1" aria-hidden="true" tabindex="-1"></a>ptsd <span class="ot">&lt;-</span> ptsd <span class="sc">%&gt;%</span></span>
<span id="cb252-2"><a href="more-on-multiple-linear-regression.html#cb252-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">Group=</span><span class="fu">as.factor</span>(Group))</span>
<span id="cb252-3"><a href="more-on-multiple-linear-regression.html#cb252-3" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(ptsd) <span class="sc">+</span> </span>
<span id="cb252-4"><a href="more-on-multiple-linear-regression.html#cb252-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>(<span class="fu">aes</span>(<span class="at">x=</span>Group, <span class="at">y=</span>Score)) <span class="sc">+</span> </span>
<span id="cb252-5"><a href="more-on-multiple-linear-regression.html#cb252-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">ylab</span>(<span class="st">&quot;Post treatment PTSD score&quot;</span>) <span class="sc">+</span> </span>
<span id="cb252-6"><a href="more-on-multiple-linear-regression.html#cb252-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code></pre></div>
<p><img src="introStatModeling_files/figure-html/ch7.11-1.png" width="672" /></p>
<p>There is a lot of variability in the PTSD score in Group 2 (Prolonged Exposure treatment). At first glance, it appears as though SIT therapy may be producing a lower average number of post-treatment symptoms.</p>
<p>The dramatic difference in variability patterns in the response from treatment to treatment may result in a violation of the constant variance assumption in the upcoming regression analysis.</p>
<p>Let’s run a regression using Group as our qualitative predictor of PTSD score. As always, first check the regression assumptions before any inference:</p>
<div class="sourceCode" id="cb253"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb253-1"><a href="more-on-multiple-linear-regression.html#cb253-1" aria-hidden="true" tabindex="-1"></a>ptsd.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(Score <span class="sc">~</span> Group, <span class="at">data=</span>ptsd)</span>
<span id="cb253-2"><a href="more-on-multiple-linear-regression.html#cb253-2" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(ptsd.fit) <span class="sc">+</span> <span class="fu">theme_minimal</span>()</span></code></pre></div>
<p><img src="introStatModeling_files/figure-html/ch7.12-1.png" width="672" /></p>
<p>Note how these plots look different when dealing with a qualitative predictor.</p>
<p>Normality appears to be OK. No outliers appear to be present. Constant variance appears to have some potential violations (see the wobbly Scale-Location plot with a peak in the group with a fitted value of about 15.5). Nonetheless, we will proceed for now with hypothesis tests as an illustration, even though we know the results may be suspect.</p>
<div class="sourceCode" id="cb254"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb254-1"><a href="more-on-multiple-linear-regression.html#cb254-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ptsd.fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Score ~ Group, data = ptsd)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -13.40  -4.40   0.50   4.93  18.60 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    11.07       1.99    5.56  1.8e-06 ***
## Group2          4.33       3.09    1.40   0.1683    
## Group3          7.02       3.00    2.34   0.0244 *  
## Group4          8.43       3.09    2.73   0.0093 ** 
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.46 on 41 degrees of freedom
## Multiple R-squared:  0.182,  Adjusted R-squared:  0.122 
## F-statistic: 3.05 on 3 and 41 DF,  p-value: 0.0394</code></pre>
<p>Note that R by default used a dummy variable coding that sets Group=1 as the reference level for Group. The default coding in R is to order the factor levels (either alphabetically or numerically, whichever applies) and choose the first level as the reference level. Here, the ordered Group levels in the data frame are 1, 2, 3 and 4: thus, 1 was chosen as the reference level.</p>
<p>For convenience, you can change the reference level. The command to do so is called <code>relevel()</code> in R. In this example, Group=4 (the Waiting List baseline control group) is the most natural choice for a reference group to which we may compare the other treatments for PTSD. So, I change the Group reference level to 4 below and re-fit the model. Take note of what changes and what does not change from the first coding scheme:</p>
<div class="sourceCode" id="cb256"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb256-1"><a href="more-on-multiple-linear-regression.html#cb256-1" aria-hidden="true" tabindex="-1"></a>ptsd <span class="ot">&lt;-</span> ptsd <span class="sc">%&gt;%</span></span>
<span id="cb256-2"><a href="more-on-multiple-linear-regression.html#cb256-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">Group=</span><span class="fu">relevel</span>(Group, <span class="at">ref=</span><span class="st">&quot;4&quot;</span>))</span>
<span id="cb256-3"><a href="more-on-multiple-linear-regression.html#cb256-3" aria-hidden="true" tabindex="-1"></a>ptsd.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(Score <span class="sc">~</span> Group, <span class="at">data=</span>ptsd)</span>
<span id="cb256-4"><a href="more-on-multiple-linear-regression.html#cb256-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ptsd.fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = Score ~ Group, data = ptsd)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -13.40  -4.40   0.50   4.93  18.60 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)    19.50       2.36    8.27  2.8e-10 ***
## Group1         -8.43       3.09   -2.73   0.0093 ** 
## Group2         -4.10       3.33   -1.23   0.2258    
## Group3         -1.41       3.26   -0.43   0.6676    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.46 on 41 degrees of freedom
## Multiple R-squared:  0.182,  Adjusted R-squared:  0.122 
## F-statistic: 3.05 on 3 and 41 DF,  p-value: 0.0394</code></pre>
<p>We can investigate the effects of different therapies using elements from the output above as follows:</p>
<ul>
<li>The whole-model F-test for <span class="math inline">\(H_0: \mu_1 = \mu_2 = \mu_3 = \mu_4\)</span> is significant (<span class="math inline">\(F(3, 41)\)</span> = 3.046, <span class="math inline">\(p\)</span>-value = 0.0394), so we can conclude that there is a significant difference in the true mean post-treatment PTSD severity scores between at least two of the treatments. [Note how the <span class="math inline">\(F\)</span>-test is invariant to the dummy variable scheme used.]</li>
<li>The different therapies applied account for only 12.2% of the total variation in the observed PTSD severity scores. [<span class="math inline">\(R_a^2\)</span> is also invariant to the dummy variable scheme used.]</li>
<li>The estimated mean PTSD scores for the four treatment groups are:
<ul>
<li><span class="math inline">\(b_0 = 19.5\)</span> for the Waiting List (WL) control group,</li>
<li><span class="math inline">\(b_0 + b_1 = 19.5 – 8.429 = 11.071\)</span> for the SIT therapy,</li>
<li><span class="math inline">\(b_0 + b_2 = 19.5 – 4.100 = 15.4\)</span> for the PE therapy,</li>
<li><span class="math inline">\(b_0 + b_3 = 19.5 – 1.409 = 18.091\)</span> for the SE therapy.</li>
</ul></li>
<li>In comparing therapies to the WL control therapy, the only significant difference in mean PTSD score occurs with Group 1 (SIT). The test of <span class="math inline">\(H_0: \beta_1 = 0\)</span> here is equivalent to a test of <span class="math inline">\(H_0: \mu_{SIT} – \mu_{WL} = 0.\)</span> There is sufficient evidence here to conclude that the true mean PTSD score is significantly lower in the SIT therapy that under the WL therapy (<span class="math inline">\(t\)</span> = –2.731, df = 41, <span class="math inline">\(p\)</span>-value = 0.00928).</li>
</ul>
<p>A 95% confidence interval for <span class="math inline">\(\beta_1\)</span> (and hence <span class="math inline">\(\mu_{SIT} – \mu_{WL}\)</span>) can be found thus:</p>
<div class="sourceCode" id="cb258"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb258-1"><a href="more-on-multiple-linear-regression.html#cb258-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(ptsd.fit)</span></code></pre></div>
<pre><code>##                 2.5 %   97.5 %
## (Intercept)  14.73889 24.26111
## Group1      -14.66232 -2.19482
## Group2      -10.83322  2.63322
## Group3       -7.98751  5.16932</code></pre>
<p>We can be 95% confident that the SIT therapy will produce, on average, between 2.19 to 14.66 fewer PTSD symptoms than will the WL therapy. [Note that this CI is the only one among the CIs for <span class="math inline">\(\beta_1\)</span>, <span class="math inline">\(\beta_2\)</span>, and <span class="math inline">\(\beta_3\)</span> that does not contain 0.]</p>
</div>
</div>
<div id="bridging-regression-and-designed-experiments-ancova" class="section level2" number="7.3">
<h2><span class="header-section-number">7.3</span> Bridging Regression and Designed Experiments – ANCOVA</h2>
<p>Analysis of Covariance (or ANCOVA) refers to regression problems that have a mixture of both quantitative and qualitative predictors.</p>
<p>As an illustration, let’s return to the experiment where we are interested in the effect of a fixed dosage of two different medications on reducing blood glucose level in men. One of these meds is an experimental drug (drug <span class="math inline">\(A\)</span>), and the other is a placebo (drug <span class="math inline">\(P\)</span>). Let’s create a dummy variable <span class="math inline">\(D\)</span> for distinguishing the drug groups as follows:</p>
<p><span class="math display">\[D = \left\{\begin{array}{ll} 0 &amp; \textrm{if drug } P \\ 1 &amp; \textrm{if drug } A \end{array}\right.\]</span>
Suppose that we know that the body weight of an individual might influence the effectiveness of the drug at the administered dose. For example, the effectiveness of the placebo might not be impacted by body weight, but it may be the case that for people taking the experimental drug, the more you weigh the less effective the drug is. If this is the case, then perhaps we should not model glucose reduction using only <span class="math inline">\(D\)</span> as our predictor.</p>
<p>Analysis of covariance is a technique that can be used to build a model to predict glucose reduction as a function of both body weight (a quantitative predictor, sometimes called a covariate) and type of drug (a qualitative predictor, or factor). ANCOVA adjusts the group comparison for weight differences and then estimates the effectiveness of the drugs.</p>
<p>ANCOVA is a powerful analytical tool because:</p>
<ul>
<li>it is tantamount to simultaneously fitting different regression models to the different groups and then formally comparing the models.</li>
<li>it can also be used when you have more than two groups and/or more than one covariate.</li>
</ul>
<p>For now, we’ll look at the simple case of two groups and one covariate.</p>
<p>Let <span class="math inline">\(Y\)</span> = glucose reduction and <span class="math inline">\(X\)</span> = body weight. A variety of different linear models may be considered here, offered below from simple to complex:</p>
<ol style="list-style-type: decimal">
<li><p>Same regression line for both groups. We fit the model <span class="math inline">\(Y = \beta_0 + \beta_1 X + \varepsilon\)</span>. Nothing in the model references which drug is administered, so both drug groups are effectively “pooled” together. This model imposes structure on the data that says that the effect of body weight on glucose reduction is identical regardless of drug – obviously, this may be too simplistic and unrealistic.</p></li>
<li><p>Separate regression lines for each group, but with equal slopes. We fit the model <span class="math inline">\(Y = \beta_0 + \beta_1 X + \beta_2 D + \varepsilon\)</span>. This is known as an additive model involving weight and drug type. This model imposes structure on the data that says that the effect that the different drugs have glucose reduction is the same regardless of weight. In other words, this model allows for a difference in drug effect to be modeled, but restricts that effect to be the same regardless of body weight.</p>
<p>Writing out the model explicitly for both drug groups will prove enlightening. The model parameters and their interpretations break down as follows:
<span class="math display">\[\begin{array}{ccc} 
\hline
\textbf{Drug} &amp; \mathbf{D} &amp; \textbf{Model} \\
\hline
placebo &amp; 0 &amp; Y = \beta_0 + \beta_1 X + \beta_2(0) + \varepsilon = \beta_0 + \beta_1 X + \varepsilon \\
A       &amp; 1 &amp;   Y = \beta_0 + \beta_1 X + \beta_2(1) + \varepsilon = (\beta_0 + \beta_2) + \beta_1 X + \varepsilon \\
\hline
\end{array}\]</span>
You can see that the slope of the model for either drug is the same (<span class="math inline">\(=\beta_1\)</span>), so the model imposes that body weight has the same effect on glucose reduction regardless of drug. However, the intercept terms differ between the models for the two drugs: it is <span class="math inline">\(\beta_0\)</span> for placebo, but <span class="math inline">\(\beta_0 + \beta_2\)</span> for drug <span class="math inline">\(A\)</span>. Since the difference between these two intercepts is <span class="math inline">\(\beta_2\)</span>, its interpretation is that it represents the constant shift in glucose reduction when changing from placebo to drug <span class="math inline">\(A\)</span>. In short, <span class="math inline">\(\beta_2\)</span> measures the effect of drug regardless of body weight, and <span class="math inline">\(\beta_1\)</span> measures the effect of body weight regardless of drug. Of course, if the effect of drug truly depends on body weight, then we can’t separate their influence on the response; i.e., their effects are not additive. Which leads us to…..</p></li>
<li><p>Separate regression lines for each group, with different slopes. Recall that earlier we said that the effectiveness of the placebo might not be impacted by body weight, but it may be the case that for people taking the experimental drug that the more you weight the less effective the drug is. If so, the effective difference between the drugs depends on body weight: perhaps for lower weight people the drugs differ greatly in effectiveness, but as weight increases the difference in effectiveness vanishes. In such a case, we say that drug and weight interact to determine the response. Here’s a formal definition:</p>
<p>Two variables <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span> are said to <strong>interact</strong> if the effect that <span class="math inline">\(X_1\)</span> has on the response <span class="math inline">\(Y\)</span> depends on the value of <span class="math inline">\(X_2\)</span>.</p>
<p>To fit an interaction model, we include multiplicative terms between the covariate(s) and factor(s). The model is</p></li>
</ol>
<p><span class="math display">\[Y = \beta_0 + \beta_1 X + \beta_2 D + \beta_3(D\cdot X) + \varepsilon\]</span></p>
<p>This is called an interaction model involving weight and drug type. This model imposes structure on the data that says that the effect that the different drugs have on glucose reduction may vary depending on weight. In other words, this model allows for a difference in drug effect to be modeled, and that the size of that drug effect may vary depending on body weight. Here is how this model breaks out explicitly for each drug:</p>
<p><span class="math display">\[\begin{array}{ccc} 
\hline
\textbf{Drug} &amp; \mathbf{D} &amp; \textbf{Model} \\
\hline
placebo &amp; 0 &amp; Y = \beta_0 + \beta_1 X + \beta_2(0) + \beta_3(0\cdot X) + \varepsilon \\
&amp; &amp; = \beta_0 + \beta_1 X + \varepsilon \\
A       &amp; 1 &amp;   Y = \beta_0 + \beta_1 X + \beta_2(1) + \beta_3(1\cdot X) + \varepsilon \\
&amp; &amp; = (\beta_0 + \beta_2) + (\beta_1 + \beta_3) X + \varepsilon \\
\hline
\end{array}\]</span></p>
<p>The models for each drug differ in both y-intercept and slope, so we are basically fitting different regression lines between glucose reduction and weight for each drug. Of specific interest is how the slopes differ between the models for the two drugs: it is <span class="math inline">\(\beta_1\)</span> for placebo and <span class="math inline">\(\beta_1 + \beta_3\)</span> for drug <span class="math inline">\(A\)</span>. Since the difference between these two slopes is <span class="math inline">\(\beta_3\)</span>, its interpretation is that it represents the difference in the rates of change in glucose reduction for a one-unit increase in weight between placebo to drug <span class="math inline">\(A\)</span>. In short, <span class="math inline">\(\beta_3\)</span> measures the difference in the effect of weight between the two drugs. If <span class="math inline">\(\beta_3 \neq 0\)</span>, then the two groups have different slopes and thus the size of the drug effect on glucose reduction depends on body weight.</p>
<p>Since the unequal slopes model is most flexible, we usually use it as our initial model and then simplify the model if warranted. The next example illustrates this using R.</p>
<div id="an-ancova-example-with-a-two-level-factor" class="section level3" number="7.3.1">
<h3><span class="header-section-number">7.3.1</span> An ANCOVA example with a two-level factor</h3>
<p><strong>Example:</strong> Glucose Reduction. We conduct an experiment to study the efficacy of a fixed dosage of two different medications (a new experimental drug A, and a placebo P) on reducing blood glucose level in men. Body weight may also have an effect on a drug’s impact on glucose levels (measured in mg/dl), so it is included as a covariate for the analysis. The data are in the R workspace <code>drug2.RData</code> in our data repository.</p>
<div class="sourceCode" id="cb260"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb260-1"><a href="more-on-multiple-linear-regression.html#cb260-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;drug2.RData&quot;</span>)</span>
<span id="cb260-2"><a href="more-on-multiple-linear-regression.html#cb260-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(drug2)</span></code></pre></div>
<pre><code>##   drug weight glu.red
## 1    P    160    -4.5
## 2    P    235    -3.4
## 3    P    202   -15.8
## 4    P    173     2.2
## 5    P    166   -11.7
## 6    P    220     5.0</code></pre>
<p>First, let’s look at the data and generate a scatterplot of glu.red by weight. Because we now have a trend line relating glucose reduction to weight for each drug group, we can use an enhanced scatterplot by adding on another aesthetic layer (color).</p>
<div class="sourceCode" id="cb262"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb262-1"><a href="more-on-multiple-linear-regression.html#cb262-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(drug2) <span class="sc">+</span> </span>
<span id="cb262-2"><a href="more-on-multiple-linear-regression.html#cb262-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_point</span>(<span class="fu">aes</span>(<span class="at">x=</span>weight, <span class="at">y=</span>glu.red, <span class="at">color=</span>drug) ) <span class="sc">+</span> </span>
<span id="cb262-3"><a href="more-on-multiple-linear-regression.html#cb262-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_smooth</span>(<span class="fu">aes</span>(<span class="at">x=</span>weight, <span class="at">y=</span>glu.red, <span class="at">color=</span>drug), <span class="at">se=</span><span class="cn">FALSE</span>, <span class="at">method=</span><span class="st">&quot;lm&quot;</span>) <span class="sc">+</span> </span>
<span id="cb262-4"><a href="more-on-multiple-linear-regression.html#cb262-4" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_minimal</span>()</span></code></pre></div>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="introStatModeling_files/figure-html/ch7.17-1.png" width="672" /></p>
<p>We get to see the scatter of points, plus both a linear trend line for both drug groups. Here are some preliminary observations:</p>
<ul>
<li>It appears as though, over this weight range, that there is higher general glucose reduction in experimental drug group (A).<br />
</li>
<li>There appears to be some effect of weight on glucose reduction (generally lower glucose reduction measurements in heavier men; slopes are negative).</li>
<li>There also appears to be some evidence that the effect of weight on glucose reduction is more pronounced under the experimental drug as opposed to the placebo (steeper slope for drug A).</li>
</ul>
<p>Because of the third bullet point above, we should fit the interaction model first. I do so below, after first checking assumptions:</p>
<div class="sourceCode" id="cb264"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb264-1"><a href="more-on-multiple-linear-regression.html#cb264-1" aria-hidden="true" tabindex="-1"></a>drug.fit <span class="ot">&lt;-</span> <span class="fu">lm</span>(glu.red <span class="sc">~</span> weight <span class="sc">+</span> drug <span class="sc">+</span> weight<span class="sc">:</span>drug, <span class="at">data=</span>drug2)</span>
<span id="cb264-2"><a href="more-on-multiple-linear-regression.html#cb264-2" aria-hidden="true" tabindex="-1"></a><span class="fu">autoplot</span>(drug.fit) <span class="sc">+</span> <span class="fu">theme_minimal</span>()</span></code></pre></div>
<p><img src="introStatModeling_files/figure-html/ch7.18-1.png" width="672" /></p>
<p>The assumptions of constant variance, normality and linearity look good. We proceed to investigate the model:</p>
<div class="sourceCode" id="cb265"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb265-1"><a href="more-on-multiple-linear-regression.html#cb265-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(drug.fit)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = glu.red ~ weight + drug + weight:drug, data = drug2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -13.350  -5.566   0.608   5.802  14.294 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)   42.5076    17.9473    2.37    0.027 *
## weight        -0.1869     0.0958   -1.95    0.063 .
## drugP        -27.3532    22.7834   -1.20    0.242  
## weight:drugP   0.0981     0.1212    0.81    0.427  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 8.01 on 23 degrees of freedom
## Multiple R-squared:  0.381,  Adjusted R-squared:   0.3 
## F-statistic: 4.71 on 3 and 23 DF,  p-value: 0.0105</code></pre>
<p>First notice that R automatically created a dummy variable for the two-level drug factor using the following coding:</p>
<p><span class="math display">\[D = \left\{\begin{array}{ll} 0 &amp; \textrm{if drug } A \\ 1 &amp; \textrm{if drug } P \end{array}\right.\]</span></p>
<p>The whole-model <span class="math inline">\(F\)</span>-test is significant (F(3, 23) = 4.712, <span class="math inline">\(p\)</span>-value = 0.01047), verifying that the model does have utility in explaining glucose reduction in men. So, we proceed to investigate the effects of weight and drug type.</p>
<p><em>The first terms you should always check in an interaction model are the interaction terms</em>! Here, we proceed to first check for “equal slopes” by testing the interaction term to see if we can simplify the model. There is only one dummy variable constructed since drug has two levels, so we can just look at the single interaction term <code>weight:drugP</code>. We test <span class="math inline">\(H_0: \beta_{weight.drug~P} = 0\)</span> to check for equal slopes. The test statistic is <span class="math inline">\(t\)</span> = 0.809 with a <span class="math inline">\(p\)</span>-value of 0.4267. Since this is not significant, we can delete the interaction term without hurting the explanatory power of the model. So, an equal slopes (i.e. additive) model is warranted.</p>
<p>We now drop the interaction term, which reduces the model to a main effects model:</p>
<div class="sourceCode" id="cb267"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb267-1"><a href="more-on-multiple-linear-regression.html#cb267-1" aria-hidden="true" tabindex="-1"></a>drug.fit.reduced <span class="ot">&lt;-</span> <span class="fu">lm</span>(glu.red <span class="sc">~</span> weight <span class="sc">+</span> drug, <span class="at">data=</span>drug2)</span>
<span id="cb267-2"><a href="more-on-multiple-linear-regression.html#cb267-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(drug.fit.reduced)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = glu.red ~ weight + drug, data = drug2)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -12.877  -5.674   0.282   5.459  12.145 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept)  31.1166    11.0526    2.82   0.0096 **
## weight       -0.1256     0.0583   -2.16   0.0413 * 
## drugP        -9.0871     3.0639   -2.97   0.0067 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.95 on 24 degrees of freedom
## Multiple R-squared:  0.363,  Adjusted R-squared:  0.31 
## F-statistic: 6.84 on 2 and 24 DF,  p-value: 0.00446</code></pre>
<p>The weight-adjusted effect of drug on glucose reduction is highly significant (<span class="math inline">\(t\)</span> = –2.966, <span class="math inline">\(p\)</span> = 0.0067). Weight is marginally significant (<span class="math inline">\(t\)</span> = –2.156, <span class="math inline">\(p\)</span> = 0.0413). Also note that the model SE has been improved from 8.011 to 7.953. The model cannot be further reduced without sacrifice of explanatory power for the response.</p>
<p>The fitted model, where <span class="math inline">\(Y\)</span> = glucose reduction, is</p>
<p><span class="math display">\[\hat{Y} = 31.116 - 0.126\times\textrm{weight} - 9.087\times D\]</span>
where <span class="math inline">\(D = 1\)</span> if the placebo is administered, and 0 otherwise. Using a breakdown like we first introduced above, we can write out the separate regression lines for each drug:</p>
<p><span class="math display">\[\begin{array}{ccc} 
\hline
\textbf{Drug} &amp; \mathbf{D} &amp; \textbf{Model} \\
\hline
placebo &amp; 1 &amp;  31.116 - 0.013\times\textrm{weight} - 9.087(1)  \\
&amp; &amp; = 22.029 - 0.013\times\textrm{weight} \\
A       &amp; 0 &amp;   31.116 - 0.013\times\textrm{weight} - 9.087() \\
&amp; &amp; = 31.116 - 0.013\times\textrm{weight} \\
\hline
\end{array}\]</span></p>
<p>We can use <code>confint()</code> to find CIs for the <span class="math inline">\(\beta\)</span>-parameters in an ANCOVA model since ANCOVA is just a special case of the usual multiple linear regression model. Here is the result from R, with interpretations provided:</p>
<div class="sourceCode" id="cb269"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb269-1"><a href="more-on-multiple-linear-regression.html#cb269-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(drug.fit.reduced)</span></code></pre></div>
<pre><code>##                  2.5 %      97.5 %
## (Intercept)   8.305129 53.92798704
## weight       -0.245841 -0.00539219
## drugP       -15.410611 -2.76351199</code></pre>
<ul>
<li>The 95% CI for <span class="math inline">\(\beta_{weight}\)</span> is given by (–0.2458, –0.0054). We can be 95% confident that each additional pound of weight lowers the true mean glucose reduction by between 0.0054 mg/dl to 0.2458 mg/dl, regardless of drug. [Note: These are very small changes when expressed in terms of weight change per pound, so it might be more useful to re-express the change in terms of a 10 lb weight increase: the same CI then becomes (<span class="math inline">\(10\times(0.0054)\)</span>, <span class="math inline">\(10\times(0.2458)\)</span>) = (0.054, 2.458) mg/dl drop in glucose reduction for every 10 lb increase in weight, regardless of drug.]</li>
<li>The 95% CI for <span class="math inline">\(\beta_D\)</span> is given by (–15.41, –2.76). Recall that <span class="math inline">\(\beta_D\)</span> measures the change in mean glucose reduction when changing from drug <span class="math inline">\(A\)</span> to the <span class="math inline">\(placebo\)</span>. Thus, we can be 95% confident that drug <span class="math inline">\(A\)</span>’s efficacy is between 2.76 mg/dl to 15.41 mg/dl higher with the placebo. This finding is valid regardless of weight since we are adjusting for weight in the equal slopes model.</li>
</ul>
<p><span class="math inline">\(R^2\)</span> is not great for this model, but none the less we may use the model for making predictions. Here are a few model predictions using R:</p>
<div class="sourceCode" id="cb271"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb271-1"><a href="more-on-multiple-linear-regression.html#cb271-1" aria-hidden="true" tabindex="-1"></a>p.A<span class="fl">.150</span> <span class="ot">&lt;-</span> <span class="fu">predict</span>(drug.fit.reduced, <span class="at">newdata=</span><span class="fu">data.frame</span>(<span class="at">weight=</span><span class="dv">150</span>, <span class="at">drug=</span><span class="st">&quot;A&quot;</span>),<span class="at">int=</span><span class="st">&quot;conf&quot;</span>)</span>
<span id="cb271-2"><a href="more-on-multiple-linear-regression.html#cb271-2" aria-hidden="true" tabindex="-1"></a>p.A<span class="fl">.150</span></span></code></pre></div>
<pre><code>##       fit     lwr     upr
## 1 12.2741 5.99873 18.5494</code></pre>
<div class="sourceCode" id="cb273"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb273-1"><a href="more-on-multiple-linear-regression.html#cb273-1" aria-hidden="true" tabindex="-1"></a>p.P<span class="fl">.150</span> <span class="ot">&lt;-</span> <span class="fu">predict</span>(drug.fit.reduced, <span class="at">newdata=</span><span class="fu">data.frame</span>(<span class="at">weight=</span><span class="dv">150</span>, <span class="at">drug=</span><span class="st">&quot;P&quot;</span>),<span class="at">int=</span><span class="st">&quot;conf&quot;</span>)</span>
<span id="cb273-2"><a href="more-on-multiple-linear-regression.html#cb273-2" aria-hidden="true" tabindex="-1"></a>p.P<span class="fl">.150</span></span></code></pre></div>
<pre><code>##       fit      lwr     upr
## 1 3.18701 -3.04853 9.42255</code></pre>
<div class="sourceCode" id="cb275"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb275-1"><a href="more-on-multiple-linear-regression.html#cb275-1" aria-hidden="true" tabindex="-1"></a>p.A<span class="fl">.225</span> <span class="ot">&lt;-</span> <span class="fu">predict</span>(drug.fit.reduced, <span class="at">newdata=</span><span class="fu">data.frame</span>(<span class="at">weight=</span><span class="dv">225</span>, <span class="at">drug=</span><span class="st">&quot;A&quot;</span>),<span class="at">int=</span><span class="st">&quot;conf&quot;</span>)</span>
<span id="cb275-2"><a href="more-on-multiple-linear-regression.html#cb275-2" aria-hidden="true" tabindex="-1"></a>p.A<span class="fl">.225</span></span></code></pre></div>
<pre><code>##       fit      lwr     upr
## 1 2.85283 -3.68925 9.39491</code></pre>
<div class="sourceCode" id="cb277"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb277-1"><a href="more-on-multiple-linear-regression.html#cb277-1" aria-hidden="true" tabindex="-1"></a>p.P<span class="fl">.225</span> <span class="ot">&lt;-</span> <span class="fu">predict</span>(drug.fit.reduced, <span class="at">newdata=</span><span class="fu">data.frame</span>(<span class="at">weight=</span><span class="dv">225</span>, <span class="at">drug=</span><span class="st">&quot;P&quot;</span>),<span class="at">int=</span><span class="st">&quot;conf&quot;</span>)</span>
<span id="cb277-2"><a href="more-on-multiple-linear-regression.html#cb277-2" aria-hidden="true" tabindex="-1"></a>p.P<span class="fl">.225</span></span></code></pre></div>
<pre><code>##        fit      lwr      upr
## 1 -6.23423 -12.5805 0.112083</code></pre>
<p>The first interval may be interpreted as follows: we can be 95% confident that the true mean reduction in glucose due to drug A for all men weighing 150 lb to be between 5.99 mg/dl to 18.55 mg/dl. For men of the same weight taking the placebo, the corresponding interval is –3.05 mg/dl to 9.39 mg/dl.</p>
<p>In short, the usual procedure for an ANCOVA model is:</p>
<ol style="list-style-type: decimal">
<li>Start by fitting a full interaction (“different slopes”) model.</li>
<li>Test the interaction term(s) first.
<ul>
<li>If the interaction term(s) is/are insignificant, delete them and fit an “equal slopes” model. Proceed to step 3.</li>
<li>If the interaction term(s) is are significant, the model cannot be further simplified. You must interpret the effect of the covariate by level of the qualitative predictor.</li>
</ul></li>
<li>If using an equal slopes model, you may investigate the overall effects of the qualitative predictor and the covariate independently.</li>
</ol>
</div>
<div id="ancova-with-a-multi-level-factor" class="section level3" number="7.3.2">
<h3><span class="header-section-number">7.3.2</span> ANCOVA with a multi-level factor</h3>
<p><strong>Example:</strong> As an example, we revisit the data from the previous section, only now augment the experiment to study the efficacy of four drugs (called <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span>, <span class="math inline">\(C\)</span> and a placebo <span class="math inline">\(P\)</span>) on glucose reduction (measured in mg/dl). Weight is still our covariate, and its effect may be different depending on the drug administered.</p>
<p>Coding up 4 drugs in the regression requires 4 – 1 = 3 dummy variables, as first cited above. We have flexibility in how we could define these three dummy variables, but here is probably the most meaningful way given the context of the drug experiment:</p>
<p><span class="math display">\[D_A = \left\{\begin{array}{ll} 1 &amp; \textrm{if drug } A \\ 0 &amp; \textrm{otherwise}\end{array}\right., ~~~ D_B = \left\{\begin{array}{ll} 1 &amp; \textrm{if drug } B \\ 0 &amp; \textrm{otherwise}\end{array}\right.,~~~ D_C = \left\{\begin{array}{ll} 1 &amp; \textrm{if drug } C \\ 0 &amp; \textrm{otherwise}\end{array}\right.\]</span></p>
<p>The above dummy variable coding scheme is the most meaningful way to code drug for this analysis because the placebo is the natural choice for a “reference” drug.</p>
<p>The data are in the R workspace <code>drug4.RData</code> and let’s look at an advanced scatterplot</p>
<pre><code>## `geom_smooth()` using formula &#39;y ~ x&#39;</code></pre>
<p><img src="introStatModeling_files/figure-html/ch7.23-1.png" width="672" /></p>
<p><em>Preliminary assessment:</em> It appears that drugs <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> perform very similarly across individuals of varying weights but lose their effectiveness in heavier individuals. However, weight does not appear to have as much of an impact on the effectiveness of drug <span class="math inline">\(C\)</span> (fairly shallow slope); moreover, glucose reduction for <span class="math inline">\(C\)</span> is generally quite high regardless of weight. The placebo <span class="math inline">\(P\)</span> appears to be largely ineffective (the responses tend to fluctuate around zero), but it is interesting that there appears to be some weight effect in the placebo (can you think of an explanation for this?).</p>
<p><strong>Step 1.</strong> We start with the full interaction (“different slopes”) model. We also choose to set the placebo as the reference level. Checking residual assumptions are left as an exercise to the interested reader.</p>
<div class="sourceCode" id="cb280"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb280-1"><a href="more-on-multiple-linear-regression.html#cb280-1" aria-hidden="true" tabindex="-1"></a>drug4 <span class="ot">&lt;-</span> drug4 <span class="sc">%&gt;%</span></span>
<span id="cb280-2"><a href="more-on-multiple-linear-regression.html#cb280-2" aria-hidden="true" tabindex="-1"></a>  <span class="fu">mutate</span>(<span class="at">drug=</span><span class="fu">relevel</span>(drug, <span class="at">ref=</span><span class="st">&quot;P&quot;</span>))</span>
<span id="cb280-3"><a href="more-on-multiple-linear-regression.html#cb280-3" aria-hidden="true" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> <span class="fu">lm</span>(glu.red <span class="sc">~</span> weight <span class="sc">+</span> drug <span class="sc">+</span> drug<span class="sc">:</span>weight, <span class="at">data=</span>drug4)</span>
<span id="cb280-4"><a href="more-on-multiple-linear-regression.html#cb280-4" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m1)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = glu.red ~ weight + drug + drug:weight, data = drug4)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -13.350  -5.304   0.729   5.456  14.294 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)  
## (Intercept)   15.1544    12.8630    1.18    0.245  
## weight        -0.0888     0.0680   -1.31    0.198  
## drugA         27.3532    20.8811    1.31    0.197  
## drugB         42.6055    16.7041    2.55    0.014 *
## drugC         10.6264    19.1505    0.55    0.582  
## weight:drugA  -0.0981     0.1111   -0.88    0.382  
## weight:drugB  -0.1604     0.0881   -1.82    0.075 .
## weight:drugC   0.0763     0.1030    0.74    0.463  
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.34 on 44 degrees of freedom
## Multiple R-squared:  0.692,  Adjusted R-squared:  0.643 
## F-statistic: 14.1 on 7 and 44 DF,  p-value: 1.98e-09</code></pre>
<p>The whole-model <span class="math inline">\(F\)</span>-test is significant. Let’s check to see if the model can be simplified. The three interaction terms in the model (<code>weight.drugA</code>, <code>weight.drugB</code> and <code>weight.drugC</code>) assess differences in slope between the reference drug <span class="math inline">\(P\)</span> versus <span class="math inline">\(A\)</span>, <span class="math inline">\(B\)</span> and <span class="math inline">\(C\)</span> respectively. The null hypothesis for testing if the slopes are equal is:</p>
<p><span class="math display">\[H_0: \beta_{weight.drugA} = \beta_{weight.drugB} = \beta_{weight.drugC} = 0\]</span></p>
<p>If this hypothesis is true, then the four slopes are the same; i.e. parallel. To see if we can simplify to an “equal slopes” model, test these three parameters using an ANOVA <span class="math inline">\(F\)</span>-test:</p>
<div class="sourceCode" id="cb282"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb282-1"><a href="more-on-multiple-linear-regression.html#cb282-1" aria-hidden="true" tabindex="-1"></a>m2 <span class="ot">&lt;-</span> <span class="fu">lm</span>(glu.red <span class="sc">~</span> weight <span class="sc">+</span> drug, <span class="at">data=</span>drug4)</span>
<span id="cb282-2"><a href="more-on-multiple-linear-regression.html#cb282-2" aria-hidden="true" tabindex="-1"></a><span class="fu">anova</span>(m2, m1)</span></code></pre></div>
<pre><code>## Analysis of Variance Table
## 
## Model 1: glu.red ~ weight + drug
## Model 2: glu.red ~ weight + drug + drug:weight
##   Res.Df  RSS Df Sum of Sq     F Pr(&gt;F)  
## 1     47 2765                            
## 2     44 2372  3     393.3 2.432 0.0777 .
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>The <span class="math inline">\(F\)</span>-test for the null hypothesis of parallel slopes is marginally significant (<span class="math inline">\(F\)</span> = 2.43, dfnum = 3, dfden = 44, <span class="math inline">\(p\)</span>-value = 0.0777). There is <em>marginal</em> evidence of an interaction between drug and weight: the effect of drug on glucose reduction depends marginally on the weight of the person.</p>
<p>Should we simplify to the “equal slopes” model (model m2 above)? There are a couple of things you can consider to help you decide:</p>
<ul>
<li>From the parameter estimates above, it appears that the largest difference between drugs with respect to how weight affects the reduction in glucose is between the placebo and drug <span class="math inline">\(B\)</span>. The slope estimate for the placebo is <span class="math inline">\(-0.08882\)</span>; for drug <span class="math inline">\(B\)</span>, the slope estimate is <span class="math inline">\(-0.08882 + (-0.16043) = -0.249\)</span>. This means that each additional pound of weight lowers the mean glucose reduction by 0.249 mg/dl if taking drug <span class="math inline">\(B\)</span>. This change is marginally significant (see <code>weight.drugB</code>: <span class="math inline">\(t\)</span> = –1.822, <span class="math inline">\(p\)</span>-value=0.0753), and can be estimated with a CI for <span class="math inline">\(\beta_{weight.drugB}\)</span>:</li>
</ul>
<div class="sourceCode" id="cb284"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb284-1"><a href="more-on-multiple-linear-regression.html#cb284-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confint</span>(m1)</span></code></pre></div>
<pre><code>##                   2.5 %     97.5 %
## (Intercept)  -10.769269 41.0780042
## weight        -0.225934  0.0482880
## drugA        -14.729867 69.4362663
## drugB          8.940645 76.2702598
## drugC        -27.968812 49.2215998
## weight:drugA  -0.321899  0.1257777
## weight:drugB  -0.337924  0.0170668
## weight:drugC  -0.131266  0.2839360</code></pre>
<p>The 95% confidence interval for the true mean difference in slopes between the placebo and <span class="math inline">\(B\)</span> is <span class="math inline">\((-0.3379, 0.0170)\)</span> mg/dl per pound. This CI contains 0, suggesting no significant slope difference.</p>
<ul>
<li>You could check the change in the residual SE or <span class="math inline">\(R_{adj}^2\)</span> values by using the “equal slopes” model:</li>
</ul>
<div class="sourceCode" id="cb286"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb286-1"><a href="more-on-multiple-linear-regression.html#cb286-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(m2)</span></code></pre></div>
<pre><code>## 
## Call:
## lm(formula = glu.red ~ weight + drug, data = drug4)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -15.46  -5.04   0.35   5.38  13.01 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)  26.6557     7.0688    3.77  0.00045 ***
## weight       -0.1504     0.0362   -4.15  0.00014 ***
## drugA         9.0639     2.9546    3.07  0.00357 ** 
## drugB        12.5859     2.8992    4.34  7.5e-05 ***
## drugC        24.1076     3.0974    7.78  5.4e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.67 on 47 degrees of freedom
## Multiple R-squared:  0.641,  Adjusted R-squared:  0.61 
## F-statistic: 20.9 on 4 and 47 DF,  p-value: 5.81e-10</code></pre>
<p><span class="math inline">\(R_{adj}^2\)</span> dropped from 0.6424 to 0.6099 by simplifying to the “equal slopes” model. This detriment to the quality of fit suggests there is validity in retaining the “different slopes” model.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="inference-regarding-multiple-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="model-building-considerations.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["introStatModeling.pdf", "introStatModeling.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
